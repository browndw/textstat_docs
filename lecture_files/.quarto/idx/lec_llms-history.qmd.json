{"title":"A Short History of LLMs","markdown":{"yaml":{"title":"A Short History of LLMs","subtitle":"Background and introduction to NLP","format":{"clean-revealjs":{"transition":"fade"}},"html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"author":[{"name":"David Brown","orcid":"0000-0001-7745-6354","email":"dwb2@andrew.cmu.edu","affiliations":"Statistics & Data Science 36-468/668"}],"date":"last-modified","date-format":"[Fall] YYYY","title-slide-attributes":{"data-background-image":"img/img_shared/text_analysis.png","data-background-size":"5%","data-background-position":"30% 50%"},"bibliography":"refs/refs_llms-history.bib"},"headingText":"Overview","headingAttr":{"id":"","classes":[],"keyvalue":[["background-color","#40666e"]]},"containsRefs":false,"markdown":"\n\n\n## Overview\n\n### How did we get to large language models (LLMs)\n\n-   Our topics\n    -   Review some history of natural language processing (NLP) and digital writing technologies\n    -   Look at the architectures of models and how they've changed over time\n    -   Share the results of some research and discuss some of the potential implications\n\n## Overview\n\n### How did we get to large language models (LLMs)\n\n-   Our goals\n    -   Show how these technologies fundamentally work.\n    -   Introduce some foundational concepts in NLP.\n    -   Walk though some considerations of research design. \n\n# History {background-color=\"#40666e\"}\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change [@gabrial2007history]\n\n\\\n\\\n\n::: r-stack\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 200px; background-position: center;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nNew surfaces:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\npapyrus\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nparchment\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nwood-pulp paper\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 300px; margin: 10px;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-stack\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 260px; height: 20px; position: absolute; top: 70%; left: 22%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 400px; margin: 10px;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nNew implements:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\nstylus\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nmetal-tipped pen\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nmass-produced pencil\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 300px; margin: 10px;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-stack\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 225px; height: 45px; position: absolute; top: 35%; left: 48%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 400px; background-position: right;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nNew systems:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\nlibraries\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\npostal networks\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\ncommercial publishers\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 300px; margin: 10px; background-position: right;\"}\n:::\n:::\n\n## History\n\n### Writing is inseparable from technological change\n\n-   Technological change is often met with skepticism, if not hostility and fear.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/syg_erasers.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *Think of the moral and intellectual training that comes to a student who writes a manuscript with the knowledge that his \\[sic\\] [errors will stand out on the page as honestly confessed and openly advertised mistakes.]{.bg style=\"--col: #f5b2c6\"}* [@syg1908erasers]\n:::\n:::\n:::\n\n## History\n\n### Writing is inseparable from technological change\n\n-   Technological change is often met with skepticism, if not hostility and fear.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/claxton_erasers.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%;\"}\n> *The eraser is an instrument of the devil because it perpetuates a culture of shame about error. It’s a way of lying to the world, which says ‘I didn’t make a mistake. I got it right first time.’ That’s what happens when you can rub it out and replace it. Instead, [we need a culture where children are not afraid to make mistakes, they look at their mistakes and they learn from them,]{.bg style=\"--col: #f5b2c6\"} where they are continuously reflecting and improving on what they’ve done, not being enthralled to getting the right answer quickly and looking smart.* [@espinoza2015erasers]\n:::\n:::\n:::\n\n## History\n\n### Writing is inseparable from technological change\n\n::: {style=\"font-size: 75%;\"}\nIn 2009, researchers at Google published an article that coincided with the release of its N-gram Viewer and the corresponding data tables [@halevy2009unreasonable].\n:::\n\n::: {style=\"font-size: 90%;\"}\n> *But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus—along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks—[if only we knew how to extract the model from the data]{.bg style=\"--col: #f5b2c6\"}.*\n:::\n\n::: {style=\"font-size: 75%; color: #636363;\"}\nWe will return to this excerpt, but for now, let's focus on this final claim...\n:::\n\n## History\n\n### The concept of a language model has been around for a long time…\n\n-   speech recognition [@bahl1983maximum; @jelinek1985realtime]\n-   spelling correction [@mays1991context]\n-   machine translation [@brown1990statistical]\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n-   machine translation [@brown1990statistical]\n\n\\\n\n::: r-stack\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/mt_timeline_01.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 200px; background-position: center;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n-   machine translation [@brown1990statistical]\n\n\\\n\n::: r-stack\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/mt_timeline_01.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 400px; background-position: left;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n![](img/img_llms-history/mt_timeline_02.png)\n\n::: r-hstack\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;\"}\n1960-1980\n\nBeginnings of NLP\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0.1\" style=\"background: #b71848; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;\"}\n1980-2015\n\nTowards Computation\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0.2\" style=\"background: #40666e; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;\"}\n2015-\n\nEmergence of ML\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n![](img/img_llms-history/mt_timeline_02.png)\n\n::: r-hstack\n::: {data-id=\"box1\" style=\"background: #d98b19; width: 375px; height: 10px; margin: 10px;\"}\n:::\n\n::: {data-id=\"box2\" style=\"background: #b71848; width: 475px; height: 10px; margin: 10px;\"}\n:::\n\n::: {data-id=\"box3\" style=\"background: #40666e; width: 150px; height: 10px; margin: 10px;\"}\n:::\n:::\n\n# [The beginnings of NLP]{style=\"color: white;\"} {background-color=\"#d98b19\"}\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"60%\"}\n![](img/img_llms-history/weaver_memo_01.png)\n:::\n\n::: {.column width=\"40%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> A memo shared with a small group of researchers who were at the forefront of machine translation after WWII, anticipates the challenges and possibilities of the computer analysis of text. [@weaver1949translation]\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/weaver_memo_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *If one examines the words in a book, one at a time as through an opaque mask with a hole in it on word wide, then it is obviously impossible to determine, one at a time, the meaning of words.*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/weaver_memo_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *But if one lengthens the slit in the opaque mask, until one can see not only the central word in question, but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word.*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/weaver_memo_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *The practical question is, what minimum value of N will, at least in a tolerable fraction of cases, lead to the correct choice of meaning for the central word?*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n-   \"You shall know a word by the company it keeps.\" [@firth1957papers]\n-   The meaning of word can be determined by examining the contextual window or span around that word.\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: {style=\"font-size: 70%; font-family: monospace; padding-top: 25px;\"}\n|                    Pre-node | Node | Post-node                                          |\n|--------------------:|:---:|-----------------------------------|\n|     upscaling generally hold | fast | during a 4K 60FPS gaming session.                  |\n|            a dragster, going | fast | in a straight line is actually pretty boring       |\n|          The benefits of the | fast | can be maintained long term,                       |\n| adopted slowly, but comes on | fast | once it's hit the mainstream                       |\n|  They simply disagree on how | fast | to go and how best to get there in superseding it. |\n|         which appeared stuck | fast | in the ground it had plowed up                     |\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: {style=\"font-size: 70%; font-family: monospace; padding-top: 25px;\"}\n|                                                                         Pre-node | Node | Post-node                                                                                               |\n|--------------------:|:---:|-----------------------------------|\n|     upscaling generally [hold]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [during]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} a 4K 60FPS gaming session.                  |\n|            a dragster, [going]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} a straight line is actually pretty boring       |\n|          The benefits of [the]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [can]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} be maintained long term,                       |\n| adopted slowly, but comes [on]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [once]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} it's hit the mainstream                       |\n|  They simply disagree on [how]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [to]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} go and how best to get there in superseding it. |\n|         which appeared [stuck]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} the ground it had plowed up                     |\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: {style=\"font-size: 70%; font-family: monospace; padding-top: 25px;\"}\n|                                                                         Pre-node | Node | Post-node                                                                                               |\n|--------------------:|:---:|-----------------------------------|\n|     upscaling [generally hold]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [during a]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} 4K 60FPS gaming session.                  |\n|            a [dragster, going]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in a]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} straight line is actually pretty boring       |\n|          The benefits [of the]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [can be]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} maintained long term,                       |\n| adopted slowly, but [comes on]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [once it's]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} hit the mainstream                       |\n|  They simply disagree [on how]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [to go]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} and how best to get there in superseding it. |\n|         which [appeared stuck]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in the]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} ground it had plowed up                     |\n:::\n\n## The beginnings of NLP\n\n-   The [\"context window\"]{.bg style=\"--col: #f5b2c6\"} is a fundamental insight that powers the training of LLMs (from word2vec to BERT to ChatGPT).\n\n\\\n\n![](img/img_llms-history/context_window.png)\n\n## The beginnings of NLP\n\n### As early as the mid-twentieth century, researchers…\n\n-   had considered the potential for a \"context window\" to solve word-sense disambiguation\n\n-   were developing the statistical tools that would eventually power the training of LLMs (i.e., neural networks).\n\n::: callout-warning\n## Question\n\nWhy didn’t we have LLMs sooner?\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n-   To cope with these limitations (and beliefs about language structure) early models resorted to hard-coding rules\n\n::: {style=\"font-size: 75%; font-family: monospace; text-indent: 30%;\"}\nS → NP VP\n\nNP → the N\n\nVP → V NP\n\nV → sings \\| eats\n\nN → cat \\| song \\| canary\n\n-- the canary sings the song\n\n-- the song eats the cat\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/alpac_01.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> The ALPAC Report, which was released in 1966, was highly skeptical of these kinds of approaches.\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/alpac_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *…we do not have useful machine translation. Furthermore, there is no immediate or predictable prospect of useful machine translation.*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/alpac_03.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *Some of the work must be done on a rather large scale, since small-scale experiments and work with miniature models of language have proved seriously deceptive in the past, and one can come to grips with real problems only above a certain scale of grammar size, dictionary size, and available corpus.*\n:::\n:::\n:::\n\n# [Towards computation]{style=\"color: white;\"} {background-color=\"#b71848\"}\n\n## Towards computation\n\n### Converting words into numbers (a typical processing pipeline)\n\n\\\n\n![](img/img_llms-history/pipeline.svg)\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n\\\n\n::: columns\n::: {.column width=\"40%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n-   The make-up or sampling frame of the Brown family of corpora. [@kucera1967computational]\n-   From the 15 categories, 2000-word text samples were selected.\n-   2000 x 500 ≈ 1,000,000 words\n:::\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| echo: false\n\nreadr::read_csv(\"data/data_llms-history/brown_sampling_frame.csv\") |>\n  knitr::kable(\"html\") |>\n  kableExtra::kable_styling(font_size = 18)\n```\n:::\n:::\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Absolute frequency in the Brown Corpus.\"\n\nreadr::read_csv(\"data/data_llms-history/brown_dtm.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 18)\n```\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Observations.\"\n\nreadr::read_csv(\"data/data_llms-history/brown_dtm.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 18) |>\n  kableExtra::column_spec(1, background = \"#f5b2c6\")\n```\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Variables.\"\n\nreadr::read_csv(\"data/data_llms-history/brown_dtm.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 18) |>\n  kableExtra::row_spec(0, background = \"#f5b2c6\")\n```\n\n## Towards computation {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Zipf's Law\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nMost frequent words:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\nthe\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nof\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nand\n:::\n\n::: {data-id=\"scatter1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 800px; height: 400px; background-position: left;\"}\n:::\n:::\n\n## Towards computation {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Zipf's Law\n\n::: r-stack\n::: {data-id=\"scatter1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 800px; height: 400px; background-position: left;\"}\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 18%; left: 20.5%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 40%; left: 20.5%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 45%; left: 20.5%; opacity: 0.5;\"}\n:::\n:::\n\n## Towards computation {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Zipf's Law\n\n::: r-stack\n::: {data-id=\"scatter1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 600px; background-position: left;\"}\n:::\n\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 500px; height: 200px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle; position: absolute; top: 18%; left: 35%;\"}\n-   Zipf's Law: the frequency of a token is inversely proportional to its rank.\n-   Most tokens are infrequent.\n-   The absence of evidence is not evidence of absence.\n:::\n:::\n\n## Towards computation\n\n### While some words may be normally distributed, most are not.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/histogram_the.svg)\n:::\n\n::: {.column width=\"50%\"}\n![](img/img_llms-history/histogram_data.svg)\n:::\n:::\n\n# Emergence of machine learning {background-color=\"#40666e\"}\n\n## Emergence of ML\n\n### Let's return to the excerpt from the Google researchers.\n\n::: {style=\"font-size: 90%;\"}\n> *But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus—along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks—if only we knew how to extract the model from the data.*\n:::\n\n::: callout-warning\n## Question\n\nWhat developments are taking place at this time (the early 2000s)?\n:::\n\n## Emergence of ML\n\n-   Word2vec is released. [@mikolov2013efficient]\n    -   Shallow (2-layer) neural network.\n    -   Trained using a relatively small context window (\\~10-12 words).\n    -   Introduces \"embeddings\".\n\n## Emergence of ML\n\n### Embeddings from a vector model.\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Embedding space.\"\n\nreadr::read_csv(\"data/data_llms-history/vector_embeddings.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 14)\n```\n\n## Emergence of ML\n\n### Embeddings from a vector model.\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Dimensions.\"\n\nreadr::read_csv(\"data/data_llms-history/vector_embeddings.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 14) |>\n  kableExtra::row_spec(0, background = \"#f5b2c6\")\n```\n\n## Emergence of ML {background-image=\"img/img_llms-history/cos_similarity_02.png\" background-size=\"60%\" background-opacity=\".5\"}\n\n### Embeddings from a vector model.\n\n-   When treated as coordinates in space, embeddings locate words that tend to appear together or in similar contexts near each other.\n\n## Emergence of ML\n\n### Embeddings from a vector model.\n\n-   The proximity of words can be assessed using measures like cosine similarity.\n\n$$\ncosine~similarity = S_{c}(A, B) := cos(\\theta) = \\frac{A \\cdot B}{||A||~||B||}\n$$ ![](img/img_llms-history/cos_similarity.png)\n\n## Emergence of ML {background-video=\"img/img_llms-history/vector_model_01.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n-   [An example of a vector model rendered in 3 dimensions]{style=\"color: white; background-color: #e64173; opacity: 0.75;\"} from <https://projector.tensorflow.org/>.\n\n## Emergence of ML {background-image=\"img/img_llms-history/vector_model_02.png\"}\n\n### Embeddings from a vector model.\n\n::: columns\n::: {.column width=\"25%\"}\n::: {style=\"font-size: 18px; padding-top: 10px;\"}\n|     token | similarity |\n|----------:|-----------:|\n|      slow |      0.448 |\n|     quick |      0.519 |\n|    faster |      0.568 |\n|    slower |      0.593 |\n|     speed |      0.602 |\n|      busy |      0.646 |\n|    simple |      0.663 |\n|      food |      0.676 |\n|    speeds |      0.688 |\n|   fastest |      0.688 |\n|      pace |      0.697 |\n| efficient |      0.703 |\n|      easy |      0.707 |\n|     small |      0.710 |\n|       too |      0.712 |\n|  straight |      0.717 |\n|     rapid |      0.717 |\n|       low |      0.718 |\n|   quickly |      0.718 |\n|    packet |      0.718 |\n\n: Tokens closest to *fast*\n:::\n:::\n\n::: {.column width=\"75%\"}\n:::\n:::\n\n## Emergence of ML \n\n::: {style=\"font-size: 70%;\"}\n-   After the introduction of vector representations and, a short time later, the transformer architecture [@vaswani2017attention], language models have rapidly evolved. They can be grouped into roughly 3 generations.\n:::\n\n![](img/img_llms-history/llms_generations.png)\n\n## Emergence of ML\n\n### Advances in LLMs…\n\n-   Allowing for out-of-vocabulary words (using sub-words or word-pieces for tokenizing).\n\n-   Adding a sequence layer.\n\n-   Sliding a context window both left-to-right and right-to-left.\n\n-   Implementing self-attention architecture.\n\n-   Training on more and more data.\n\n-   Expanding the context window (from 512 word-pieces for BERT to 128,000 word-pieces for GPT-4 Turbo 128K).\n\n-   Introducing reinforcement learning from human feedback (RLHF) with Instruct GPT.\n\n## Emergence of ML\n\n### An example of contextual embeddings using BERT\n\n::: {style=\"font-size: 80%;\"}\n```{.python}\n\nsentences = [\"bank\",\n\t\"He eventually sold the shares back to the bank at a premium.\",\n\t\"The bank strongly resisted cutting interest rates.\",\n\t\"The bank will supply and buy back foreign currency.\",\n\t\"The bank is pressing us for repayment of the loan.\",\n\t\"The bank left its lending rates unchanged.\",\n\t\"The river flowed over the bank.\",\n\t\"Tall, luxuriant plants grew along the river bank.\",\n\t\"His soldiers were arrayed along the river bank.\",\n\t\"Wild flowers adorned the river bank.\",\n\t\"Two fox cubs romped playfully on the river bank.\",\n\t\"The jewels were kept in a bank vault.\",\n\t\"You can stow your jewelry away in the bank.\",\n\t\"Most of the money was in storage in bank vaults.\",\n\t\"The diamonds are shut away in a bank vault somewhere.\",\n\t\"Thieves broke into the bank vault.\",\n\t\"Can I bank on your support?\",\n\t\"You can bank on him to hand you a reasonable bill for your services.\",\n\t\"Don't bank on your friends to help you out of trouble.\",\n\t\"You can bank on me when you need money.\",\n\t\"I bank on your help.\"]\n\n```\n:::\n\n## Emergence of ML\n\n### An example of contextual embeddings using BERT\n\n::: {style=\"font-size: 80%;\"}\n```{.python}\n\nfrom collections import OrderedDict\n\ncontext_embeddings = []\ncontext_tokens = []\nfor sentence in sentences:\n\ttokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n\tlist_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n  # make ordered dictionary to keep track of the position of each word\n\ttokens = OrderedDict()\n  # loop over tokens in sensitive sentence\n\tfor token in tokenized_text[1:-1]:\n\t\t# keep track of position of word and whether it occurs multiple times\n\t\tif token in tokens:\n\t\t\ttokens[token] += 1\n\t\telse:\n\t\ttokens[token] = 1\n  \t# compute the position of the current token\n\t\ttoken_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n\t\tcurrent_index = token_indices[tokens[token]-1]\n  \t# get the corresponding embedding\n\t\ttoken_vec = list_token_embeddings[current_index]\n  \t# save values\n\t\tcontext_tokens.append(token)\n\t\tcontext_embeddings.append(token_vec)\n\t\t\n```\n:::\n\n::: aside\nA Colab with the full code is [here](https://colab.research.google.com/drive/1ea3zDFrCQFQhkvinaQfdbvXlOhR7hw01?usp=sharing#scrollTo=4wf0epYMLh22)\n:::\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_01.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_02.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_03.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_04.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML\n\n### LLMs have a broad range of applications…\n\n-   Generation tasks\n    -   Chat bots\n    -   Content creation\n    -   Summarization\n    -   Translation\n-   Classification tasks\n    -   Text classification\n    -   Segment classification\n\n## Emergence of ML\n\n### Just as they raise questions regarding…\n\n-   The production of content hallucinations [@ji2023survey; @zhang2023language]\n-   Expressions of bias [@santurkar2023opinions]\n-   A tendency to repeat back a user’s stated views (“sycophancy”) [@perez2022discovering]\n\n# Investigating LLM-Generated Text {background-color=\"#40666e\"}\n\n## Investigating LLM-generated text\n\n-  A group in the Statistics & Data Science Department was inspired by claims that were circulating when ChatGPT was first introduced. (e.g., \"Wow! I asked ChatGPT to write a podcast and [it looks pretty good](https://www.npr.org/transcripts/1178290105)!!!\")\n-  We wondered what the text it produces looks like when it is repeated. (e.g., \"What happens if you ask it to write 100 podcasts?\")\n-  We gave it the same writing prompt that students are given in 36-200, generated 100 introduction, and compared those with introductions written by the actual students and introductions that appear in published, data-driven, academic papers.\n-  Then, we tagged the data using Biber's [-@biber1991variation] features (which counts things like [passives, nominalizations, attributive adjectives](https://cmu-textstat-docs.readthedocs.io/en/latest/pseudobibeR/pseudobibeR.html#categories), etc.).\n\n## Investigating LLM-generated text\n\n-   It turns out, machine-authored prose and human-authored prose don’t really look the same in their morphosyntactic and functional features. [@herbold2023large; @markey2024dense]\n\n![Projection of student, published, and ChatGPT-generated writing onto the first two linear discriminants, based on the 67 Biber features.](img/img_llms-history/lda_scatter.svg)\n\n## Investigating LLM-generated text\n\n### Human-generated vs. machine-generated text\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Top text features discriminating between human- and machine-generated writing. Color-coded cells show the average z score of each feature. R2 and p-value correspond to one-way ANOVAs predicting each feature with text type.\"\n\nld1_tbl <- readr::read_csv(\"data/data_llms-history/ld1_tbl.csv\")\n\nld1_tbl <- ld1_tbl |> \n  dplyr::mutate(direction = paste0(\"Features indicating \", direction, \"-generated writing\")) |>\n  dplyr::mutate(variable = stringr::str_remove(variable, \"f_\\\\d+_\")) |>\n  dplyr::mutate(variable = stringr::str_replace_all(variable, \"_\", \" \")) |>\n  dplyr::filter(direction == \"Features indicating human-generated writing\") |>\n  gt::gt(groupname_col = 'direction') |>\n  gt::cols_label(\n    variable = gt::md(\"  \"),\n    ChatGPT = gt::md(\"**ChatGPT<br>n:100**\"),\n    Published = gt::md(\"**Published<br>n:100**\"),\n    Student = gt::md(\"**Student<br>n:100**\"),\n    r.squared = gt::md(\"***R*^2^**\"),\n    p.value = gt::md(\"***p*-value**\")\n  ) |> \n  gt::fmt_number(\n    columns = dplyr::everything(),\n    decimals = 2\n  )  |> \n  gt::data_color(\n    columns = c(ChatGPT:Student),\n    colors = scales::col_numeric(\n      palette = c(\n        \"#FF6666\", \"white\", \"#336699\"),\n      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student), \n                 0, \n                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n      ),\n    locations = gt::cells_body(\n      columns = variable)\n    ) |>\n  gt::tab_options(quarto.use_bootstrap = TRUE)\n\nld1_tbl |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n```\n\n## Implications\n\n### Human-generated vs. machine-generated text\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Top text features discriminating between human- and machine-generated writing. Color-coded cells show the average z score of each feature. R2 and p-value correspond to one-way ANOVAs predicting each feature with text type.\"\n\nld1_tbl <- readr::read_csv(\"data/data_llms-history/ld1_tbl.csv\")\n\nld1_tbl <- ld1_tbl |> \n  dplyr::mutate(direction = paste0(\"Features indicating \", direction, \"-generated writing\")) |>\n  dplyr::mutate(variable = stringr::str_remove(variable, \"f_\\\\d+_\")) |>\n  dplyr::mutate(variable = stringr::str_replace_all(variable, \"_\", \" \")) |>\n  dplyr::filter(direction == \"Features indicating machine-generated writing\") |>\n  gt::gt(groupname_col = 'direction') |>\n  gt::cols_label(\n    variable = gt::md(\"  \"),\n    ChatGPT = gt::md(\"**ChatGPT<br>n:100**\"),\n    Published = gt::md(\"**Published<br>n:100**\"),\n    Student = gt::md(\"**Student<br>n:100**\"),\n    r.squared = gt::md(\"***R*^2^**\"),\n    p.value = gt::md(\"***p*-value**\")\n  ) |> \n  gt::fmt_number(\n    columns = dplyr::everything(),\n    decimals = 2\n  )  |> \n  gt::data_color(\n    columns = c(ChatGPT:Student),\n    colors = scales::col_numeric(\n      palette = c(\n        \"#FF6666\", \"white\", \"#336699\"),\n      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student), \n                 0, \n                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n      ),\n    locations = gt::cells_body(\n      columns = variable)\n    ) |>\n  gt::tab_options(quarto.use_bootstrap = TRUE)\n\nld1_tbl |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n```\n\n## Investigating LLM-generated text\n\n::: {style=\"font-size: 60%;\"}\n-   ChatGPT, for example, produces a more restricted set of modal verbs -- one that is different from both expert and novice writers.\n:::\n\n```{r}\n#| echo: false\n#| warning: false\n#| results: asis\n#| tbl-cap: \"Frequency of different modal verbs, often modulating the confidence of claims, in the different types of writing.\"\n\nmodal_freq <- readr::read_csv(\"data/data_llms-history/modal_freq.csv\")\n\nmodal_freq <- modal_freq |>\n  gt::gt(groupname_col = 'modal_type') |>\n  gt::cols_label(\n    token =  gt::md(\"Modal verb\"),\n    AF_chatgpt =  gt::md(\"ChatGPT\"),\n    AF_published =  gt::md(\"Published\"),\n    AF_student =  gt::md(\"Student\"),\n    RF_chatgpt =  gt::md(\"ChatGPT\"),\n    RF_published =  gt::md(\"Published\"),\n    RF_student =  gt::md(\"Student\"),\n  ) |> \n  gt::tab_spanner(\n    label = \"Absolute Frequency\",\n    columns = c(AF_chatgpt, AF_published, AF_student)\n  ) |>\n  gt::tab_spanner(\n    label =  gt::md(\"Relative Frequency (per 10^5^ words)\"),\n    columns = c(RF_chatgpt, RF_published, RF_student)\n  ) |>\n  gt::fmt_number(\n    columns = c(RF_chatgpt, RF_published, RF_student),\n    decimals = 2\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n    ),\n    locations =  gt::cells_body(\n      columns = token,\n    )\n  )\n\nmodal_freq |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::tab_options(quarto.disable_processing = TRUE,\n                  table.font.size = 14) |>\n  gt::as_raw_html()\n```\n\n## Investigating LLM-generated text\n\n### Human-generated vs. machine-generated text\n\n![Excerpts from texts produced by ChatGPT](img/img_llms-history/gpt_excerpts_01.png){width=80%}\n\n::: aside\nThe LLM has a propensity to condense information into chunked noun phrases, constructed as either *noun* + *noun* or *adjective* + *noun* sequences. Such sequences are sometimes aggregated using the coordinator *and*. On the one hand, these phrases can project a kind of authoritative voice. On the other, their content can range from vague, to ambiguous, to vapid. (What exactly is “a comprehensive dataset encompassing [..] healthcare utilization”?) At issue here is what pertinent information is compressed out of these phrases.\n:::\n\n## Investigating LLM-generated text\n\n::: callout-important\n## Lab Set Question\n\nIf you were working on this project, what would you suggest the team do next? In other words, what limitations do you see in the results of this initial study? What might be done to increase its reliability? Or its generalizability? And what potential challenges do you foresee in applying your suggestions? (Discuss with a couple of your neighbors and write your response)\n:::\n\n## Investigating LLM-generated text\n\n-   We also created an experiment at scale, with 10,000 samples, across 9 text-types, and querying 4 different models (ChatGPT 3.5, ChatGPT 4.0, Llama 3 8B-Base, and Llama 3 8B-Instruct).\n\n```{r}\n#| echo: false\n#| warning: false\n#| tbl-cap: \"A confusion matrix for a classifier pridicting all 4 LLMs and human-generated text.\"\n\ntest_preds <- readr::read_csv(\"data/data_llms-history/test_preds.csv\")\n\ntest_preds <- test_preds |>\n  dplyr::group_by(label, prediction) |>\n  dplyr::summarize(n = dplyr::n(), .groups = \"drop\") |>\n  tidyr::pivot_wider(id_cols = label,\n              names_from = prediction,\n              values_from = n) |>\n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric),\n                ~ .x / (chunk_2 + chunk_gpt3 + chunk_gpt4 + llama_3_8b + llama_3_8b_instruct))) |>\n  dplyr::mutate(label = stringr::str_replace_all(label, \"_\", \" \")) |>\n  dplyr::mutate(label = stringr::str_remove(label, \"chunk \")) |>\n  dplyr::mutate(label = stringr::str_remove(label, \"llama 3 \")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"2\", \"Human\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"gpt\", \"GPT-\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"3\", \"3.5\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"8b\", \"8B-Base\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"-Base instruct\", \"-Instruct\")) |>\n  gt::gt() |>\n  gt::fmt_percent(c(chunk_2, chunk_gpt3, chunk_gpt4, llama_3_8b, llama_3_8b_instruct), decimals = 1)  |>\n  gt::cols_label(label = \"Generator\",\n             chunk_2 = \"Human\",\n             chunk_gpt3 = \"GPT-3.5\",\n             chunk_gpt4 = \"GPT-4\",\n             llama_3_8b = \"8B-Base\",\n             llama_3_8b_instruct = \"8B-Instruct\")\n\ntest_preds |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n\n```\n\n## Investigating LLM-generated text\n\n-   Many of the features are the same ones we saw in the smaller study.\n\n```{r}\n#| echo: false\n#| warning: false\n#| tbl-cap: \"The 10 features with the highest importance.\"\n\nfeature_imp <- readr::read_csv(\"data/data_llms-history/feature_importance.csv\")\n\nfeature_imp <- feature_imp |>\n  dplyr::mutate(feature = stringr::str_remove(feature, \"f_\\\\d+_\")) |>\n  dplyr::mutate(feature = stringr::str_replace_all(feature, \"_\", \" \")) |>\n  head(10) |>\n  gt::gt() |>\n  gt::cols_label(feature = \"Feature\",\n             chunk_1 = \"Chunk 1\",\n             chunk_2 = \"Chunk 2\",\n             chunk_gpt3 = \"GPT-3.5\",\n             chunk_gpt4 = \"GPT-4\",\n             llama_3_8b = \"8B-Base\",\n             llama_3_8b_instruct = \"8B-Instruct\",\n             importance = \"Importance\") |>\n  gt::tab_spanner(\"Human\", c(chunk_1, chunk_2)) |>\n  gt::tab_spanner(\"GPT\", chunk_gpt3:chunk_gpt4) |>\n  gt::tab_spanner(\"Llama 3\", llama_3_8b:llama_3_8b_instruct) |>\n  gt::fmt_percent(c(chunk_gpt3, chunk_gpt4, llama_3_8b, llama_3_8b_instruct), decimals = 0) |>\n  gt::fmt_number(c(chunk_1, chunk_2, importance), decimals = 1) |>\n  gt::tab_header(title = \"Features in human- and LLM-written text\",\n             subtitle = \"Rate per 1,000 tokens; LLM rates relative to Chunk 2\") |> \n  gt::data_color(\n    columns = c(chunk_gpt3:llama_3_8b_instruct),\n    direction = \"row\",\n    method = \"numeric\",\n    palette = c(\"#FF6666\", \"white\", \"#336699\"),\n    domain = c(0, 1 ,2),\n    na_color = \"#336699\"\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n    ),\n    locations = gt::cells_body(\n      columns = feature,\n    )\n  )\n\nfeature_imp |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n```\n\n## Implications\n\n### Key takeaways\n\n1.  ChatGPT produces sentences that are more informationally dense than that of student writing, but the information density is created using repetitive grammatical patterns.\n2.  Human-generated writing demonstrates more modulation of stress or confidence.\n3.  Broadly, LLMs are not as grammatically nimble as human writers.\n4.  LLM-generated text generally does not produce academic prose that engages with core, disciplinary concepts either in the way that experts do or in the way that novice students do. (Though LLMs can arrive at something more like expert writing with iterative prompting.)\n5.  It is a open question as to the effects of LLMs on the development of students' disciplinary expertise.\n\n## Works Cited\n","srcMarkdownNoYaml":"\n\n# Overview {background-color=\"#40666e\"}\n\n## Overview\n\n### How did we get to large language models (LLMs)\n\n-   Our topics\n    -   Review some history of natural language processing (NLP) and digital writing technologies\n    -   Look at the architectures of models and how they've changed over time\n    -   Share the results of some research and discuss some of the potential implications\n\n## Overview\n\n### How did we get to large language models (LLMs)\n\n-   Our goals\n    -   Show how these technologies fundamentally work.\n    -   Introduce some foundational concepts in NLP.\n    -   Walk though some considerations of research design. \n\n# History {background-color=\"#40666e\"}\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change [@gabrial2007history]\n\n\\\n\\\n\n::: r-stack\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 200px; background-position: center;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nNew surfaces:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\npapyrus\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nparchment\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nwood-pulp paper\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 300px; margin: 10px;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-stack\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 260px; height: 20px; position: absolute; top: 70%; left: 22%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 400px; margin: 10px;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nNew implements:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\nstylus\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nmetal-tipped pen\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nmass-produced pencil\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 300px; margin: 10px;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-stack\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 225px; height: 45px; position: absolute; top: 35%; left: 48%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 400px; background-position: right;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Writing is inseparable from technological change\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nNew systems:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\nlibraries\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\npostal networks\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\ncommercial publishers\n:::\n\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/writing_timeline.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 300px; margin: 10px; background-position: right;\"}\n:::\n:::\n\n## History\n\n### Writing is inseparable from technological change\n\n-   Technological change is often met with skepticism, if not hostility and fear.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/syg_erasers.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *Think of the moral and intellectual training that comes to a student who writes a manuscript with the knowledge that his \\[sic\\] [errors will stand out on the page as honestly confessed and openly advertised mistakes.]{.bg style=\"--col: #f5b2c6\"}* [@syg1908erasers]\n:::\n:::\n:::\n\n## History\n\n### Writing is inseparable from technological change\n\n-   Technological change is often met with skepticism, if not hostility and fear.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/claxton_erasers.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%;\"}\n> *The eraser is an instrument of the devil because it perpetuates a culture of shame about error. It’s a way of lying to the world, which says ‘I didn’t make a mistake. I got it right first time.’ That’s what happens when you can rub it out and replace it. Instead, [we need a culture where children are not afraid to make mistakes, they look at their mistakes and they learn from them,]{.bg style=\"--col: #f5b2c6\"} where they are continuously reflecting and improving on what they’ve done, not being enthralled to getting the right answer quickly and looking smart.* [@espinoza2015erasers]\n:::\n:::\n:::\n\n## History\n\n### Writing is inseparable from technological change\n\n::: {style=\"font-size: 75%;\"}\nIn 2009, researchers at Google published an article that coincided with the release of its N-gram Viewer and the corresponding data tables [@halevy2009unreasonable].\n:::\n\n::: {style=\"font-size: 90%;\"}\n> *But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus—along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks—[if only we knew how to extract the model from the data]{.bg style=\"--col: #f5b2c6\"}.*\n:::\n\n::: {style=\"font-size: 75%; color: #636363;\"}\nWe will return to this excerpt, but for now, let's focus on this final claim...\n:::\n\n## History\n\n### The concept of a language model has been around for a long time…\n\n-   speech recognition [@bahl1983maximum; @jelinek1985realtime]\n-   spelling correction [@mays1991context]\n-   machine translation [@brown1990statistical]\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n-   machine translation [@brown1990statistical]\n\n\\\n\n::: r-stack\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/mt_timeline_01.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 200px; background-position: center;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n-   machine translation [@brown1990statistical]\n\n\\\n\n::: r-stack\n::: {data-id=\"time1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/mt_timeline_01.svg');  background-repeat: no-repeat; background-size: cover; width: 900px; height: 400px; background-position: left;\"}\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n![](img/img_llms-history/mt_timeline_02.png)\n\n::: r-hstack\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;\"}\n1960-1980\n\nBeginnings of NLP\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0.1\" style=\"background: #b71848; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;\"}\n1980-2015\n\nTowards Computation\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0.2\" style=\"background: #40666e; width: 250px; height: 150px; margin: 10px; font-size: 75%; color: white; text-align: center;\"}\n2015-\n\nEmergence of ML\n:::\n:::\n\n## History {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### The concept of a language model has been around for a long time…\n\n![](img/img_llms-history/mt_timeline_02.png)\n\n::: r-hstack\n::: {data-id=\"box1\" style=\"background: #d98b19; width: 375px; height: 10px; margin: 10px;\"}\n:::\n\n::: {data-id=\"box2\" style=\"background: #b71848; width: 475px; height: 10px; margin: 10px;\"}\n:::\n\n::: {data-id=\"box3\" style=\"background: #40666e; width: 150px; height: 10px; margin: 10px;\"}\n:::\n:::\n\n# [The beginnings of NLP]{style=\"color: white;\"} {background-color=\"#d98b19\"}\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"60%\"}\n![](img/img_llms-history/weaver_memo_01.png)\n:::\n\n::: {.column width=\"40%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> A memo shared with a small group of researchers who were at the forefront of machine translation after WWII, anticipates the challenges and possibilities of the computer analysis of text. [@weaver1949translation]\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/weaver_memo_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *If one examines the words in a book, one at a time as through an opaque mask with a hole in it on word wide, then it is obviously impossible to determine, one at a time, the meaning of words.*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/weaver_memo_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *But if one lengthens the slit in the opaque mask, until one can see not only the central word in question, but also say N words on either side, then if N is large enough one can unambiguously decide the meaning of the central word.*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/weaver_memo_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *The practical question is, what minimum value of N will, at least in a tolerable fraction of cases, lead to the correct choice of meaning for the central word?*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n-   \"You shall know a word by the company it keeps.\" [@firth1957papers]\n-   The meaning of word can be determined by examining the contextual window or span around that word.\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: {style=\"font-size: 70%; font-family: monospace; padding-top: 25px;\"}\n|                    Pre-node | Node | Post-node                                          |\n|--------------------:|:---:|-----------------------------------|\n|     upscaling generally hold | fast | during a 4K 60FPS gaming session.                  |\n|            a dragster, going | fast | in a straight line is actually pretty boring       |\n|          The benefits of the | fast | can be maintained long term,                       |\n| adopted slowly, but comes on | fast | once it's hit the mainstream                       |\n|  They simply disagree on how | fast | to go and how best to get there in superseding it. |\n|         which appeared stuck | fast | in the ground it had plowed up                     |\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: {style=\"font-size: 70%; font-family: monospace; padding-top: 25px;\"}\n|                                                                         Pre-node | Node | Post-node                                                                                               |\n|--------------------:|:---:|-----------------------------------|\n|     upscaling generally [hold]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [during]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} a 4K 60FPS gaming session.                  |\n|            a dragster, [going]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} a straight line is actually pretty boring       |\n|          The benefits of [the]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [can]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} be maintained long term,                       |\n| adopted slowly, but comes [on]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [once]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} it's hit the mainstream                       |\n|  They simply disagree on [how]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [to]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} go and how best to get there in superseding it. |\n|         which appeared [stuck]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} the ground it had plowed up                     |\n:::\n\n## The beginnings of NLP\n\n### The question of multiple meanings (or polysemy)\n\n::: {style=\"font-size: 70%; font-family: monospace; padding-top: 25px;\"}\n|                                                                         Pre-node | Node | Post-node                                                                                               |\n|--------------------:|:---:|-----------------------------------|\n|     upscaling [generally hold]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [during a]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} 4K 60FPS gaming session.                  |\n|            a [dragster, going]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in a]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} straight line is actually pretty boring       |\n|          The benefits [of the]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [can be]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} maintained long term,                       |\n| adopted slowly, but [comes on]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [once it's]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} hit the mainstream                       |\n|  They simply disagree [on how]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [to go]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} and how best to get there in superseding it. |\n|         which [appeared stuck]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} | fast | [in the]{style=\"background-color: #f5b2c6; opacity: 0.75;\"} ground it had plowed up                     |\n:::\n\n## The beginnings of NLP\n\n-   The [\"context window\"]{.bg style=\"--col: #f5b2c6\"} is a fundamental insight that powers the training of LLMs (from word2vec to BERT to ChatGPT).\n\n\\\n\n![](img/img_llms-history/context_window.png)\n\n## The beginnings of NLP\n\n### As early as the mid-twentieth century, researchers…\n\n-   had considered the potential for a \"context window\" to solve word-sense disambiguation\n\n-   were developing the statistical tools that would eventually power the training of LLMs (i.e., neural networks).\n\n::: callout-warning\n## Question\n\nWhy didn’t we have LLMs sooner?\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n-   To cope with these limitations (and beliefs about language structure) early models resorted to hard-coding rules\n\n::: {style=\"font-size: 75%; font-family: monospace; text-indent: 30%;\"}\nS → NP VP\n\nNP → the N\n\nVP → V NP\n\nV → sings \\| eats\n\nN → cat \\| song \\| canary\n\n-- the canary sings the song\n\n-- the song eats the cat\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/alpac_01.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> The ALPAC Report, which was released in 1966, was highly skeptical of these kinds of approaches.\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/alpac_02.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *…we do not have useful machine translation. Furthermore, there is no immediate or predictable prospect of useful machine translation.*\n:::\n:::\n:::\n\n## The beginnings of NLP\n\n### Context free grammar\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/alpac_03.png)\n:::\n\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n> *Some of the work must be done on a rather large scale, since small-scale experiments and work with miniature models of language have proved seriously deceptive in the past, and one can come to grips with real problems only above a certain scale of grammar size, dictionary size, and available corpus.*\n:::\n:::\n:::\n\n# [Towards computation]{style=\"color: white;\"} {background-color=\"#b71848\"}\n\n## Towards computation\n\n### Converting words into numbers (a typical processing pipeline)\n\n\\\n\n![](img/img_llms-history/pipeline.svg)\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n\\\n\n::: columns\n::: {.column width=\"40%\"}\n::: {style=\"font-size: 75%; padding-top: 25px;\"}\n-   The make-up or sampling frame of the Brown family of corpora. [@kucera1967computational]\n-   From the 15 categories, 2000-word text samples were selected.\n-   2000 x 500 ≈ 1,000,000 words\n:::\n:::\n\n::: {.column width=\"60%\"}\n```{r}\n#| echo: false\n\nreadr::read_csv(\"data/data_llms-history/brown_sampling_frame.csv\") |>\n  knitr::kable(\"html\") |>\n  kableExtra::kable_styling(font_size = 18)\n```\n:::\n:::\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Absolute frequency in the Brown Corpus.\"\n\nreadr::read_csv(\"data/data_llms-history/brown_dtm.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 18)\n```\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Observations.\"\n\nreadr::read_csv(\"data/data_llms-history/brown_dtm.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 18) |>\n  kableExtra::column_spec(1, background = \"#f5b2c6\")\n```\n\n## Towards computation\n\n### A document-feature matrix (or a document-term matrix)\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Variables.\"\n\nreadr::read_csv(\"data/data_llms-history/brown_dtm.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 18) |>\n  kableExtra::row_spec(0, background = \"#f5b2c6\")\n```\n\n## Towards computation {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Zipf's Law\n\n::: r-vstack\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 200px; height: 50px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle;\"}\nMost frequent words:\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle;\"}\nthe\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nof\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 200px; height: 50px; margin: 2px; font-size: 16px; color: white; text-align: center; vertical-align: middle\"}\nand\n:::\n\n::: {data-id=\"scatter1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 800px; height: 400px; background-position: left;\"}\n:::\n:::\n\n## Towards computation {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Zipf's Law\n\n::: r-stack\n::: {data-id=\"scatter1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 800px; height: 400px; background-position: left;\"}\n:::\n\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #d98b19; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 18%; left: 20.5%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #b71848; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 40%; left: 20.5%; opacity: 0.5;\"}\n:::\n\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #40666e; width: 20px; height: 20px; border-radius: 20px; position: absolute; top: 45%; left: 20.5%; opacity: 0.5;\"}\n:::\n:::\n\n## Towards computation {auto-animate=\"true\" auto-animate-easing=\"ease-in-out\"}\n\n### Zipf's Law\n\n::: r-stack\n::: {data-id=\"scatter1\" auto-animate-delay=\"0\" style=\"background-image: url('img/img_llms-history/brown_scatter.svg');  background-repeat: no-repeat; background-size: cover; width: 1200px; height: 600px; background-position: left;\"}\n:::\n\n::: {data-id=\"box0\" auto-animate-delay=\"0\" style=\"background: white; width: 500px; height: 200px; margin: 4px; font-size: 20px; color: black; text-align: center; vertical-align: middle; position: absolute; top: 18%; left: 35%;\"}\n-   Zipf's Law: the frequency of a token is inversely proportional to its rank.\n-   Most tokens are infrequent.\n-   The absence of evidence is not evidence of absence.\n:::\n:::\n\n## Towards computation\n\n### While some words may be normally distributed, most are not.\n\n::: columns\n::: {.column width=\"50%\"}\n![](img/img_llms-history/histogram_the.svg)\n:::\n\n::: {.column width=\"50%\"}\n![](img/img_llms-history/histogram_data.svg)\n:::\n:::\n\n# Emergence of machine learning {background-color=\"#40666e\"}\n\n## Emergence of ML\n\n### Let's return to the excerpt from the Google researchers.\n\n::: {style=\"font-size: 90%;\"}\n> *But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus—along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks—if only we knew how to extract the model from the data.*\n:::\n\n::: callout-warning\n## Question\n\nWhat developments are taking place at this time (the early 2000s)?\n:::\n\n## Emergence of ML\n\n-   Word2vec is released. [@mikolov2013efficient]\n    -   Shallow (2-layer) neural network.\n    -   Trained using a relatively small context window (\\~10-12 words).\n    -   Introduces \"embeddings\".\n\n## Emergence of ML\n\n### Embeddings from a vector model.\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Embedding space.\"\n\nreadr::read_csv(\"data/data_llms-history/vector_embeddings.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 14)\n```\n\n## Emergence of ML\n\n### Embeddings from a vector model.\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Dimensions.\"\n\nreadr::read_csv(\"data/data_llms-history/vector_embeddings.csv\", na = \"NA\") |> \n  knitr::kable(\"html\", align = \"r\", table.attr = \"style='width:90%;'\") |>\n  kableExtra::kable_styling(font_size = 14) |>\n  kableExtra::row_spec(0, background = \"#f5b2c6\")\n```\n\n## Emergence of ML {background-image=\"img/img_llms-history/cos_similarity_02.png\" background-size=\"60%\" background-opacity=\".5\"}\n\n### Embeddings from a vector model.\n\n-   When treated as coordinates in space, embeddings locate words that tend to appear together or in similar contexts near each other.\n\n## Emergence of ML\n\n### Embeddings from a vector model.\n\n-   The proximity of words can be assessed using measures like cosine similarity.\n\n$$\ncosine~similarity = S_{c}(A, B) := cos(\\theta) = \\frac{A \\cdot B}{||A||~||B||}\n$$ ![](img/img_llms-history/cos_similarity.png)\n\n## Emergence of ML {background-video=\"img/img_llms-history/vector_model_01.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n-   [An example of a vector model rendered in 3 dimensions]{style=\"color: white; background-color: #e64173; opacity: 0.75;\"} from <https://projector.tensorflow.org/>.\n\n## Emergence of ML {background-image=\"img/img_llms-history/vector_model_02.png\"}\n\n### Embeddings from a vector model.\n\n::: columns\n::: {.column width=\"25%\"}\n::: {style=\"font-size: 18px; padding-top: 10px;\"}\n|     token | similarity |\n|----------:|-----------:|\n|      slow |      0.448 |\n|     quick |      0.519 |\n|    faster |      0.568 |\n|    slower |      0.593 |\n|     speed |      0.602 |\n|      busy |      0.646 |\n|    simple |      0.663 |\n|      food |      0.676 |\n|    speeds |      0.688 |\n|   fastest |      0.688 |\n|      pace |      0.697 |\n| efficient |      0.703 |\n|      easy |      0.707 |\n|     small |      0.710 |\n|       too |      0.712 |\n|  straight |      0.717 |\n|     rapid |      0.717 |\n|       low |      0.718 |\n|   quickly |      0.718 |\n|    packet |      0.718 |\n\n: Tokens closest to *fast*\n:::\n:::\n\n::: {.column width=\"75%\"}\n:::\n:::\n\n## Emergence of ML \n\n::: {style=\"font-size: 70%;\"}\n-   After the introduction of vector representations and, a short time later, the transformer architecture [@vaswani2017attention], language models have rapidly evolved. They can be grouped into roughly 3 generations.\n:::\n\n![](img/img_llms-history/llms_generations.png)\n\n## Emergence of ML\n\n### Advances in LLMs…\n\n-   Allowing for out-of-vocabulary words (using sub-words or word-pieces for tokenizing).\n\n-   Adding a sequence layer.\n\n-   Sliding a context window both left-to-right and right-to-left.\n\n-   Implementing self-attention architecture.\n\n-   Training on more and more data.\n\n-   Expanding the context window (from 512 word-pieces for BERT to 128,000 word-pieces for GPT-4 Turbo 128K).\n\n-   Introducing reinforcement learning from human feedback (RLHF) with Instruct GPT.\n\n## Emergence of ML\n\n### An example of contextual embeddings using BERT\n\n::: {style=\"font-size: 80%;\"}\n```{.python}\n\nsentences = [\"bank\",\n\t\"He eventually sold the shares back to the bank at a premium.\",\n\t\"The bank strongly resisted cutting interest rates.\",\n\t\"The bank will supply and buy back foreign currency.\",\n\t\"The bank is pressing us for repayment of the loan.\",\n\t\"The bank left its lending rates unchanged.\",\n\t\"The river flowed over the bank.\",\n\t\"Tall, luxuriant plants grew along the river bank.\",\n\t\"His soldiers were arrayed along the river bank.\",\n\t\"Wild flowers adorned the river bank.\",\n\t\"Two fox cubs romped playfully on the river bank.\",\n\t\"The jewels were kept in a bank vault.\",\n\t\"You can stow your jewelry away in the bank.\",\n\t\"Most of the money was in storage in bank vaults.\",\n\t\"The diamonds are shut away in a bank vault somewhere.\",\n\t\"Thieves broke into the bank vault.\",\n\t\"Can I bank on your support?\",\n\t\"You can bank on him to hand you a reasonable bill for your services.\",\n\t\"Don't bank on your friends to help you out of trouble.\",\n\t\"You can bank on me when you need money.\",\n\t\"I bank on your help.\"]\n\n```\n:::\n\n## Emergence of ML\n\n### An example of contextual embeddings using BERT\n\n::: {style=\"font-size: 80%;\"}\n```{.python}\n\nfrom collections import OrderedDict\n\ncontext_embeddings = []\ncontext_tokens = []\nfor sentence in sentences:\n\ttokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n\tlist_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n  # make ordered dictionary to keep track of the position of each word\n\ttokens = OrderedDict()\n  # loop over tokens in sensitive sentence\n\tfor token in tokenized_text[1:-1]:\n\t\t# keep track of position of word and whether it occurs multiple times\n\t\tif token in tokens:\n\t\t\ttokens[token] += 1\n\t\telse:\n\t\ttokens[token] = 1\n  \t# compute the position of the current token\n\t\ttoken_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n\t\tcurrent_index = token_indices[tokens[token]-1]\n  \t# get the corresponding embedding\n\t\ttoken_vec = list_token_embeddings[current_index]\n  \t# save values\n\t\tcontext_tokens.append(token)\n\t\tcontext_embeddings.append(token_vec)\n\t\t\n```\n:::\n\n::: aside\nA Colab with the full code is [here](https://colab.research.google.com/drive/1ea3zDFrCQFQhkvinaQfdbvXlOhR7hw01?usp=sharing#scrollTo=4wf0epYMLh22)\n:::\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_01.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_02.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_03.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML {background-video=\"img/img_llms-history/bert_model_04.mp4\" background-video-loop=\"true\" background-video-muted=\"true\"}\n\n## Emergence of ML\n\n### LLMs have a broad range of applications…\n\n-   Generation tasks\n    -   Chat bots\n    -   Content creation\n    -   Summarization\n    -   Translation\n-   Classification tasks\n    -   Text classification\n    -   Segment classification\n\n## Emergence of ML\n\n### Just as they raise questions regarding…\n\n-   The production of content hallucinations [@ji2023survey; @zhang2023language]\n-   Expressions of bias [@santurkar2023opinions]\n-   A tendency to repeat back a user’s stated views (“sycophancy”) [@perez2022discovering]\n\n# Investigating LLM-Generated Text {background-color=\"#40666e\"}\n\n## Investigating LLM-generated text\n\n-  A group in the Statistics & Data Science Department was inspired by claims that were circulating when ChatGPT was first introduced. (e.g., \"Wow! I asked ChatGPT to write a podcast and [it looks pretty good](https://www.npr.org/transcripts/1178290105)!!!\")\n-  We wondered what the text it produces looks like when it is repeated. (e.g., \"What happens if you ask it to write 100 podcasts?\")\n-  We gave it the same writing prompt that students are given in 36-200, generated 100 introduction, and compared those with introductions written by the actual students and introductions that appear in published, data-driven, academic papers.\n-  Then, we tagged the data using Biber's [-@biber1991variation] features (which counts things like [passives, nominalizations, attributive adjectives](https://cmu-textstat-docs.readthedocs.io/en/latest/pseudobibeR/pseudobibeR.html#categories), etc.).\n\n## Investigating LLM-generated text\n\n-   It turns out, machine-authored prose and human-authored prose don’t really look the same in their morphosyntactic and functional features. [@herbold2023large; @markey2024dense]\n\n![Projection of student, published, and ChatGPT-generated writing onto the first two linear discriminants, based on the 67 Biber features.](img/img_llms-history/lda_scatter.svg)\n\n## Investigating LLM-generated text\n\n### Human-generated vs. machine-generated text\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Top text features discriminating between human- and machine-generated writing. Color-coded cells show the average z score of each feature. R2 and p-value correspond to one-way ANOVAs predicting each feature with text type.\"\n\nld1_tbl <- readr::read_csv(\"data/data_llms-history/ld1_tbl.csv\")\n\nld1_tbl <- ld1_tbl |> \n  dplyr::mutate(direction = paste0(\"Features indicating \", direction, \"-generated writing\")) |>\n  dplyr::mutate(variable = stringr::str_remove(variable, \"f_\\\\d+_\")) |>\n  dplyr::mutate(variable = stringr::str_replace_all(variable, \"_\", \" \")) |>\n  dplyr::filter(direction == \"Features indicating human-generated writing\") |>\n  gt::gt(groupname_col = 'direction') |>\n  gt::cols_label(\n    variable = gt::md(\"  \"),\n    ChatGPT = gt::md(\"**ChatGPT<br>n:100**\"),\n    Published = gt::md(\"**Published<br>n:100**\"),\n    Student = gt::md(\"**Student<br>n:100**\"),\n    r.squared = gt::md(\"***R*^2^**\"),\n    p.value = gt::md(\"***p*-value**\")\n  ) |> \n  gt::fmt_number(\n    columns = dplyr::everything(),\n    decimals = 2\n  )  |> \n  gt::data_color(\n    columns = c(ChatGPT:Student),\n    colors = scales::col_numeric(\n      palette = c(\n        \"#FF6666\", \"white\", \"#336699\"),\n      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student), \n                 0, \n                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n      ),\n    locations = gt::cells_body(\n      columns = variable)\n    ) |>\n  gt::tab_options(quarto.use_bootstrap = TRUE)\n\nld1_tbl |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n```\n\n## Implications\n\n### Human-generated vs. machine-generated text\n\n```{r}\n#| echo: false\n#| tbl-cap: \"Top text features discriminating between human- and machine-generated writing. Color-coded cells show the average z score of each feature. R2 and p-value correspond to one-way ANOVAs predicting each feature with text type.\"\n\nld1_tbl <- readr::read_csv(\"data/data_llms-history/ld1_tbl.csv\")\n\nld1_tbl <- ld1_tbl |> \n  dplyr::mutate(direction = paste0(\"Features indicating \", direction, \"-generated writing\")) |>\n  dplyr::mutate(variable = stringr::str_remove(variable, \"f_\\\\d+_\")) |>\n  dplyr::mutate(variable = stringr::str_replace_all(variable, \"_\", \" \")) |>\n  dplyr::filter(direction == \"Features indicating machine-generated writing\") |>\n  gt::gt(groupname_col = 'direction') |>\n  gt::cols_label(\n    variable = gt::md(\"  \"),\n    ChatGPT = gt::md(\"**ChatGPT<br>n:100**\"),\n    Published = gt::md(\"**Published<br>n:100**\"),\n    Student = gt::md(\"**Student<br>n:100**\"),\n    r.squared = gt::md(\"***R*^2^**\"),\n    p.value = gt::md(\"***p*-value**\")\n  ) |> \n  gt::fmt_number(\n    columns = dplyr::everything(),\n    decimals = 2\n  )  |> \n  gt::data_color(\n    columns = c(ChatGPT:Student),\n    colors = scales::col_numeric(\n      palette = c(\n        \"#FF6666\", \"white\", \"#336699\"),\n      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student), \n                 0, \n                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n      ),\n    locations = gt::cells_body(\n      columns = variable)\n    ) |>\n  gt::tab_options(quarto.use_bootstrap = TRUE)\n\nld1_tbl |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n```\n\n## Investigating LLM-generated text\n\n::: {style=\"font-size: 60%;\"}\n-   ChatGPT, for example, produces a more restricted set of modal verbs -- one that is different from both expert and novice writers.\n:::\n\n```{r}\n#| echo: false\n#| warning: false\n#| results: asis\n#| tbl-cap: \"Frequency of different modal verbs, often modulating the confidence of claims, in the different types of writing.\"\n\nmodal_freq <- readr::read_csv(\"data/data_llms-history/modal_freq.csv\")\n\nmodal_freq <- modal_freq |>\n  gt::gt(groupname_col = 'modal_type') |>\n  gt::cols_label(\n    token =  gt::md(\"Modal verb\"),\n    AF_chatgpt =  gt::md(\"ChatGPT\"),\n    AF_published =  gt::md(\"Published\"),\n    AF_student =  gt::md(\"Student\"),\n    RF_chatgpt =  gt::md(\"ChatGPT\"),\n    RF_published =  gt::md(\"Published\"),\n    RF_student =  gt::md(\"Student\"),\n  ) |> \n  gt::tab_spanner(\n    label = \"Absolute Frequency\",\n    columns = c(AF_chatgpt, AF_published, AF_student)\n  ) |>\n  gt::tab_spanner(\n    label =  gt::md(\"Relative Frequency (per 10^5^ words)\"),\n    columns = c(RF_chatgpt, RF_published, RF_student)\n  ) |>\n  gt::fmt_number(\n    columns = c(RF_chatgpt, RF_published, RF_student),\n    decimals = 2\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n    ),\n    locations =  gt::cells_body(\n      columns = token,\n    )\n  )\n\nmodal_freq |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::tab_options(quarto.disable_processing = TRUE,\n                  table.font.size = 14) |>\n  gt::as_raw_html()\n```\n\n## Investigating LLM-generated text\n\n### Human-generated vs. machine-generated text\n\n![Excerpts from texts produced by ChatGPT](img/img_llms-history/gpt_excerpts_01.png){width=80%}\n\n::: aside\nThe LLM has a propensity to condense information into chunked noun phrases, constructed as either *noun* + *noun* or *adjective* + *noun* sequences. Such sequences are sometimes aggregated using the coordinator *and*. On the one hand, these phrases can project a kind of authoritative voice. On the other, their content can range from vague, to ambiguous, to vapid. (What exactly is “a comprehensive dataset encompassing [..] healthcare utilization”?) At issue here is what pertinent information is compressed out of these phrases.\n:::\n\n## Investigating LLM-generated text\n\n::: callout-important\n## Lab Set Question\n\nIf you were working on this project, what would you suggest the team do next? In other words, what limitations do you see in the results of this initial study? What might be done to increase its reliability? Or its generalizability? And what potential challenges do you foresee in applying your suggestions? (Discuss with a couple of your neighbors and write your response)\n:::\n\n## Investigating LLM-generated text\n\n-   We also created an experiment at scale, with 10,000 samples, across 9 text-types, and querying 4 different models (ChatGPT 3.5, ChatGPT 4.0, Llama 3 8B-Base, and Llama 3 8B-Instruct).\n\n```{r}\n#| echo: false\n#| warning: false\n#| tbl-cap: \"A confusion matrix for a classifier pridicting all 4 LLMs and human-generated text.\"\n\ntest_preds <- readr::read_csv(\"data/data_llms-history/test_preds.csv\")\n\ntest_preds <- test_preds |>\n  dplyr::group_by(label, prediction) |>\n  dplyr::summarize(n = dplyr::n(), .groups = \"drop\") |>\n  tidyr::pivot_wider(id_cols = label,\n              names_from = prediction,\n              values_from = n) |>\n  dplyr::mutate(dplyr::across(dplyr::where(is.numeric),\n                ~ .x / (chunk_2 + chunk_gpt3 + chunk_gpt4 + llama_3_8b + llama_3_8b_instruct))) |>\n  dplyr::mutate(label = stringr::str_replace_all(label, \"_\", \" \")) |>\n  dplyr::mutate(label = stringr::str_remove(label, \"chunk \")) |>\n  dplyr::mutate(label = stringr::str_remove(label, \"llama 3 \")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"2\", \"Human\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"gpt\", \"GPT-\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"3\", \"3.5\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"8b\", \"8B-Base\")) |>\n  dplyr::mutate(label = stringr::str_replace(label, \"-Base instruct\", \"-Instruct\")) |>\n  gt::gt() |>\n  gt::fmt_percent(c(chunk_2, chunk_gpt3, chunk_gpt4, llama_3_8b, llama_3_8b_instruct), decimals = 1)  |>\n  gt::cols_label(label = \"Generator\",\n             chunk_2 = \"Human\",\n             chunk_gpt3 = \"GPT-3.5\",\n             chunk_gpt4 = \"GPT-4\",\n             llama_3_8b = \"8B-Base\",\n             llama_3_8b_instruct = \"8B-Instruct\")\n\ntest_preds |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n\n```\n\n## Investigating LLM-generated text\n\n-   Many of the features are the same ones we saw in the smaller study.\n\n```{r}\n#| echo: false\n#| warning: false\n#| tbl-cap: \"The 10 features with the highest importance.\"\n\nfeature_imp <- readr::read_csv(\"data/data_llms-history/feature_importance.csv\")\n\nfeature_imp <- feature_imp |>\n  dplyr::mutate(feature = stringr::str_remove(feature, \"f_\\\\d+_\")) |>\n  dplyr::mutate(feature = stringr::str_replace_all(feature, \"_\", \" \")) |>\n  head(10) |>\n  gt::gt() |>\n  gt::cols_label(feature = \"Feature\",\n             chunk_1 = \"Chunk 1\",\n             chunk_2 = \"Chunk 2\",\n             chunk_gpt3 = \"GPT-3.5\",\n             chunk_gpt4 = \"GPT-4\",\n             llama_3_8b = \"8B-Base\",\n             llama_3_8b_instruct = \"8B-Instruct\",\n             importance = \"Importance\") |>\n  gt::tab_spanner(\"Human\", c(chunk_1, chunk_2)) |>\n  gt::tab_spanner(\"GPT\", chunk_gpt3:chunk_gpt4) |>\n  gt::tab_spanner(\"Llama 3\", llama_3_8b:llama_3_8b_instruct) |>\n  gt::fmt_percent(c(chunk_gpt3, chunk_gpt4, llama_3_8b, llama_3_8b_instruct), decimals = 0) |>\n  gt::fmt_number(c(chunk_1, chunk_2, importance), decimals = 1) |>\n  gt::tab_header(title = \"Features in human- and LLM-written text\",\n             subtitle = \"Rate per 1,000 tokens; LLM rates relative to Chunk 2\") |> \n  gt::data_color(\n    columns = c(chunk_gpt3:llama_3_8b_instruct),\n    direction = \"row\",\n    method = \"numeric\",\n    palette = c(\"#FF6666\", \"white\", \"#336699\"),\n    domain = c(0, 1 ,2),\n    na_color = \"#336699\"\n  ) |>\n  gt::tab_style(\n    style = list(\n      gt::cell_text(style = \"italic\",\n                align = \"right\")\n    ),\n    locations = gt::cells_body(\n      columns = feature,\n    )\n  )\n\nfeature_imp |>\n  gt::opt_table_font(weight = \"bolder\") |>\n  gt::as_raw_html()\n```\n\n## Implications\n\n### Key takeaways\n\n1.  ChatGPT produces sentences that are more informationally dense than that of student writing, but the information density is created using repetitive grammatical patterns.\n2.  Human-generated writing demonstrates more modulation of stress or confidence.\n3.  Broadly, LLMs are not as grammatically nimble as human writers.\n4.  LLM-generated text generally does not produce academic prose that engages with core, disciplinary concepts either in the way that experts do or in the way that novice students do. (Though LLMs can arrive at something more like expert writing with iterative prompting.)\n5.  It is a open question as to the effects of LLMs on the development of students' disciplinary expertise.\n\n## Works Cited\n"},"formats":{"clean-revealjs":{"identifier":{"display-name":"RevealJS","target-format":"clean-revealjs","base-format":"revealjs","extension-name":"clean"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"shortcodes":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"},"slide-level":2,"to":"revealjs","filters":[],"output-file":"lec_llms-history.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"theme":["default","_extensions/grantmcdermott/clean/clean.scss"],"menu":{"side":"left"},"slideNumber":true,"date-format":"[Fall] YYYY","revealjs-plugins":[],"editor":"visual","title":"A Short History of LLMs","subtitle":"Background and introduction to NLP","author":[{"name":"David Brown","orcid":"0000-0001-7745-6354","email":"dwb2@andrew.cmu.edu","affiliations":"Statistics & Data Science 36-468/668"}],"date":"last-modified","title-slide-attributes":{"data-background-image":"img/img_shared/text_analysis.png","data-background-size":"5%","data-background-position":"30% 50%"},"bibliography":["refs/refs_llms-history.bib"],"transition":"fade"}}},"projectFormats":[]}