[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Analysis for Statistics & Data Science",
    "section": "",
    "text": "Quarto is required to process all the assignments in this course. Download and install for your operating system. No other LaTeX installation is needed.\n\n\nSchedule\n\n\n\nTopic\nLabs\nRequired Readings\nOptional Readings\nLecture Slides\n\n\n\n\nLinguistic Facts of Life\n\n\n\n\n\n\nA Short History of LLMs\n\n\n\n\n\n\nClassification & the Federalist Papers",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html",
    "href": "labs/classification-federalist-papers.html",
    "title": "1  Mosteller & Wallace",
    "section": "",
    "text": "1.1 Variables\nBecause of computational limits, they needed to identify potentially productive variables ahead of building their regression model. This is not how we would go about it now, but it was a constraint at the time. They ended up creating 6 bins of likely words, and those are reported in 3 groupings in their study.\nTheir first group contains 70 tokens…\nmw_group1 &lt;- c(\"a\", \"all\", \"also\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"been\", \"but\", \"by\", \"can\", \"do\", \"down\", \"even\", \"every\", \"for\", \"from\", \"had\", \"has\", \"have\", \"her\", \"his\", \"if\", \"in\", \"into\", \"is\", \"it\",  \"its\", \"may\", \"more\", \"must\", \"my\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \"our\", \"shall\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"then\", \"there\", \"things\", \"this\", \"to\", \"up\", \"upon\", \"was\", \"were\", \"what\", \"when\", \"which\", \"who\", \"will\", \"with\", \"would\", \"your\")\nTheir second an additional 47…\nmw_group2 &lt;- c(\"affect\", \"again\", \"although\", \"among\", \"another\", \"because\", \"between\", \"both\", \"city\", \"commonly\", \"consequently\", \"considerable\", \"contribute\", \"defensive\", \"destruction\", \"did\", \"direction\", \"disgracing\", \"either\", \"enough\", \"fortune\", \"function\", \"himself\", \"innovation\", \"join\", \"language\", \"most\", \"nor\", \"offensive\", \"often\", \"pass\", \"perhaps\", \"rapid\", \"same\", \"second\", \"still\", \"those\", \"throughout\", \"under\", \"vigor\", \"violate\", \"violence\", \"voice\", \"where\", \"whether\", \"while\", \"whilst\")\nAnd their third another 48 (though they identify some by lemmas and another “expence” doesn’t appear in our data, possibly because of later editing done in our particular edition)…\nmw_group3 &lt;- c(\"about\", \"according\", \"adversaries\", \"after\", \"aid\", \"always\", \"apt\", \"asserted\", \"before\", \"being\", \"better\", \"care\", \"choice\", \"common\", \"danger\", \"decide\", \"decides\", \"decided\", \"deciding\", \"degree\", \"during\", \"expense\", \"expenses\", \"extent\", \"follow\", \"follows\", \"followed\", \"following\", \"i\", \"imagine\", \"imagined\", \"intrust\", \"intrusted\", \"intrusting\",\"kind\", \"large\", \"likely\", \"matter\", \"matters\", \"moreover\", \"necessary\", \"necessity\", \"necessities\", \"others\", \"particularly\", \"principle\", \"probability\", \"proper\", \"propriety\", \"provision\", \"provisions\", \"requisite\", \"substance\", \"they\", \"though\", \"truth\", \"truths\", \"us\", \"usage\", \"usages\", \"we\", \"work\", \"works\")\nAll together, they list 165 candidate variables, though it works out to be 180 unlemmatized tokens as potential variables for their model.\nWe’ll concatenate a vector of all their variables into a single vector.\nmw_all &lt;- sort(c(mw_group1, mw_group2, mw_group3))",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html#the-federalist-papers",
    "href": "labs/classification-federalist-papers.html#the-federalist-papers",
    "title": "1  Mosteller & Wallace",
    "section": "1.2 The Federalist Papers",
    "text": "1.2 The Federalist Papers\nFor their task, Mosteller & Wallace were interested in solving a long-standing historical debate about the disputed authorship of 12 of the Federalist Papers.\nThe Federalist Papers are made up of 85 articles and essays written by Alexander Hamilton, James Madison, and John Jay under the pseudonym “Publius” to promote the ratification of the United States Constitution.\nAuthorship of the articles has been disputed since their publication, with Hamilton providing a list to his lawyer before his death, and Madison another disputed some of Hamilton’s claims.\nWe’re going to work from the generally accepted authorship designations, which assign authorship of 51 articles to Hamilton, 14 to Madison, 5 to Jay, and 3 to joint authorship. The other 12 are in doubt as to whether they were written by Hamilton or Madison.\nSo let’s begin. First, we’ll get the metadata.\n\nload(\"../data/federalist_meta.rda\")\nload(\"../data/federalist_papers.rda\")\n\n\nfed_meta &lt;- federalist_meta %&gt;%\n  dplyr::select(doc_id, author_id)\n\nAnd we’re going to read in ALL of the data. Why do it this way? We could build out separate data sets for training, validating, and predicting. HOWEVER, we need our data to be identical in structure at every step. This can become tedious if you’re forever wrangling data.frames to get them as the need to be. It’s much easier to begin with one dfm and subset as necessary for the classification process.\nSo let’s read in the text.\n\nfed_txt &lt;- federalist_papers",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html#preparing-the-data",
    "href": "labs/classification-federalist-papers.html#preparing-the-data",
    "title": "1  Mosteller & Wallace",
    "section": "1.3 Preparing the Data",
    "text": "1.3 Preparing the Data\nNow, we’ll tokenize the data.\n\nfed_tokens &lt;- fed_txt %&gt;%\n  corpus() %&gt;%\n  tokens(remove_punct = T, remove_symbols = T, what = \"word\")\n\nAnd create a weighted dfm. The 3rd line preps the column so it can be merged with our metadata. The 4th orders the tokens by their mean frequencies. This isn’t necessary here, but can be useful when doing quick sub-setting of variables. And the 5th changes the column name for easy joining.\n\nfed_dfm &lt;- fed_tokens %&gt;% dfm() %&gt;% dfm_weight(scheme = \"prop\") %&gt;%\n  convert(to = \"data.frame\") %&gt;%\n  select(doc_id, names(sort(colMeans(.[,-1]), decreasing = TRUE)))\n\nNow let’s join the author_id from the metadata.\n\nfed_dfm &lt;- fed_dfm %&gt;% \n  right_join(fed_meta) %&gt;% \n  dplyr::select(doc_id, author_id, everything()) %&gt;% \n  as_tibble()\n\n\n1.3.1 Training and testing data\nNow we can subset out our training and testing data.\n\ntrain_dfm &lt;- fed_dfm %&gt;% filter(author_id == \"Hamilton\" | author_id == \"Madison\")\ntest_dfm &lt;- fed_dfm %&gt;% filter(author_id == \"Disputed\")\n\nFor the next step we’re going to again separate our training data. We want a subset of known data against which we can validate our model.\nFor this, we’ll use some handy functions from the rsample package. First, we make an 80/20 split. From that we create a new, smaller training set, and a validation set.\n\nset.seed(123)\nvalid_split &lt;- initial_split(train_dfm, .8)\ntrain_dfm_v2 &lt;- analysis(valid_split)\ntrain_valid &lt;- assessment(valid_split)\n\nNext, we’ll select only those 70 tokens from Mosteller & Wallace’s first group. We also need to convert the author_id into a 2-level factor, and to move the text_id to row names. The same for the validation data, but we don’t need to worry about the factor conversion.\n\ntrain_dfm_v2_1 &lt;- train_dfm_v2 %&gt;% \n  dplyr::select(doc_id, author_id, all_of(mw_group1)) %&gt;%\n  mutate(doc_id = factor(doc_id)) %&gt;%\n  column_to_rownames(\"doc_id\")\n\ntrain_valid_1 &lt;- train_valid %&gt;% \n  dplyr::select(doc_id, author_id, all_of(mw_group1)) %&gt;%\n  column_to_rownames(\"doc_id\")",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html#lasso",
    "href": "labs/classification-federalist-papers.html#lasso",
    "title": "1  Mosteller & Wallace",
    "section": "1.4 Lasso",
    "text": "1.4 Lasso\nFor our regression, we’re going to take advantage of lasso regression. This is a form of penalized logistic regression, which imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive variables toward zero. This is also known as regularization.\nFor this, we’ll use the glmnet package.\n\nlibrary(glmnet)\n\n\n1.4.1 Ridge & lasso regression\nLeast squares fits a model by minimizing the sum of squared residuals.\n\\[RSS = \\sum_{i=1}^{n}(y_{i}-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_ij)^2\\]\nRidge Regression is similar, but it includes another term.\n\\[\\sum_{i=1}^{n}(y_{i}-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_ij)^2 + \\lambda \\sum_{j=1}^{p}\\beta_{j}^{2} = RSS + \\lambda \\sum_{j=1}^{p}\\beta_{j}^{2}\\]\nIn order to minimize this equation \\(\\beta_1,...\\beta_p\\) should be close to zero and so it shrinks the coefficients. The tuning parameter, \\(\\lambda\\), controls the impact.\nRidge regression does have some disadvantages.\n\nUnlike subset selection, ridge regression includes all p predictors.\nThe penalty term will shrink all of the coefficients towards zero, but none of them will be exactly zero.\nSuch a large model often makes interpretation difficult.\n\nThe lasso helps overcome these problems. It is similar to ridge regression, but the penalty term is slightly different.\n\\[\\sum_{i=1}^{n}(y_{i}-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_ij)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_{j}| = RSS + \\lambda \\sum_{j=1}^{p}|\\beta_{j}|\\]\nLike ridge regression it shrinks the coefficients towards zero. However, the lasso allows some of the coefficients to be exactly zero.\nFor more detail on lasso regression you can look here:\nhttps://eight2late.wordpress.com/2017/07/11/a-gentle-introduction-to-logistic-regression-and-lasso-regularisation-using-r/\nThis is a very useful technique for variable selection and can reduce the likelihood of overfitting. This is particularly helpful in linguistic analysis where we’re often working with many variables making the implementation of functions like step() sometimes tedious.\n\n\n1.4.2 Using glmnet\nTo help you decide which lambda to use, the cv.glmnet() function does cross-validation. The default sets alpha=1 for lasso. If we wanted ridge, we would set alpha=0.\n\ncv_fit &lt;- cv.glmnet(as.matrix(train_dfm_v2_1[, -1]), train_dfm_v2_1[, 1], family = \"binomial\")\n\nWe can plot the log of the resulting lambdas.\n\nplot(cv_fit)\n\n\n\n\n\n\n\n\nThe plot displays the cross-validation error according to the log of lambda. The left dashed vertical line indicates that the log of the optimal value of lambda is approximately -6, which is the one that minimizes the prediction error. This lambda value will give the most accurate model.\nThe exact value of lambda can also be viewed. We’ll save our regression coefficients.\n\nlambda_min &lt;- cv_fit$lambda.min\nlambda_lse &lt;- cv_fit$lambda.1se\n\nBy filtering those variables with coefficients of zero, we see only the variables have been included in the model. Ours has 13.\n\ncoef(cv_fit, s = \"lambda.min\") |&gt;\n  as.matrix() |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"Variable\") |&gt;\n  filter(s1 !=0) |&gt;\n  dplyr::rename(Coeff = s1)  |&gt;\n  gt() |&gt;\n  fmt_number(columns = \"Coeff\",\n             decimals = 2)\n\n\n\n\n\n\n\nVariable\nCoeff\n\n\n\n\n(Intercept)\n5.16\n\n\nand\n109.28\n\n\nat\n−423.42\n\n\nby\n273.33\n\n\ninto\n180.70\n\n\nis\n25.16\n\n\nno\n236.10\n\n\nof\n−40.02\n\n\non\n176.93\n\n\nthere\n−1,205.46\n\n\nthis\n−152.95\n\n\nto\n−137.28\n\n\nup\n−208.23\n\n\nupon\n−1,218.02",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html#validate-the-model",
    "href": "labs/classification-federalist-papers.html#validate-the-model",
    "title": "1  Mosteller & Wallace",
    "section": "1.5 Validate the model",
    "text": "1.5 Validate the model\nTo validate the model, let’s create a model matrix from the texts we’ve split off for that purpose.\n\nx_test &lt;- model.matrix(author_id ~., train_valid_1)[,-1]\n\nFrom our model, we’ll predict the author_id of the validation set.\n\nlasso_prob &lt;- predict(cv_fit, newx = x_test, s = lambda_lse, type = \"response\")\n\nFrom the probabilities, we can return the predicted authors.\n\nlasso_predict &lt;- ifelse(lasso_prob &gt; 0.5, \"Madison\", \"Hamilton\")\n\n\n\nCode\nlasso_predict |&gt; \n  data.frame() |&gt;\n  dplyr::rename(Predict = s1) |&gt;\n  tibble::rownames_to_column(\"Test\") |&gt;\n  gt()\n\n\n\n\n\n\n\n\nTest\nPredict\n\n\n\n\nFEDERALIST_08\nHamilton\n\n\nFEDERALIST_27\nHamilton\n\n\nFEDERALIST_29\nHamilton\n\n\nFEDERALIST_31\nHamilton\n\n\nFEDERALIST_37\nMadison\n\n\nFEDERALIST_45\nMadison\n\n\nFEDERALIST_47\nMadison\n\n\nFEDERALIST_65\nHamilton\n\n\nFEDERALIST_66\nHamilton\n\n\nFEDERALIST_73\nHamilton\n\n\nFEDERALIST_76\nHamilton\n\n\nFEDERALIST_77\nHamilton\n\n\nFEDERALIST_81\nHamilton\n\n\n\n\n\n\n\nRetrieve what they actually are and calculate our model accuracy.\n\ntable(pred=lasso_predict, true=train_valid_1$author_id)\n\n          true\npred       Hamilton Madison\n  Hamilton       10       0\n  Madison         0       3\n\n\n\npaste0(mean(lasso_predict == train_valid_1$author_id)*100, \"%\")\n\n[1] \"100%\"\n\n\nOurs is 100% accurate. Not bad. Note that if you wanted to really test the model, we could create a function to run through this process starting with the sampling.That way, we could generate a range of accuracy over repeated sampling of training and validation data.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html#mosteller-wallaces-experiment",
    "href": "labs/classification-federalist-papers.html#mosteller-wallaces-experiment",
    "title": "1  Mosteller & Wallace",
    "section": "1.6 Mosteller & Wallace’s Experiment",
    "text": "1.6 Mosteller & Wallace’s Experiment\nLet’s repeat the process, but this time we’ll start with all of Mosteller & Wallace’s candidate variables.\n\n1.6.1 Create a new training and validation set\n\ntrain_dfm_v2_2 &lt;- train_dfm_v2 %&gt;% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %&gt;%\n  mutate(author_id = factor(author_id)) %&gt;%\n  column_to_rownames(\"doc_id\")\n\ntrain_valid_2 &lt;- train_valid %&gt;% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %&gt;%\n  column_to_rownames(\"doc_id\")\n\n\n\n1.6.2 Carry out cross-validation\n\ncv_fit &lt;- cv.glmnet(as.matrix(train_dfm_v2_2[, -1]), train_dfm_v2_2[, 1], family = \"binomial\")\n\nLook at our coefficients… 17 this time…\n\ncoef(cv_fit, s = \"lambda.min\") |&gt;\n  as.matrix() |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"Variable\") |&gt;\n  filter(s1 !=0) |&gt;\n  dplyr::rename(Coeff = s1) |&gt;\n  gt() |&gt;\n  fmt_number(columns = \"Coeff\",\n             decimals = 2)\n\n\n\n\n\n\n\nVariable\nCoeff\n\n\n\n\n(Intercept)\n−7.01\n\n\namong\n81.37\n\n\nand\n113.93\n\n\nboth\n631.34\n\n\nby\n308.59\n\n\nconsequently\n691.28\n\n\nfollowed\n407.75\n\n\nkind\n−441.51\n\n\nlanguage\n1,001.80\n\n\non\n287.74\n\n\nparticularly\n1,406.99\n\n\nprobability\n−483.88\n\n\nthere\n−224.77\n\n\nto\n−1.07\n\n\nupon\n−1,503.67\n\n\nvigor\n−1,770.60\n\n\nwhilst\n4,871.85\n\n\nwould\n−22.64\n\n\n\n\n\n\n\nSave our minimum lambda and our regression coefficients.\n\nlambda_min &lt;- cv_fit$lambda.min\nlambda_lse &lt;- cv_fit$lambda.1se\n\n\n\n1.6.3 Create a matrix from the validation set\n\nx_test &lt;- model.matrix(author_id ~., train_valid_2)[,-1]\n\n\n\n1.6.4 Predict the author_id of the validation set.\n\nlasso_prob &lt;- predict(cv_fit, newx = x_test, s = lambda_lse, type = \"response\")\n\n\n\n1.6.5 Return the predicted authors.\n\nlasso_predict &lt;- ifelse(lasso_prob &gt; 0.5, \"Madison\", \"Hamilton\")\n\n\n\n1.6.6 Check confusion matrix\n\n table(pred=lasso_predict, true=train_valid_1$author_id)\n\n          true\npred       Hamilton Madison\n  Hamilton       10       0\n  Madison         0       3\n\n\nThe model looks good… So let’s proceed with the data in question.\n\n\n1.6.7 Prepare full training set\n\ntrain_dfm &lt;- train_dfm %&gt;% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %&gt;%\n  mutate(author_id = factor(author_id)) %&gt;%\n  column_to_rownames(\"doc_id\")\n\n\n\n1.6.8 Prepare test data\n\ntest_dfm &lt;- test_dfm %&gt;% \n  dplyr::select(doc_id, author_id, all_of(mw_all)) %&gt;%\n  column_to_rownames(\"doc_id\")\n\n\n\n1.6.9 Carry out cross-validation\n\ncv_fit &lt;- cv.glmnet(as.matrix(train_dfm[, -1]), train_dfm[, 1], family = \"binomial\")\n\nAs we would expect, this is close to what we saw previously.\n\ncoef(cv_fit, s = \"lambda.min\") |&gt;\n  as.matrix() |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"Variable\") |&gt;\n  filter(s1 !=0) |&gt;\n  dplyr::rename(Coeff = s1) |&gt;\n  gt() |&gt;\n  fmt_number(columns = \"Coeff\",\n             decimals = 2)\n\n\n\n\n\n\n\nVariable\nCoeff\n\n\n\n\n(Intercept)\n−6.75\n\n\nalthough\n210.14\n\n\namong\n248.45\n\n\nand\n132.41\n\n\nboth\n212.07\n\n\nby\n331.66\n\n\nfollowed\n831.92\n\n\nkind\n−487.71\n\n\nlanguage\n167.35\n\n\non\n352.43\n\n\nparticularly\n1,426.48\n\n\nthere\n−342.61\n\n\nto\n−32.91\n\n\nup\n−99.57\n\n\nupon\n−1,112.23\n\n\nvigor\n−1,555.14\n\n\nwhilst\n5,136.21\n\n\nwould\n−52.19\n\n\n\n\n\n\n\n\n\n1.6.10 Run lasso\n\nlasso_fit &lt;- glmnet(as.matrix(train_dfm[, -1]), train_dfm[, 1], alpha = 1, family = \"binomial\", lambda = cv_fit$lambda.min)\n\n\n\n1.6.11 Create a matrix from the test set and predict author\n\nx_test &lt;- model.matrix(author_id ~., test_dfm)[,-1]\nlasso_prob &lt;- predict(cv_fit, newx = x_test, s = lambda_lse, type = \"response\")\nlasso_predict &lt;- ifelse(lasso_prob &gt; 0.5, \"Madison\", \"Hamilton\")\n\n\n\n1.6.12 Check results\n\ndata.frame(lasso_predict, lasso_prob) |&gt;\n  dplyr::rename(Author = s1, Prob = s1.1) |&gt;\n  gt() |&gt;\n  fmt_number(columns = \"Prob\",\n             decimals = 2)\n\n\n\n\n\n\n\nAuthor\nProb\n\n\n\n\nMadison\n0.90\n\n\nMadison\n0.71\n\n\nMadison\n0.99\n\n\nMadison\n0.76\n\n\nMadison\n0.89\n\n\nMadison\n0.62\n\n\nHamilton\n0.26\n\n\nMadison\n0.89\n\n\nMadison\n0.99\n\n\nMadison\n0.68\n\n\nMadison\n0.75\n\n\nMadison\n0.88\n\n\n\n\n\n\n\nOur model predicts all but 55 were written by Madison. Our model is not particularly confident about that result. This hews pretty closely to Mosteller & Wallace’s findings, through they come down (sort of) on the side of Madison for 55. However, they also acknowledge that the evidence is weak and not very convincing.\nIt’s worth noting, too, that other studies using other techniques have suggested that 55 was authored by Hamilton. See, for example, here:\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0054998\n\n\n\n\n\n\nPause for Lab Set Question\n\n\n\nComplete Tasks 1 and 2 in Lab Set 1.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/classification-federalist-papers.html#works-cited",
    "href": "labs/classification-federalist-papers.html#works-cited",
    "title": "1  Mosteller & Wallace",
    "section": "1.7 Works cited",
    "text": "1.7 Works cited\n\n\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an Authorship Problem: A Comparative Study of Discrimination Methods Applied to the Authorship of the Disputed Federalist Papers.” Journal Article. Journal of the American Statistical Association 58 (302): 275–309. https://doi.org/10.1080/01621459.1963.10500849.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mosteller & Wallace</span>"
    ]
  },
  {
    "objectID": "labs/corpus-basics.html",
    "href": "labs/corpus-basics.html",
    "title": "2  NLP Basics",
    "section": "",
    "text": "2.1 A simple processing pipeline\nLet’s begin by creating an object consisting of a character string. In this case, the first sentence from A Tale of Two Cities.\ntotc_txt &lt;- \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\nAnd we’ll load the tidyverse libraries.\nlibrary(gt)\nlibrary(tidyverse)\nWe could then split the vector, say at each space.\ntotc_tkns &lt;- totc_txt %&gt;% str_split(\" \")\nThen, we can create a table of counts.\ntotc_df &lt;- table(totc_tkns) %&gt;% # make a table of counts\n  as_tibble() %&gt;%\n  rename(Token = totc_tkns, AF = n) %&gt;% # rename columns\n  arrange(-AF) # sort the data by frequency\nCode\ntotc_df |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nToken\nAF\n\n\n\n\nof\n10\n\n\nthe\n10\n\n\nwas\n10\n\n\nit\n9\n\n\nage\n2\n\n\nepoch\n2\n\n\nseason\n2\n\n\ntimes,\n2\n\n\nbelief,\n1\n\n\nbest\n1\nThe process of splitting the string vector into constituent parts is called tokenizing. Think of this as telling the computer how to define a word (or a “token”, which is a more precise, technical term). In this case, we’ve done it in an extremely simple way–by defining a token as any string that is bounded by spaces.\nCode\ntotc_df |&gt;\n  filter(str_detect(Token, regex(\"^it$\", ignore_case= T))) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nToken\nAF\n\n\n\n\nit\n9\n\n\nIt\n1\n\n\n\n\n\nCase sensitive counts of the token it\nNote that in doing so, we are counting capitalized and non-capitalized words as distinct tokens.\nThere may be specific instances when we want to do this. But normally, we’d want it and It to be the same token. To do that, we can add a step in the processing pipeline that converts our vector to lower case before tokenizing.\ntotc_df &lt;- tolower(totc_txt) %&gt;%\n  str_split(\" \") %&gt;%\n  table() %&gt;% # make a table of counts\n  as_tibble() %&gt;%\n  rename(Token = \".\", AF = n) %&gt;% # rename columns\n  arrange(-AF) # sort the data by frequency\nCode\ntotc_df |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nToken\nAF\n\n\n\n\nit\n10\n\n\nof\n10\n\n\nthe\n10\n\n\nwas\n10\n\n\nage\n2\n\n\nepoch\n2\n\n\nseason\n2\n\n\ntimes,\n2\n\n\nbelief,\n1\n\n\nbest\n1\n\n\n\n\n\nToken counts of sample sentence.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>NLP Basics</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html",
    "href": "labs/processing-pipelines.html",
    "title": "3  Tokenizing with quanteda",
    "section": "",
    "text": "3.1 Create a corpus\nThe first step is to create a corpus object:\ntotc_corpus &lt;- corpus(totc_txt)\nAnd see what we have:\nCode\ntotc_corpus |&gt;\n  summary() |&gt;\n  gt()\n\n\n\n\n\n\nSummary of a corpus.\n\n\nText\nTypes\nTokens\nSentences\n\n\n\n\ntext1\n23\n70\n1\nNote that if we had more than 1 document, we would get a count of how many documents in which the token appear, and that we can assign documents to grouping variable. This will become useful later.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#tokenize-the-corpus",
    "href": "labs/processing-pipelines.html#tokenize-the-corpus",
    "title": "3  Tokenizing with quanteda",
    "section": "3.2 Tokenize the corpus",
    "text": "3.2 Tokenize the corpus\n\ntotc_tkns &lt;- tokens(totc_corpus, what = \"word\", remove_punct = TRUE)",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#create-a-document-feature-matrix-dfm",
    "href": "labs/processing-pipelines.html#create-a-document-feature-matrix-dfm",
    "title": "3  Tokenizing with quanteda",
    "section": "3.3 Create a document-feature matrix (dfm)",
    "text": "3.3 Create a document-feature matrix (dfm)\n\ntotc_dfm &lt;- dfm(totc_tkns)\n\nA dfm is an important data structure to understand, as it often serves as the foundation for all kinds of downstream statistical processing. It is a table with rows for documents (or observations) and columns for tokens (or variables)\n\n\nCode\ntotc_dfm |&gt;\n  convert(to = \"data.frame\") |&gt;\n  dplyr::select(1:12) |&gt;\n  gt()\n\n\n\n\n\n\nPart of a document-feature matrix.\n\n\ndoc_id\nit\nwas\nthe\nbest\nof\ntimes\nworst\nage\nwisdom\nfoolishness\nepoch\n\n\n\n\ntext1\n10\n10\n10\n1\n10\n2\n1\n2\n1\n1\n2",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#and-count-our-tokens",
    "href": "labs/processing-pipelines.html#and-count-our-tokens",
    "title": "3  Tokenizing with quanteda",
    "section": "3.4 And count our tokens",
    "text": "3.4 And count our tokens\n\n\nCode\ntotc_dfm |&gt;\n  textstat_frequency() |&gt;\n  gt()\n\n\n\n\n\n\nToken counts of sample sentence.\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\nit\n10\n1\n1\nall\n\n\nwas\n10\n1\n1\nall\n\n\nthe\n10\n1\n1\nall\n\n\nof\n10\n1\n1\nall\n\n\ntimes\n2\n5\n1\nall\n\n\nage\n2\n5\n1\nall\n\n\nepoch\n2\n5\n1\nall\n\n\nseason\n2\n5\n1\nall\n\n\nbest\n1\n9\n1\nall\n\n\nworst\n1\n9\n1\nall\n\n\nwisdom\n1\n9\n1\nall\n\n\nfoolishness\n1\n9\n1\nall\n\n\nbelief\n1\n9\n1\nall\n\n\nincredulity\n1\n9\n1\nall\n\n\nlight\n1\n9\n1\nall\n\n\ndarkness\n1\n9\n1\nall\n\n\nspring\n1\n9\n1\nall\n\n\nhope\n1\n9\n1\nall\n\n\nwinter\n1\n9\n1\nall\n\n\ndespair\n1\n9\n1\nall",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#using-pipes-to-expidite-the-process",
    "href": "labs/processing-pipelines.html#using-pipes-to-expidite-the-process",
    "title": "3  Tokenizing with quanteda",
    "section": "3.5 Using pipes to expidite the process",
    "text": "3.5 Using pipes to expidite the process\nThis time, we will change remove_punct to FALSE.\n\ntotc_freq &lt;- totc_corpus %&gt;%\n  tokens(what = \"word\", remove_punct = FALSE) %&gt;%\n  dfm() %&gt;%\n  textstat_frequency()\n\n\n\nCode\ntotc_freq |&gt;\n  gt()\n\n\n\n\n\n\nToken counts of sample sentence.\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\nit\n10\n1\n1\nall\n\n\nwas\n10\n1\n1\nall\n\n\nthe\n10\n1\n1\nall\n\n\nof\n10\n1\n1\nall\n\n\n,\n9\n5\n1\nall\n\n\ntimes\n2\n6\n1\nall\n\n\nage\n2\n6\n1\nall\n\n\nepoch\n2\n6\n1\nall\n\n\nseason\n2\n6\n1\nall\n\n\nbest\n1\n10\n1\nall\n\n\nworst\n1\n10\n1\nall\n\n\nwisdom\n1\n10\n1\nall\n\n\nfoolishness\n1\n10\n1\nall\n\n\nbelief\n1\n10\n1\nall\n\n\nincredulity\n1\n10\n1\nall\n\n\nlight\n1\n10\n1\nall\n\n\ndarkness\n1\n10\n1\nall\n\n\nspring\n1\n10\n1\nall\n\n\nhope\n1\n10\n1\nall\n\n\nwinter\n1\n10\n1\nall\n\n\ndespair\n1\n10\n1\nall\n\n\n.\n1\n10\n1\nall",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#tokenizing-options",
    "href": "labs/processing-pipelines.html#tokenizing-options",
    "title": "3  Tokenizing with quanteda",
    "section": "3.6 Tokenizing options",
    "text": "3.6 Tokenizing options\nIn the previous lab, you were asked to consider the questions: What counts as a token/word? And how do you tell the computer to count what you want?\nAs the above code block suggest, the tokens() function in quanteda gives you some measure on control.\nWe’ll read in a more complex string:\n\ntext_2 &lt;- \"Jane Austen was not credited as the author of 'Pride and Prejudice.' In 1813, the title page simply read \\\"by the author of Sense and Sensibility.\\\" It wasn't until after Austen's death that her identity was revealed. #MentalFlossBookClub with @HowLifeUnfolds #15Pages https://pbs.twimg.com/media/EBOUqbfWwAABEoj.jpg\"\n\nAnd process it as we did earlier.\n\ntext_2_freq &lt;- text_2 %&gt;%\n  corpus() %&gt;%\n  tokens(what = \"word\", remove_punct = TRUE) %&gt;%\n  dfm() %&gt;%\n  textstat_frequency()\n\n\n\nCode\ntext_2_freq |&gt;\n  gt()\n\n\n\n\n\n\nToken counts of sample Tweet\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\nthe\n3\n1\n1\nall\n\n\nwas\n2\n2\n1\nall\n\n\nauthor\n2\n2\n1\nall\n\n\nof\n2\n2\n1\nall\n\n\nand\n2\n2\n1\nall\n\n\njane\n1\n6\n1\nall\n\n\nausten\n1\n6\n1\nall\n\n\nnot\n1\n6\n1\nall\n\n\ncredited\n1\n6\n1\nall\n\n\nas\n1\n6\n1\nall\n\n\npride\n1\n6\n1\nall\n\n\nprejudice\n1\n6\n1\nall\n\n\nin\n1\n6\n1\nall\n\n\n1813\n1\n6\n1\nall\n\n\ntitle\n1\n6\n1\nall\n\n\npage\n1\n6\n1\nall\n\n\nsimply\n1\n6\n1\nall\n\n\nread\n1\n6\n1\nall\n\n\nby\n1\n6\n1\nall\n\n\nsense\n1\n6\n1\nall\n\n\nsensibility\n1\n6\n1\nall\n\n\nit\n1\n6\n1\nall\n\n\nwasn't\n1\n6\n1\nall\n\n\nuntil\n1\n6\n1\nall\n\n\nafter\n1\n6\n1\nall\n\n\nausten's\n1\n6\n1\nall\n\n\ndeath\n1\n6\n1\nall\n\n\nthat\n1\n6\n1\nall\n\n\nher\n1\n6\n1\nall\n\n\nidentity\n1\n6\n1\nall\n\n\nrevealed\n1\n6\n1\nall\n\n\n#mentalflossbookclub\n1\n6\n1\nall\n\n\nwith\n1\n6\n1\nall\n\n\n@howlifeunfolds\n1\n6\n1\nall\n\n\n#15pages\n1\n6\n1\nall\n\n\nhttps://pbs.twimg.com/media/ebouqbfwwaabeoj.jpg\n1\n6\n1\nall\n\n\n\n\n\n\n\nNote that in addition to various logical “remove” arguments (remove_punct, remove_symbols, etc.), the tokens() function has a what argument. The default, “word”, is “smarter”, but also slower. Another option is “fastestword”, which splits at spaces.\n\ntext_2_freq &lt;- text_2 %&gt;%\n  corpus() %&gt;%\n  tokens(what = \"fastestword\", remove_punct = TRUE, remove_url = TRUE) %&gt;%\n  dfm() %&gt;%\n  textstat_frequency()  %&gt;%\n  as_tibble() %&gt;%\n  dplyr::select(feature, frequency)\n\n\n\nCode\ntext_2_freq |&gt;\n  gt()\n\n\n\n\n\n\nToken counts of sample Tweet\n\n\nfeature\nfrequency\n\n\n\n\nthe\n3\n\n\nwas\n2\n\n\nauthor\n2\n\n\nof\n2\n\n\nand\n2\n\n\njane\n1\n\n\nausten\n1\n\n\nnot\n1\n\n\ncredited\n1\n\n\nas\n1\n\n\n'pride\n1\n\n\nprejudice.'\n1\n\n\nin\n1\n\n\n1813,\n1\n\n\ntitle\n1\n\n\npage\n1\n\n\nsimply\n1\n\n\nread\n1\n\n\n\"by\n1\n\n\nsense\n1\n\n\nsensibility.\"\n1\n\n\nit\n1\n\n\nwasn't\n1\n\n\nuntil\n1\n\n\nafter\n1\n\n\nausten's\n1\n\n\ndeath\n1\n\n\nthat\n1\n\n\nher\n1\n\n\nidentity\n1\n\n\nrevealed.\n1\n\n\n#mentalflossbookclub\n1\n\n\nwith\n1\n\n\n@howlifeunfolds\n1\n\n\n#15pages\n1\n\n\n\n\n\n\n\nThis, of course, makes no difference with just a few tokens, but does if you’re trying to process millions.\nAlso note that we’ve used the select() function to choose specific columns.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#pre-processing",
    "href": "labs/processing-pipelines.html#pre-processing",
    "title": "3  Tokenizing with quanteda",
    "section": "3.7 Pre-processing",
    "text": "3.7 Pre-processing\nAn alternative to making tokenizing decisions inside the tokenizing process, you can process the text before tokenizing using functions for manipulating strings in stringr, stringi, textclean, or base R (like grep( )). Some common and convenient transformations are wrapped in a function called preprocess_text( )\n\ntext_2_freq &lt;- text_2 %&gt;%\n  preprocess_text() %&gt;%\n  corpus() %&gt;%\n  tokens(what = \"fastestword\") %&gt;%\n  dfm() %&gt;%\n  textstat_frequency() %&gt;%\n  as_tibble() %&gt;%\n  dplyr::select(feature, frequency) %&gt;%\n  rename(Token = feature, AF = frequency) %&gt;%\n  mutate(New = NA)\n\n\n\nCode\ntext_2_freq |&gt;\n  gt()\n\n\n\n\n\n\nToken counts of sample Tweet\n\n\nToken\nAF\nNew\n\n\n\n\nwas\n3\nNA\n\n\nthe\n3\nNA\n\n\nausten\n2\nNA\n\n\nauthor\n2\nNA\n\n\nof\n2\nNA\n\n\nand\n2\nNA\n\n\njane\n1\nNA\n\n\nnot\n1\nNA\n\n\ncredited\n1\nNA\n\n\nas\n1\nNA\n\n\npride\n1\nNA\n\n\nprejudice\n1\nNA\n\n\nin\n1\nNA\n\n\n1813\n1\nNA\n\n\ntitle\n1\nNA\n\n\npage\n1\nNA\n\n\nsimply\n1\nNA\n\n\nread\n1\nNA\n\n\nby\n1\nNA\n\n\nsense\n1\nNA\n\n\nsensibility\n1\nNA\n\n\nit\n1\nNA\n\n\nn't\n1\nNA\n\n\nuntil\n1\nNA\n\n\nafter\n1\nNA\n\n\ns\n1\nNA\n\n\ndeath\n1\nNA\n\n\nthat\n1\nNA\n\n\nher\n1\nNA\n\n\nidentity\n1\nNA\n\n\nrevealed\n1\nNA\n\n\nmentalflossbookclub\n1\nNA\n\n\nwith\n1\nNA\n\n\nhowlifeunfolds\n1\nNA\n\n\n15pages\n1\nNA\n\n\nhttpspbs.twimg.com/media/ebouqbfwwaabeoj.jpg\n1\nNA\n\n\n\n\n\n\n\nNote how the default arguments treat negation and possessive markers. As with the tokens () function, many of these (options)[http://htmlpreview.github.io/?https://raw.githubusercontent.com/browndw/quanteda.extras/main/vignettes/preprocess_introduction.html] are logical.\nNote, too, that we’ve renamed the columns and added a new one using mutate().\n\n\n\n\n\n\nPause for Lab Set Question\n\n\n\nComplete Task 1 in Lab Set 1.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#creating-a-corpus-composition-table",
    "href": "labs/processing-pipelines.html#creating-a-corpus-composition-table",
    "title": "3  Tokenizing with quanteda",
    "section": "3.8 Creating a corpus composition table",
    "text": "3.8 Creating a corpus composition table\nWhenever you report the results of a corpus-based analysis, it is best practice to include a table that summarizes the composition of your corpus (or corpora) and any relevant variables. Most often this would include token counts aggregated by relevant categorical variables and a row of totals.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/processing-pipelines.html#adding-a-grouping-variable",
    "href": "labs/processing-pipelines.html#adding-a-grouping-variable",
    "title": "3  Tokenizing with quanteda",
    "section": "3.9 Adding a grouping variable",
    "text": "3.9 Adding a grouping variable\nWe have 2 short texts (one from fiction and one from Twitter). Let’s first combine them into a single corpus. First, a data frame is created that has 2 columns (doc_id and text). Then, the text column is passed to the preprocess_text() function before creating the corpus.\n\ncomb_corpus &lt;-   data.frame(doc_id = c(\"text_1\", \"text_2\"), text = c(totc_txt, text_2)) %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus()\n\nNext well assign a grouping variable using docvars(). In later labs, we’ll use a similar process to assign variables from tables of metadata.\n\ndocvars(comb_corpus) &lt;- data.frame(text_type = c(\"Fiction\", \"Twitter\"))\n\nNow we can tokenize.\n\ncomb_tkns &lt;- comb_corpus %&gt;%\n  tokens(what = \"fastestword\")\n\nOnce we have done this, we can use that grouping variable to manipulate the data in a variety of ways. We could use dfm_group() to aggregate by group instead of individual text. (Though because we have only 2 texts here, it amounts to the same thing.)\n\ncomb_dfm &lt;- dfm(comb_tkns) %&gt;% \n  dfm_group(groups = text_type)\n\ncorpus_comp &lt;- ntoken(comb_dfm) %&gt;%\n  data.frame(frequency = .) %&gt;%\n  rownames_to_column(\"group\") %&gt;%\n  group_by(group) %&gt;%\n  summarize(Texts = n(),\n            Tokens = sum(frequency))\n\n\n\nCode\ncorpus_comp |&gt; \n  gt() |&gt;\n  fmt_integer() |&gt;\n  cols_label(\n    group = md(\"**Text Type**\"),\n    Texts = md(\"**Texts**\"),\n    Tokens = md(\"**Tokens**\")\n  ) |&gt;\n  grand_summary_rows(\n    columns = c(Texts, Tokens),\n    fns = list(\n      Total ~ sum(.)\n    ) ,\n    fmt = ~ fmt_integer(.)\n    )\n\n\n\n\nTable 3.1: Composition of corpus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Type\nTexts\nTokens\n\n\n\n\n\nFiction\n1\n60\n\n\n\nTwitter\n1\n44\n\n\nTotal\n—\n2\n104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause for Lab Set Question\n\n\n\nComplete Task 2 in Lab Set 1.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tokenizing with quanteda</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html",
    "href": "labs/distributions.html",
    "title": "4  Distributions",
    "section": "",
    "text": "4.1 Prepare a corpus",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#prepare-a-corpus",
    "href": "labs/distributions.html#prepare-a-corpus",
    "title": "4  Distributions",
    "section": "",
    "text": "4.1.1 Load the needed packages\n\nlibrary(gt)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(tidyverse)\n\n\n\n4.1.2 Load a corpus\nThe repository comes with some data sets. The conventional way to format text data prior to processing is as a table with a column of document ids (which correspond to the the file names) and a column of texts. Such a table is easy to create from text data on your own local drive using the package readtext.\n\nload(\"../data/sample_corpus.rda\")\nload(\"../data/multiword_expressions.rda\")\n\nTo peek at the data, we’ll look at the first 100 characters in the “text” column of the first row:\n\n\nCode\nsample_corpus |&gt;\n  mutate(text = substr(text, start = 1, stop = 100)) |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\n\n\ndoc_id\ntext\n\n\n\n\nacad_01\nTeachers and other school personnel are often counseled to use research findings in making curricula\n\n\nacad_02\nAbstract Does the conflict in El Salvador, conceptualized by the U.S. government as a battle in the\n\n\nacad_03\nJanuary 17, 1993, will mark the 100th anniversary of the deposing of the Hawaiian monarchy. \"Prior t\n\n\nacad_04\nThirty years have passed since the T1961 meeting of the National Council for the Social Studies in C\n\n\nacad_05\nABSTRACT -- A common property resource with open access, such as a fishery, will be used to excess w\n\n\nacad_06\nDespite some encouraging signs and hopeful expectations that democracy has made reasonable progress\n\n\nacad_07\nevaluation component. Similarly, Stewart ( 1982,1989 ) has shown that a common response to unfamilia\n\n\nacad_08\nSection: Education \"A lab is where you do science\" ( Thornton 1972 ). An investigative laboratory (\n\n\nacad_09\nIn 1968, the thirtieth anniversary issue of the Journal of Politics celebrated the great advance in\n\n\nacad_10\nmonologue -- and of Novas Calvo's story -- may thus be clarified, not as that of facilitating an obj\n\n\n\n\n\n\n\n\n\n4.1.3 Load functions\nThe repository also contains a number of useful functions. Here, will load some that will calculate a number of common dispersion measures.\n\nsource(\"../R/dispersion_functions.R\")\n\n\n\n4.1.4 Create and corpus\nMake a corpus object.\n\nsc &lt;- corpus(sample_corpus)\n\nAnd check the result:\n\n\nCode\nsc |&gt;\n  summary() |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\nTable 4.1: Partial summary of sample corpus.\n\n\n\n\n\n\n\n\n\nText\nTypes\nTokens\nSentences\n\n\n\n\nacad_01\n842\n2818\n95\n\n\nacad_02\n983\n2845\n88\n\n\nacad_03\n968\n2885\n126\n\n\nacad_04\n1017\n2864\n102\n\n\nacad_05\n914\n2837\n109\n\n\nacad_06\n1007\n2813\n86\n\n\nacad_07\n663\n2952\n92\n\n\nacad_08\n870\n2830\n118\n\n\nacad_09\n980\n2899\n131\n\n\nacad_10\n1118\n2883\n77",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#document-variables-name-your-files-systematically",
    "href": "labs/distributions.html#document-variables-name-your-files-systematically",
    "title": "4  Distributions",
    "section": "4.2 Document variables (Name your files systematically!)",
    "text": "4.2 Document variables (Name your files systematically!)\n\n\n\n\n\n\nImportant\n\n\n\nFile names can encode important meta-data. In this case, the names include text-types, much like the Corpus of Contemporary American English.\nThis is extremely important. When you build your own corpora, you want to purposefully and systematically name your files and organize your directories. This will save you time and effort later in your analysis.\n\n\nWe are now going to extract the meta-data from the file names and pass them as a variable.\n\ndoc_categories &lt;- str_extract(sample_corpus$doc_id, \"^[a-z]+\")\n\nCheck the result:\n\n\n\n\n\n\nDocument categories.\n\n\ndoc_cats\n\n\n\n\nacad\n\n\nblog\n\n\nfic\n\n\nmag\n\n\nnews\n\n\nspok\n\n\ntvm\n\n\nweb\n\n\n\n\n\n\n\nWe will now assign the variable to the corpus. The following command might look backwards, with the function on the left hand side of the &lt;- operator. That is because it’s an accessor function, which lets us add or modify data in an object. You can tell when a function is an accessor function like this because its help file will show that you can use it with &lt;-, for example in ?docvars.\n\ndocvars(sc, field = \"text_type\") &lt;- doc_categories\n\nAnd check the summary again:\n\n\nCode\nsc |&gt;\n  summary() |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\nPartial summary of sample corpus.\n\n\nText\nTypes\nTokens\nSentences\ntext_type\n\n\n\n\nacad_01\n842\n2818\n95\nacad\n\n\nacad_02\n983\n2845\n88\nacad\n\n\nacad_03\n968\n2885\n126\nacad\n\n\nacad_04\n1017\n2864\n102\nacad\n\n\nacad_05\n914\n2837\n109\nacad\n\n\nacad_06\n1007\n2813\n86\nacad\n\n\nacad_07\n663\n2952\n92\nacad\n\n\nacad_08\n870\n2830\n118\nacad\n\n\nacad_09\n980\n2899\n131\nacad\n\n\nacad_10\n1118\n2883\n77\nacad\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAssigning docvars is based entirely on ordering. In other words, you are simply attaching a vector of categories to the corpus object. There is no merging by shared keys. Thus, you always need to be sure that your docvars are in the same order as your doc_ids. This is the reason why we extracted them directly from the doc_ids.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#tokenize-the-corpus",
    "href": "labs/distributions.html#tokenize-the-corpus",
    "title": "4  Distributions",
    "section": "4.3 Tokenize the corpus",
    "text": "4.3 Tokenize the corpus\nWe’ll use quanteda to tokenize. And after tokenization, we’ll convert them to lower case. Why do that here? As a next step, we’ll being combining tokens like a and lot into single units. And we’ll be using a list of expressions that isn’t case sensitive.\n\nsc_tokens &lt;- tokens(sc, include_docvars=TRUE, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, what = \"word\")\n\nsc_tokens &lt;- tokens_tolower(sc_tokens)",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#multi-word-expressions",
    "href": "labs/distributions.html#multi-word-expressions",
    "title": "4  Distributions",
    "section": "4.4 Multi-word Expressions",
    "text": "4.4 Multi-word Expressions\nAn issue that we run into frequently with corpus analysis is what to do with multi-word expressions. For example, consider a common English quantifier: “a lot”. Typical tokenization rules will split this into two tokens: a and lot. But counting a lot as a single unit might be important depending on our task. We have a way of telling quanteda to account for these tokens.\nAll that we need is a list of multi-word expressions.\nThe cmu.textstat comes with an example of an mwe list called multiword_expressions:\n\n\n\nExamples of multi-word expressions.\n\n\nwinter haven\n\n\nwith a view to\n\n\nwith reference to\n\n\nwith regard to\n\n\nwith relation to\n\n\nwith respect to\n\n\n\n\n\nThe tokens_compound() function looks for token sequences that match our list and combines them using an underscore.\n\nsc_tokens &lt;- tokens_compound(sc_tokens, pattern = phrase(multiword_expressions))",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#create-a-document-feature-matrix",
    "href": "labs/distributions.html#create-a-document-feature-matrix",
    "title": "4  Distributions",
    "section": "4.5 Create a document-feature matrix",
    "text": "4.5 Create a document-feature matrix\nWith our tokens object we can now create a document-feature-matrix using the dfm() function. As a reminder, a dfm is table with one row per document in the corpus, and one column per unique token in the corpus. Each cell contains a count of how many times a token shows up in that document.\n\nsc_dfm &lt;- dfm(sc_tokens)\n\nNext we’ll create a dfm with proportionally weighted counts.\n\nprop_dfm &lt;- dfm_weight(sc_dfm, scheme = \"prop\")",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#token-distributions",
    "href": "labs/distributions.html#token-distributions",
    "title": "4  Distributions",
    "section": "4.6 Token distributions",
    "text": "4.6 Token distributions\n\n4.6.1 Distributions of the\nLet’s start by selecting frequencies of the most common token in the corpus:\n\nfreq_df &lt;- textstat_frequency(sc_dfm) %&gt;%\n  data.frame(stringsAsFactors = F)\n\n\n\nCode\nfreq_df |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\nThe 10 most frequent tokens in the sample corpus.\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\nthe\n50920\n1\n399\nall\n\n\nand\n25232\n2\n398\nall\n\n\nto\n24753\n3\n397\nall\n\n\nof\n22060\n4\n399\nall\n\n\na\n21614\n5\n398\nall\n\n\nin\n15969\n6\n399\nall\n\n\ni\n12568\n7\n348\nall\n\n\nthat\n12537\n8\n396\nall\n\n\nyou\n10951\n9\n341\nall\n\n\nis\n9901\n10\n389\nall\n\n\n\n\n\n\n\nFrom the weighted dfm, we can select any token that we’d like to look at more closely. In this case, we’ll select the most frequent token: the.\nAfter selecting the variable, we will convert the data into a more friendly data structure.\nThere are easier ways of doing this, but the first bit of the code-chunk allows us to filter by rank and return a character vector that we can pass. This way, we can find a word of any arbitrary rank.\nAlso note how the rename() function is set up. Let’s say our token is the. The dfm_select() function would result with a column named the that we’d want to rename RF. So our typical syntax would be: rename(RF = the). In the chunk below, however, our column name is the variable word. To pass that variable to rename, we use !!name(word).\n\nword &lt;- freq_df %&gt;% \n  filter(rank == 1) %&gt;% \n  dplyr::select(feature) %&gt;%\n  as.character()\n\nword_df &lt;- dfm_select(prop_dfm, word, valuetype = \"fixed\") # select the token\n\nword_df &lt;- word_df %&gt;% \n  convert(to = \"data.frame\") %&gt;% \n  cbind(docvars(word_df)) %&gt;% \n  rename(RF = !!as.name(word)) %&gt;% \n  mutate(RF = RF*1000000)\n\nWith that data it is a simple matter to generate basic summary statistics using the group_by() function:\n\nsummary_table &lt;- word_df %&gt;% \n  group_by(text_type) %&gt;%\n  summarize(MEAN = mean(RF),\n              SD = sd(RF),\n              N = n())\n\n\n\nCode\nsummary_table |&gt;\n  gt()\n\n\n\n\n\n\nMeans and standard deviations by text-type.\n\n\ntext_type\nMEAN\nSD\nN\n\n\n\n\nacad\n68619.64\n18327.10\n50\n\n\nblog\n51270.54\n13389.38\n50\n\n\nfic\n54202.41\n14254.79\n50\n\n\nmag\n57117.76\n13986.30\n50\n\n\nnews\n50574.84\n18333.09\n50\n\n\nspok\n42693.44\n9727.79\n50\n\n\ntvm\n32532.85\n11981.88\n50\n\n\nweb\n60592.68\n21641.35\n50\n\n\n\n\n\n\n\nAnd we can inspect a histogram of the frequencies. To set the width of our bins we’ll use the Freedman-Diaconis rule. The bin-width is set to: \\[h = 2 x \\frac{IQR(x)}{n^{1/3}}\\]\nSo the number of bins is (max-min)/h, where n is the number of observations, max is the maximum value and min is the minimum value.\n\nbin_width &lt;- function(x){\n  2 * IQR(x) / length(x)^(1/3)\n  }\n\nNow we can plot a histogram. We’re also adding a dashed line showing the mean. Note we’re also going to use the scales package to remove scientific notation from our tick labels.\n\nggplot(word_df,aes(RF)) + \n  geom_histogram(binwidth = bin_width(word_df$RF), colour=\"black\", fill=\"white\", linewidth=.25) +\n  geom_vline(aes(xintercept=mean(RF)), color=\"red\", linetype=\"dashed\", linewidth=.5) +\n  theme_classic() +\n  scale_x_continuous(labels = scales::comma) +\n  xlab(\"RF (per mil. words)\")\n\n\n\n\nHistogram of the token the\n\n\n\n\n\n\n4.6.2 Distributions of the and of\nNow let’s try plotting histograms of two tokens on the same plot. First we’re going to use regular expressions to select the columns. The carat or hat ^ looks for the start of line. Without it, we would also get words like “blather”. The dollar symbol $ looks for the end of a line. The straight line | means OR. Think about how useful this flexibility can be. You could, for example, extract all words that end in -ion.\n\n# Note \"regex\" rather than \"fixed\"\nword_df &lt;- dfm_select(prop_dfm, \"^the$|^of$\", valuetype = \"regex\")\n\n# Now we'll convert our selection and normalize to 10000 words.\nword_df &lt;- word_df %&gt;% \n  convert(to = \"data.frame\") %&gt;%\n  mutate(the = the*10000) %&gt;%\n  mutate(of = of*10000)\n\n# Use \"pivot_longer\" to go from a wide format to a long one\nword_df &lt;- word_df %&gt;% \n  pivot_longer(!doc_id, names_to = \"token\", values_to = \"RF\") %&gt;% \n  mutate(token = factor(token))\n\nNow let’s make a new histogram. Here we assign the values of color and fill to the “token” column. We also make the columns a little transparent using the “alpha” setting.\n\nggplot(word_df,aes(x = RF, color = token, fill = token)) + \n  geom_histogram(binwidth = bin_width(word_df$RF), alpha=.5, position = \"identity\") +\n  theme_classic() +\n  xlab(\"RF (per mil. words)\") +\n  theme(axis.text = element_text(size=5))\n\n\n\n\nHistogram of the tokens the and of.\n\n\n\n\nIf we don’t want overlapping histograms, we can use facet_wrap() to split the plots.\n\nggplot(word_df,aes(x = RF, color = token, fill = token)) + \n  geom_histogram(binwidth = bin_width(word_df$RF), alpha=.5, position = \"identity\") +\n  theme_classic() +\n  theme(axis.text = element_text(size=5)) +\n  theme(legend.position = \"none\") +\n  xlab(\"RF (per mil. words)\") +\n  facet_wrap(~ token)\n\n\n\n\nHistogram of the tokens the and of.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#dispersion",
    "href": "labs/distributions.html#dispersion",
    "title": "4  Distributions",
    "section": "4.7 Dispersion",
    "text": "4.7 Dispersion\nWe can also calculate dispersion, and there are a variety of measures at our disposal. Our toolkit has several functions for producing these calculations.\nFor example, we can find the dispersion of any specific token:\n\nthe &lt;- dispersions_token(sc_dfm, \"the\") %&gt;% unlist()\n\n\n\n\nDispersion measures for the token .\n\n\nAbsolute frequency\n50920.000\n\n\nPer_10.5\n5240.015\n\n\nRelative entropy of all sizes of the corpus parts\n1.000\n\n\nRange\n399.000\n\n\nMaxmin\n292.000\n\n\nStandard deviation\n45.926\n\n\nVariation coefficient\n0.361\n\n\nChi-square\n6295.422\n\n\nJuilland et al.’s D (based on equally-sized corpus parts)\n0.982\n\n\nJuilland et al.’s D (not requiring equally-sized corpus parts)\n0.982\n\n\nCarroll’s D2\n0.989\n\n\nRosengren’s S (based on equally-sized corpus parts)\n0.963\n\n\nRosengren’s S (not requiring equally-sized corpus parts)\n0.966\n\n\nLyne’s D3 (not requiring equally-sized corpus parts)\n0.968\n\n\nDistributional consistency DC\n0.963\n\n\nInverse document frequency IDF\n0.004\n\n\nEngvall’s measure\n50792.700\n\n\nJuilland et al.’s U (based on equally-sized corpus parts)\n50000.333\n\n\nJuilland et al.’s U (not requiring equally-sized corpus parts)\n50010.750\n\n\nCarroll’s Um (based on equally sized corpus parts)\n50342.268\n\n\nRosengren’s Adjusted Frequency (based on equally sized corpus parts)\n49032.932\n\n\nRosengren’s Adjusted Frequency (not requiring equally sized corpus parts)\n49185.689\n\n\nKromer’s Ur\n2135.564\n\n\nDeviation of proportions DP\n0.139\n\n\nDeviation of proportions DP (normalized)\n0.139\n\n\n\n\n\nAnd let’s try another token to compare:\n\ndata &lt;- dispersions_token(sc_dfm, \"data\") %&gt;% unlist()\n\n\n\n\nDeviation of Proportions for the tokens and .\n\n\n\nthe\ndata\n\n\n\n\nDeviation of proportions DP\n0.139\n0.846\n\n\n\n\n\n\n\n\n4.7.1 Dispersions for all tokens\nWe can also calculate selected dispersion measures for all tokens using dispersions_all():\n\nd &lt;- dispersions_all(sc_dfm)\n\n\n\n\nDispersion measures for all tokens.\n\n\nToken\nAF\nPer_10.5\nCarrolls_D2\nRosengrens_S\nLynes_D3\nDC\nJuillands_D\nDP\nDP_norm\n\n\n\n\nthe\n50920\n5240.015\n0.989\n0.966\n0.968\n0.963\n0.982\n0.139\n0.139\n\n\nand\n25232\n2596.545\n0.990\n0.970\n0.973\n0.967\n0.983\n0.123\n0.124\n\n\nto\n24753\n2547.252\n0.993\n0.980\n0.983\n0.976\n0.987\n0.090\n0.090\n\n\nof\n22060\n2270.124\n0.978\n0.933\n0.935\n0.932\n0.974\n0.199\n0.199\n\n\na\n21614\n2224.228\n0.992\n0.977\n0.978\n0.973\n0.986\n0.109\n0.109\n\n\nin\n15969\n1643.319\n0.988\n0.962\n0.963\n0.960\n0.981\n0.146\n0.146",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#generating-a-frequency-table",
    "href": "labs/distributions.html#generating-a-frequency-table",
    "title": "4  Distributions",
    "section": "4.8 Generating a frequency table",
    "text": "4.8 Generating a frequency table\nAlternatively, frequency_table() returns only Deviation of Proportions and Average Reduced Frequency.\nNote that ARF requires a tokens object and takes a couple of minutes to calculate.\n\nft &lt;- frequency_table(sc_tokens)\n\n\n\n\nFrequency and dispersion measures for all tokens.\n\n\n\nToken\nAF\nPer_10.5\nARF\nDP\n\n\n\n\n1\nthe\n50920\n5240.015\n31901.106\n0.139\n\n\n2\nand\n25232\n2596.545\n15914.149\n0.123\n\n\n3\nto\n24753\n2547.252\n15468.494\n0.090\n\n\n5\nof\n22060\n2270.124\n13089.728\n0.199\n\n\n4\na\n21614\n2224.228\n13239.704\n0.109\n\n\n6\nin\n15969\n1643.319\n9772.267\n0.146",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/distributions.html#zipfs-law",
    "href": "labs/distributions.html#zipfs-law",
    "title": "4  Distributions",
    "section": "4.9 Zipf’s Law",
    "text": "4.9 Zipf’s Law\nLet’s plot rank against frequency for the 100 most frequent tokens in the sample corpus.\n\nggplot(freq_df %&gt;% filter(rank &lt; 101), aes(x = rank, y = frequency)) +\n  geom_point(shape = 1, alpha = .5) +\n  theme_classic() +\n  ylab(\"Absolute frequency\") +\n  xlab(\"Rank\")\n\n\n\n\nToken rank vs. frequency.\n\n\n\n\nThe relationship you’re seeing between the rank of a token and it’s frequency holds true for almost any corpus and is referred to as Zipf’s Law (see Brezina pg. 44).",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "labs/collocations.html",
    "href": "labs/collocations.html",
    "title": "5  Collocations & Association Measures",
    "section": "",
    "text": "5.1 Load the needed packages\nlibrary(ggraph)\nlibrary(gt)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(tidyverse)\nLoad data:\nload(\"../data/sample_corpus.rda\")\nLoad functions:\nsource(\"../R/helper_functions.R\")\nsource(\"../R/utility_functions.R\")\nsource(\"../R/collocation_functions.R\")",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collocations & Association Measures</span>"
    ]
  },
  {
    "objectID": "labs/collocations.html#prepare-the-data",
    "href": "labs/collocations.html#prepare-the-data",
    "title": "5  Collocations & Association Measures",
    "section": "5.2 Prepare the data",
    "text": "5.2 Prepare the data\nFirst, we’ll pre-process our text, create a corpus and tokenize the data:\n\nsc_tokens &lt;- sample_corpus %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus() %&gt;%\n  tokens(what=\"fastestword\", remove_numbers=TRUE)",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collocations & Association Measures</span>"
    ]
  },
  {
    "objectID": "labs/collocations.html#collocates-by-mutual-information-mi",
    "href": "labs/collocations.html#collocates-by-mutual-information-mi",
    "title": "5  Collocations & Association Measures",
    "section": "5.3 Collocates by mutual information (MI)",
    "text": "5.3 Collocates by mutual information (MI)\nThe collocates_by_MI( ) function produces collocation measures (by pointwise mutual information) for a specified token in a quanteda tokens object. In addition to a token, a span or window (as given by a number of words to the left and right of the node word) is required. The default is 5 to the left and 5 to the right.\nThe formula for calculating MI is as follows:\n\\[log_{2} \\frac{O_{11}}{E_{11}}\\] Where O11 and E11 are the observed (i.e., node + collocate) and expected frequencies of the node word within a given window. The expected frequency is given by:\n\\[E_{11} = \\frac{R_{1} \\times C_{1}}{N}\\]\n\nN is the number of words in the corpus\nR1 is the frequency of the node in the whole corpus\nC1 is the frequency of the collocate in the whole corpus\n\nWe’ll start by making a table of tokens that collocate with the token money.\n\nmoney_collocations &lt;- collocates_by_MI(sc_tokens, \"money\")\n\nCheck the result:\n\n\n\n\n\n\n\n\ntoken\ncol_freq\ntotal_freq\nMI_1\n\n\n\n\n10:29\n1\n1\n11.08049\n\n\n38th\n1\n1\n11.08049\n\n\nallocations\n1\n1\n11.08049\n\n\namericanizing\n1\n1\n11.08049\n\n\nanthedon\n1\n1\n11.08049\n\n\nassignats\n1\n1\n11.08049\n\n\nbamboozling\n1\n1\n11.08049\n\n\nbaser\n1\n1\n11.08049\n\n\nborrowers\n1\n1\n11.08049\n\n\nbridegrooms\n1\n1\n11.08049\n\n\n\n\n\n\n\nNow, let’s make a similar table for collocates of time.\n\ntime_collocations &lt;- collocates_by_MI(sc_tokens, \"time\")\n\n\n\n\n\n\n\n\n\ntoken\ncol_freq\ntotal_freq\nMI_1\n\n\n\n\ndecleat\n2\n1\n10.135473\n\n\npoignantly\n2\n1\n10.135473\n\n\n16a\n1\n1\n9.135473\n\n\n17a\n1\n1\n9.135473\n\n\n21h\n1\n1\n9.135473\n\n\naba\n1\n1\n9.135473\n\n\nablution\n1\n1\n9.135473\n\n\nabnegate\n1\n1\n9.135473\n\n\nadmonitions\n1\n1\n9.135473\n\n\naguada\n1\n1\n9.135473\n\n\n\n\n\n\n\nAs is clear from the above table, MI is sensitive to rare/infrequent words. Because of that sensitivity, it is common to make thresholds for both token frequency (absolute frequency) and MI score (usually at some value \\(\\ge\\) 3).\nFor our purposes, we’ll filter for AF \\(\\ge\\) 5 and MI \\(\\ge\\) 5.\n\ntc &lt;- time_collocations %&gt;% filter(col_freq &gt;= 5 & MI_1 &gt;= 5)\nmc &lt;- money_collocations %&gt;% filter(col_freq &gt;= 5 & MI_1 &gt;= 5)\n\nCheck the result:\n\n\n\n\n\n\n\n\n\n\n\n\nTime collocations\n\n\ntoken\ncol_freq\ntotal_freq\nMI_1\n\n\n\n\nwarner\n6\n8\n8.720435\n\n\ncessation\n5\n7\n8.650046\n\n\nirradiation\n5\n7\n8.650046\n\n\nlag\n5\n7\n8.650046\n\n\nwasting\n7\n11\n8.483396\n\n\nframe\n7\n16\n7.942828\n\n\nperiods\n5\n14\n7.650046\n\n\nspent\n34\n122\n7.292199\n\n\nspend\n26\n111\n7.041497\n\n\nwaste\n11\n58\n6.736924\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoney collocations.\n\n\ntoken\ncol_freq\ntotal_freq\nMI_1\n\n\n\n\nowe\n5\n21\n9.010102\n\n\nraise\n10\n79\n8.098639\n\n\nextra\n6\n64\n7.665454\n\n\nspend\n10\n111\n7.608004\n\n\ninsurance\n5\n64\n7.402420\n\n\nspent\n9\n122\n7.319679\n\n\namount\n6\n109\n6.897270\n\n\nmaking\n14\n343\n6.465782\n\n\ncost\n6\n154\n6.398668\n\n\nbuy\n5\n150\n6.173601",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collocations & Association Measures</span>"
    ]
  },
  {
    "objectID": "labs/collocations.html#create-a-tbl_graph-object-for-plotting",
    "href": "labs/collocations.html#create-a-tbl_graph-object-for-plotting",
    "title": "5  Collocations & Association Measures",
    "section": "5.4 Create a tbl_graph object for plotting",
    "text": "5.4 Create a tbl_graph object for plotting\nA tbl_graph is a data structure for tidyverse (ggplot2) network plotting.\nFor this, we’ll use the col_network( ) function.\n\nnet &lt;- col_network(tc, mc)\n\n\n\n\n\n\n\nPause for Lab Set Question\n\n\n\nComplete Task 1 Lab Set 2.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collocations & Association Measures</span>"
    ]
  },
  {
    "objectID": "labs/collocations.html#plot-network",
    "href": "labs/collocations.html#plot-network",
    "title": "5  Collocations & Association Measures",
    "section": "5.5 Plot network",
    "text": "5.5 Plot network\nThe network plot shows the tokens that distinctly collocate with either time or money, as well as those that intersect. The distance from the central tokens (time and money) is governed by the MI score and the transparency (or alpha) is governed by the token frequency.\nThe aesthetic details of the plot can be manipulated in the various ggraph options.\n\nggraph(net, weight = link_weight, layout = \"stress\") + \n  geom_edge_link(color = \"gray80\", alpha = .75) + \n  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +\n  geom_node_text(aes(label = label), repel = T, size = 3) +\n  scale_alpha(range = c(0.2, 0.9)) +\n  theme_graph() +\n  theme(legend.position=\"none\")",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collocations & Association Measures</span>"
    ]
  },
  {
    "objectID": "labs/collocations.html#reading-in-local-files",
    "href": "labs/collocations.html#reading-in-local-files",
    "title": "5  Collocations & Association Measures",
    "section": "5.6 Reading in local files",
    "text": "5.6 Reading in local files\n\n5.6.1 Create a vector of file paths\nIn the interest of time, we’ll skip this step. However, it’s important to know how to load in plain text files.\n\nCreate an organized directory of .txt files. Name them systematically, so it’s easy to extract metadata from the file names. As an example, a corpus of screenplays is on our course Canvas site. You could download the file, unzip it, and place it in your data directory.\nTo down-sample the data, create a vector of the file paths. Remember to replace your path with the place-holder path in the list.files() function.\nRead in the files using readtext. And for the purposes of efficiency, we’ll sample out 50 rows.\n\n\nset.seed(1234)\n\nfiles_list &lt;- list.files(\"../data/screenplay_corpus\", full.names = T, pattern = \"*.txt\")\n\nsp &lt;- sample(files_list, 50) %&gt;%\n  readtext::readtext()\n\n\n\n5.6.2 Extract the dialogue\nFor the purposes of the lab, we’ll simply load the data. These particular files are formatted using some simple markup. So we’ll use the from_play() function to extract the dialogue.\n\nload(\"../data/screenplays.rda\")\nsp &lt;- from_play(sp, extract = \"dialogue\")\n\n\n\n5.6.3 Tokenize\n\nsp &lt;-   sp %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus() %&gt;%\n  tokens(what=\"fastestword\", remove_numbers=TRUE)\n\n\n\n5.6.4 Calculate MI\nNow we’ll calculate collocations for the tokens boy and girl, and filter. Note that we’re only looking for tokens 3 words to the left of the node word.\n\nb &lt;- collocates_by_MI(sp, \"boy\", left = 3, right = 0) %&gt;% \n  filter(col_freq &gt;= 3 & MI_1 &gt;= 3)\n\ng  &lt;- collocates_by_MI(sp, \"girl\", left = 3, right = 0) %&gt;% \n  filter(col_freq &gt;= 3 & MI_1 &gt;= 3)\n\n\n\n5.6.5 Plot the network\n\nnet &lt;- col_network(b, g)\n\nggraph(net, weight = link_weight, layout = \"stress\") + \n  geom_edge_link(color = \"gray80\", alpha = .75) + \n  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +\n  geom_node_text(aes(label = label), repel = T, size = 3) +\n  scale_alpha(range = c(0.2, 0.9)) +\n  theme_graph() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause for Lab Set Question\n\n\n\nComplete Task 2 Lab Set 2.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collocations & Association Measures</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html",
    "href": "labs/keyness.html",
    "title": "6  Keyness",
    "section": "",
    "text": "6.1 What is keyness?\nKeyness is a generic term for various tests that compare observed vs. expected frequencies.\nThe most commonly used (though not the only option) is called log-likelihood in corpus linguistics, but you will see it else where called a G-test goodness-of-fit.\nThe calculation is based on a 2 x 2 contingency table. It is similar to a chi-square test, but performs better when corpora are unequally sized.\nExpected frequencies are based on the relative size of each corpus (in total number of words Ni) and the total number of observed frequencies:\n\\[\nE_i = \\sum_i O_i \\times \\frac{N_i}{\\sum_i N_i}\n\\] And log-likelihood is calculted according the formula:\n\\[\nLL = 2 \\times \\sum_i O_i \\ln \\frac{O_i}{E_i}\n\\] A good explanation of its implementation in linguistics can be found here: http://ucrel.lancs.ac.uk/llwizard.html\nIn addition to log-likelihood, the textstat_keyness() function in quanteda has other optional measures.\nSee here: https://quanteda.io/reference/textstat_keyness.html",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#prepare-a-corpus",
    "href": "labs/keyness.html#prepare-a-corpus",
    "title": "6  Keyness",
    "section": "6.2 Prepare a corpus",
    "text": "6.2 Prepare a corpus\nWe’ll begin, just as we did in the distributions lab.\n\n6.2.1 Load the needed packages\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(gt)\n\nLoad the functions:\n\nsource(\"../R/keyness_functions.R\")\nsource(\"../R/helper_functions.R\")\n\nLoad the data:\n\nload(\"../data/sample_corpus.rda\")\nload(\"../data/multiword_expressions.rda\")\n\n\n\n6.2.2 Pre-process the data & create a corpus\n\nsc &lt;- sample_corpus %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus()\n\n\n\n6.2.3 Extract meta-data from file names\nWe’ll extract some meta-data by (1) selecting the doc_id column, (2) extracting the initial letter string before the underscore, and (3) renaming the vector text_type.\n\ndoc_categories &lt;- sample_corpus %&gt;%\n  dplyr::select(doc_id) %&gt;%\n  mutate(doc_id = str_extract(doc_id, \"^[a-z]+\")) %&gt;%\n  rename(text_type = doc_id)\n\n\n\n6.2.4 Assign the meta-data to the corpus\nThe accessor function docvars() lets us add or modify data in an object. We’re going to use it to assign text_type as a variable. Note that doc_categories could include more than one column and the assignment process would be the same.\n\ndocvars(sc) &lt;- doc_categories\n\nAnd check the result:\n\n\nCode\nsc |&gt;\n  summary() |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\nPartial summary of sample corpus.\n\n\nText\nTypes\nTokens\nSentences\ntext_type\n\n\n\n\nacad_01\n772\n2534\n1\nacad\n\n\nacad_02\n933\n2544\n1\nacad\n\n\nacad_03\n889\n2525\n1\nacad\n\n\nacad_04\n941\n2541\n1\nacad\n\n\nacad_05\n857\n2504\n1\nacad\n\n\nacad_06\n962\n2575\n1\nacad\n\n\nacad_07\n615\n2443\n1\nacad\n\n\nacad_08\n805\n2540\n1\nacad\n\n\nacad_09\n912\n2544\n1\nacad\n\n\nacad_10\n1063\n2532\n1\nacad\n\n\n\n\n\n\n\nNote the new column (text_type on the right). We could assign any number of categorical variables to our corpus, which could be used for analysis downstream.\n\n\n6.2.5 Create a dfm\n\nsc_dfm &lt;- sc %&gt;%\n  tokens(what=\"fastestword\", remove_numbers=TRUE) %&gt;%\n  tokens_compound(pattern = phrase(multiword_expressions)) %&gt;%\n  dfm()",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#a-corpus-composition-table",
    "href": "labs/keyness.html#a-corpus-composition-table",
    "title": "6  Keyness",
    "section": "6.3 A corpus composition table",
    "text": "6.3 A corpus composition table\nIt is conventional to report out the composition of the corpus or corpora you are using for your study. Here will will sum our tokens by text-type and similarly count the number of texts in each grouping.\n\ncorpus_comp &lt;- ntoken(sc_dfm) %&gt;% \n  data.frame(Tokens = .) %&gt;%\n  rownames_to_column(\"Text_Type\") %&gt;%\n  mutate(Text_Type = str_extract(Text_Type, \"^[a-z]+\")) %&gt;%\n  group_by(Text_Type) %&gt;%\n  summarize(Texts = n(),\n    Tokens = sum(Tokens)) %&gt;%\n  mutate(Text_Type = c(\"Academic\", \"Blog\", \"Fiction\", \"Magazine\", \"News\", \"Spoken\", \"Television/Movies\", \"Web\"))\n\nNow, using grand_summary_rows(), we can append a row of totals at the bottom of the table.\n\n\nCode\ncorpus_comp |&gt; \n  gt() |&gt;\n  fmt_integer() |&gt;\n  cols_label(\n    Text_Type = md(\"**Text Type**\"),\n    Texts = md(\"**Texts**\"),\n    Tokens = md(\"**Tokens**\")\n  ) |&gt;\n  grand_summary_rows(\n    columns = c(Texts, Tokens),\n    fns = list(\n      Total ~ sum(.)\n    ) ,\n    fmt = ~ fmt_integer(.)\n    )\n\n\n\n\n\n\nComposition of the sample corpus.\n\n\n\n\n\n\n\n\n\nText Type\nTexts\nTokens\n\n\n\n\n\nAcademic\n50\n121,442\n\n\n\nBlog\n50\n125,492\n\n\n\nFiction\n50\n128,644\n\n\n\nMagazine\n50\n126,631\n\n\n\nNews\n50\n119,029\n\n\n\nSpoken\n50\n127,156\n\n\n\nTelevision/Movies\n50\n128,191\n\n\n\nWeb\n50\n124,302\n\n\nTotal\n—\n400\n1,000,887",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#creating-sub-corpora",
    "href": "labs/keyness.html#creating-sub-corpora",
    "title": "6  Keyness",
    "section": "7.1 Creating sub-corpora",
    "text": "7.1 Creating sub-corpora\nIf we want to compare one text-type (as our target corpus) to another (as our reference corpus), we can easily subset the data.\n\nsub_dfm &lt;- dfm_subset(sc_dfm, text_type == \"acad\" | text_type == \"fic\")\n\nWhen we do this, the resulting data will still include all the tokens in the sample corpus, including those that do not appear in either the academic or fiction text-type. To deal with this, we will trim the dfm.\n\nsub_dfm &lt;- dfm_trim(sub_dfm, min_termfreq = 1)\n\nWe’ll do the same for fiction.\n\nfic_kw &lt;- textstat_keyness(sub_dfm, docvars(sub_dfm, \"text_type\") == \"fic\", measure = \"lr\")\n\n\n\nCode\nfic_kw |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  fmt_number(columns = \"G2\",\n             decimals = 2)\n\n\n\n\n\n\nTokens with the highest keyness values in the fiction text-type when compared to the academic text-type.\n\n\nfeature\nG2\np\nn_target\nn_reference\n\n\n\n\ni\n2,350.21\n0\n2428\n143\n\n\nshe\n1,861.48\n0\n1763\n70\n\n\nhe\n1,699.10\n0\n1978\n170\n\n\nher\n1,453.01\n0\n1559\n104\n\n\nyou\n1,361.11\n0\n1286\n50\n\n\nn't\n929.29\n0\n914\n43\n\n\nhis\n805.06\n0\n1155\n157\n\n\nmy\n732.26\n0\n758\n44\n\n\nme\n591.50\n0\n557\n21\n\n\nhim\n541.51\n0\n548\n29\n\n\n\n\n\n\n\nNote that if we switch our target and reference corpora (academic as target, fiction as reference), the tail of the keyness table contains the negative values of the original (fiction as target, academic and reference), which you may have already gathered given the formula above.\n\nacad_kw &lt;- textstat_keyness(sub_dfm, docvars(sub_dfm, \"text_type\") == \"acad\", measure = \"lr\")\n\n\n\nCode\nacad_kw |&gt;\n  tail(10) |&gt;\n  gt() |&gt;\n  fmt_number(columns = \"G2\",\n             decimals = 2)\n\n\n\n\n\n\nTokens with the lowest keyness values int the academic text-type when compared to the fiction text-type.\n\n\nfeature\nG2\np\nn_target\nn_reference\n\n\n\n\nhim\n−541.51\n0\n29\n548\n\n\nme\n−591.50\n0\n21\n557\n\n\nmy\n−732.26\n0\n44\n758\n\n\nhis\n−805.06\n0\n157\n1155\n\n\nn't\n−929.29\n0\n43\n914\n\n\nyou\n−1,361.11\n0\n50\n1286\n\n\nher\n−1,453.01\n0\n104\n1559\n\n\nhe\n−1,699.10\n0\n170\n1978\n\n\nshe\n−1,861.48\n0\n70\n1763\n\n\ni\n−2,350.21\n0\n143\n2428",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#log-ratio-lr",
    "href": "labs/keyness.html#log-ratio-lr",
    "title": "6  Keyness",
    "section": "8.1 Log Ratio (LR)",
    "text": "8.1 Log Ratio (LR)\nYou are welcome to use any of these effect size measures. Our repo comes with a function for calculating Hardie’s Log Ratio, which is easy and intuitive.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#the-keyness_table-function",
    "href": "labs/keyness.html#the-keyness_table-function",
    "title": "6  Keyness",
    "section": "8.2 The keyness_table(* function",
    "text": "8.2 The keyness_table(* function\nWe’ll start by creating 2 dfms–a target and a reference:\n\nacad_dfm &lt;- dfm_subset(sc_dfm, text_type == \"acad\") %&gt;% dfm_trim(min_termfreq = 1)\nfic_dfm &lt;- dfm_subset(sc_dfm, text_type == \"fic\") %&gt;% dfm_trim(min_termfreq = 1)\n\nThen we will use the keyness_table() function.\n\nacad_kw &lt;- keyness_table(acad_dfm, fic_dfm)\n\nAnd check the result:\n\n\nCode\nacad_kw |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  fmt_number(columns = c(\"LL\", \"LR\", \"Per_10.5_Tar\", \"Per_10.5_Ref\", \"DP_Tar\", \"DP_Ref\"),\n             decimals = 2) |&gt;\n  fmt_number(columns = \"PV\",\n             decimals = 5)\n\n\n\n\n\n\nTokens with the highest keyness values in the academic text-type when compared to the fiction text-type.\n\n\nToken\nLL\nLR\nPV\nAF_Tar\nAF_Ref\nPer_10.5_Tar\nPer_10.5_Ref\nDP_Tar\nDP_Ref\n\n\n\n\nof\n1,225.77\n1.25\n0.00000\n4848\n2153\n3,992.03\n1,673.61\n0.09\n0.15\n\n\nthe\n250.03\n0.37\n0.00000\n8273\n6768\n6,812.31\n5,261.03\n0.11\n0.10\n\n\nsocial\n248.10\n4.98\n0.00000\n208\n7\n171.28\n5.44\n0.65\n0.88\n\n\nare\n221.19\n1.44\n0.00000\n707\n276\n582.17\n214.55\n0.21\n0.30\n\n\nstudies\n213.17\n7.36\n0.00000\n155\n1\n127.63\n0.78\n0.67\n0.98\n\n\nby\n209.00\n1.29\n0.00000\n790\n342\n650.52\n265.85\n0.17\n0.22\n\n\nin\n185.82\n0.58\n0.00000\n2715\n1922\n2,235.64\n1,494.05\n0.11\n0.09\n\n\nstudents\n179.36\n5.27\n0.00000\n146\n4\n120.22\n3.11\n0.80\n0.94\n\n\nresearch\n175.19\n4.74\n0.00000\n151\n6\n124.34\n4.66\n0.63\n0.90\n\n\nis\n171.74\n0.87\n0.00000\n1241\n720\n1,021.89\n559.68\n0.23\n0.36\n\n\n\n\n\n\n\nThe columns are as follows:\n\nLL: the keyness value or log-likelihood, also know as a G2 or goodness-of-fit test.\nLR: the effect size, which here is the log ratio\nPV: the p-value associated with the log-likelihood\nAF_Tar: the absolute frequency in the target corpus\nAF_Ref: the absolute frequency in the reference corpus\nPer_10.x_Tar: the relative frequency in the target corpus (automatically calibrated to a normalizing factor, where here is per 100,000 tokens)\nPer_10.x_Ref: the relative frequency in the reference corpus (automatically calibrated to a normalizing factor, where here is per 100,000 tokens)\nDP_Tar: the deviation of proportions (a dispersion measure) in the target corpus\nDP_Ref: the deviation of proportions in the reference corpus",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#keyness-pairs",
    "href": "labs/keyness.html#keyness-pairs",
    "title": "6  Keyness",
    "section": "8.3 Keyness pairs",
    "text": "8.3 Keyness pairs\nThere is also a function for quickly generating pair-wise keyness comparisions among multiple sub-corpora. To demonstrate, create a third dfm, this time containing news articles.\n\nnews_dfm &lt;- dfm_subset(sc_dfm, text_type == \"news\") %&gt;% dfm_trim(min_termfreq = 1)\n\nTo produce a data.frame comparing more than two sup-corpora, use the keyness_pairs( ) function:\n\nkp &lt;- keyness_pairs(news_dfm, acad_dfm, fic_dfm)\n\nCheck the result:\n\n\nCode\nkp |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  fmt_number(everything(),\n             decimals = 2)\n\n\n\n\n\n\nPairwise comparisions of news (target) vs. academic (reference), news (target) vs. fiction (reference), and academic (target) vs. fiction (reference).\n\n\nToken\nA_v_B_LL\nA_v_B_LR\nA_v_B_PV\nA_v_C_LL\nA_v_C_LR\nA_v_C_PV\nB_v_C_LL\nB_v_C_LR\nB_v_C_PV\n\n\n\n\nhe\n492.01\n2.32\n0.00\n−394.56\n−1.13\n0.00\n−1,686.80\n−3.46\n0.00\n\n\nsaid\n455.34\n3.69\n0.00\n1.94\n0.13\n0.16\n−414.33\n−3.56\n0.00\n\n\ni\n430.97\n2.36\n0.00\n−853.61\n−1.65\n0.00\n−2,330.44\n−4.00\n0.00\n\n\nn't\n333.06\n3.23\n0.00\n−173.20\n−1.10\n0.00\n−926.44\n−4.33\n0.00\n\n\nyou\n327.51\n3.06\n0.00\n−410.98\n−1.54\n0.00\n−1,355.34\n−4.60\n0.00\n\n\nmr\n236.58\n5.10\n0.00\n75.25\n1.61\n0.00\n−60.92\n−3.48\n0.00\n\n\npark\n226.44\n8.36\n0.00\n139.47\n3.20\n0.00\n−25.26\n−5.16\n0.00\n\n\nshe\n212.56\n2.36\n0.00\n−919.21\n−2.21\n0.00\n−1,850.64\n−4.57\n0.00\n\n\np.m\n209.56\n8.25\n0.00\n199.71\n6.33\n0.00\n−2.66\n−1.92\n0.10\n\n\nob\n198.31\n8.17\n0.00\n206.63\n8.25\n0.00\n0.00\n0.08\n1.00",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/keyness.html#works-cited",
    "href": "labs/keyness.html#works-cited",
    "title": "6  Keyness",
    "section": "9.1 Works cited",
    "text": "9.1 Works cited\n\n\n\n\nGabrielatos, Costas, and Anna Marchi. 2011. “Keyness: Matching Metrics to Definitions.” In Theoretical-Methodological Challenges in Corpus Approaches to Discourse Studies and Some Ways of Addressing Them. https://research.edgehill.ac.uk/en/publications/keyness-matching-metrics-to-definitions-2.\n\n\nJohnston, Janis E, Kenneth J Berry, and Paul W Mielke Jr. 2006. “Measures of Effect Size for Chi-Squared and Likelihood-Ratio Goodness-of-Fit Tests.” Perceptual and Motor Skills 103 (2): 412–14.\n\n\nWilson, Andrew. 2013. “Embracing Bayes Factors for Key Item Analysis in Corpus Linguistics.” New Approaches to the Study of Linguistic Variability 4: 3–11.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "labs/feature-engineering.html",
    "href": "labs/feature-engineering.html",
    "title": "7  Part-of-speech tagging and dependency parsing",
    "section": "",
    "text": "7.1 What does udpipe do?\nBefore we start processing in R, let’s get some sense of what “universal dependency parsing” is and what its output looks like.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part-of-speech tagging and dependency parsing</span>"
    ]
  },
  {
    "objectID": "labs/feature-engineering.html#what-does-udpipe-do",
    "href": "labs/feature-engineering.html#what-does-udpipe-do",
    "title": "7  Part-of-speech tagging and dependency parsing",
    "section": "",
    "text": "7.1.1 Parse a sample sentence online\nGo to this webpage: http://lindat.mff.cuni.cz/services/udpipe/.\nAnd paste the following sentence into the text field:\n\nThe company offers credit cards, loans and interest-generating accounts.\n\nThen, click the “Process Input” button. You should now see an output. If you choose the “Table” tab, you can view the output in a tablular format.\n\n\n7.1.2 Basic parse structure\nThere is a column for the token and one for the token’s base form or lemma.\nThose are followed by a tag for the general lexical class or “universal part-of-speech” (upos) tag, and a tree-bank specific (xpos) part-of-speech tag.\nThe xpos tags are Penn Treebank tags, which you can find here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\nThe part-of-speech tags are followed by a column of integers that refer to the id of the token that is at the head of the dependency structure, which is followed by the dependency relation identifier.\nFor a list of all dependency abbreviaitons see here: https://universaldependencies.org/u/dep/index.html.\n\n\n7.1.3 Visualize the dependency\nFrom the “Output Text” tab, copy the output start with the sent_id including the pound sign\nPaste the information into the text field here: https://urd2.let.rug.nl/~kleiweg/conllu/. Then click the “Submit Query” button below the text field. This should generate a visualization of the dependency structure.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part-of-speech tagging and dependency parsing</span>"
    ]
  },
  {
    "objectID": "labs/feature-engineering.html#load-the-needed-packages",
    "href": "labs/feature-engineering.html#load-the-needed-packages",
    "title": "7  Part-of-speech tagging and dependency parsing",
    "section": "7.2 Load the needed packages",
    "text": "7.2 Load the needed packages\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(udpipe)\nlibrary(gt)\n\nLoad the functions:\n\nsource(\"../R/keyness_functions.R\")\n\nLoad the data:\n\nload(\"../data/sample_corpus.rda\")",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part-of-speech tagging and dependency parsing</span>"
    ]
  },
  {
    "objectID": "labs/feature-engineering.html#parsing",
    "href": "labs/feature-engineering.html#parsing",
    "title": "7  Part-of-speech tagging and dependency parsing",
    "section": "7.3 Parsing",
    "text": "7.3 Parsing\n\n7.3.1 Preparing a corpus\nWhen we parse texts using a model like ones available in udpipe or spacy, we need to do very little to prepare the corpus. We could trim extra spaces and returns using str_squish() or remove urls, but generally we want the text to be mostly “as is” so the model can do its job.\n\n\n7.3.2 Download a model\nYou only need to run this line of code once. To run it, remove the pound sign, run the line, then add the pound sign after you’ve downloaded the model. Or you can run the next chunk and the model will automatically be downloaded in your working directory.\n\n# udpipe_download_model(language = \"english\")\n\n\n\n7.3.3 Annotate a sentence\n\ntxt &lt;- \"The company offers credit cards, loans and interest-generating accounts.\"\nud_model &lt;- udpipe_load_model(\"../models/english-ewt-ud-2.5-191206.udpipe\")\nannotation &lt;- udpipe(txt, ud_model)\n\n\n\nCode\nannotation[,8:15] |&gt;\n  gt() |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\nAnnotation of a sample sentence.\n\n\ntoken_id\ntoken\nlemma\nupos\nxpos\nfeats\nhead_token_id\ndep_rel\n\n\n\n\n1\nThe\nthe\nDET\nDT\nDefinite=Def|PronType=Art\n2\ndet\n\n\n2\ncompany\ncompany\nNOUN\nNN\nNumber=Sing\n3\nnsubj\n\n\n3\noffers\noffer\nVERB\nVBZ\nMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n0\nroot\n\n\n4\ncredit\ncredit\nNOUN\nNN\nNumber=Sing\n5\ncompound\n\n\n5\ncards\ncard\nNOUN\nNNS\nNumber=Plur\n3\nobj\n\n\n6\n,\n,\nPUNCT\n,\nNA\n7\npunct\n\n\n7\nloans\nloans\nNOUN\nNNS\nNumber=Plur\n5\nconj\n\n\n8\nand\nand\nCCONJ\nCC\nNA\n12\ncc\n\n\n9\ninterest\ninterest\nNOUN\nNN\nNumber=Sing\n11\ncompound\n\n\n10\n-\n-\nPUNCT\nHYPH\nNA\n11\npunct\n\n\n11\ngenerating\ngenera\nNOUN\nNN\nNumber=Sing\n12\ncompound\n\n\n12\naccounts\naccount\nNOUN\nNNS\nNumber=Plur\n5\nconj\n\n\n13\n.\n.\nPUNCT\n.\nNA\n3\npunct\n\n\n\n\n\n\n\n\n\n7.3.4 Plot the annotation\nWe can also plot the dependency structure using igraph:\n\nlibrary(igraph)\nlibrary(ggraph)\n\nFirst we’ll create a plotting function.\n\nplot_annotation &lt;- function(x, size = 3){\n  stopifnot(is.data.frame(x) & all(c(\"sentence_id\", \"token_id\", \"head_token_id\", \"dep_rel\",\n                                     \"token_id\", \"token\", \"lemma\", \"upos\", \"xpos\", \"feats\") %in% colnames(x)))\n  x &lt;- x[!is.na(x$head_token_id), ]\n  x &lt;- x[x$sentence_id %in% min(x$sentence_id), ]\n  edges &lt;- x[x$head_token_id != 0, c(\"token_id\", \"head_token_id\", \"dep_rel\")]\n  edges &lt;- edges[edges$dep_rel != \"punct\",]\n  edges$head_token_id &lt;- ifelse(edges$head_token_id == 0, edges$token_id, edges$head_token_id)\n  nodes = x[, c(\"token_id\", \"token\", \"lemma\", \"upos\", \"xpos\", \"feats\")]\n  edges$label &lt;- edges$dep_rel\n  g &lt;- graph_from_data_frame(edges,\n                             vertices = nodes,\n                             directed = TRUE)\n  ggraph(g, layout = \"linear\") +\n    geom_edge_arc(ggplot2::aes(label = dep_rel, vjust = -0.20), fold = T,linemitre = 2,\n                  arrow = grid::arrow(length = unit(3, 'mm'), ends = \"last\", type = \"closed\"),\n                  end_cap = ggraph::label_rect(\"wordswordswords\"),\n                  label_colour = \"red\", check_overlap = TRUE, label_size = size) +\n    geom_node_label(ggplot2::aes(label = token), col = \"black\", size = size, fontface = \"bold\") +\n    geom_node_text(ggplot2::aes(label = xpos), nudge_y = -0.35, size = size) +\n    theme_graph(base_family = \"Arial Narrow\")\n}\n\nAnd plot the annotation:\n\nplot_annotation(annotation, size = 2.5)\n\n\n\n\nDependency structure of a sample parsed sentence.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part-of-speech tagging and dependency parsing</span>"
    ]
  },
  {
    "objectID": "labs/feature-engineering.html#annotate-a-corpus",
    "href": "labs/feature-engineering.html#annotate-a-corpus",
    "title": "7  Part-of-speech tagging and dependency parsing",
    "section": "7.4 Annotate a corpus",
    "text": "7.4 Annotate a corpus\nParsing text is a computationally intensive process and can take time. So for the purposes of this lab, we’ll create a smaller sub-sample of the the data. By adding a column called text_type which includes information extracted from the file names, we can sample 5 texts from each.\n\nset.seed(123)\nsub_corpus &lt;- quanteda.extras::sample_corpus %&gt;%\n  mutate(text_type = str_extract(doc_id, \"^[a-z]+\")) %&gt;%\n  group_by(text_type) %&gt;%\n  sample_n(5) %&gt;%\n  ungroup() %&gt;%\n  dplyr::select(doc_id, text)\n\n\n7.4.1 Parallel processing\nParallel processing is a method whereby separate parts of an overall complex task are broken up and run simultaneously on multiple CPUs, thereby reducing the amount of time for processing. Part-of-speech tagging and dependency parsing are computationally intensive, so using parallel processing can save valuable time.\nThe udpipe() function has an argument for assigning cores: parallel.cores = 1L. It’s easy to set up, so feel free to use that option.\nA second option, requires more preparation, but is even faster. So we’ll walk through how it works. First, we will split the corpus based on available cores.\n\ncorpus_split &lt;- split(sub_corpus, seq(1, nrow(sub_corpus), by = 10))\n\nFor parallel processing in R, we’ll us the package future.apply.\n\nlibrary(future.apply)\n\nNext, we set up our parallel session by specifying the number of cores, and creating a simple annotation function.\n\nncores &lt;- 4L\nplan(multisession, workers = ncores)\n\nannotate_splits &lt;- function(corpus_text) {\n  ud_model &lt;- udpipe_load_model(\"../models/english-ewt-ud-2.5-191206.udpipe\")\n  x &lt;- data.table::as.data.table(udpipe_annotate(ud_model, x = corpus_text$text,\n                                                 doc_id = corpus_text$doc_id))\n  return(x)\n}\n\nFinally, we annotate using future_lapply. On my machine, this takes roughly 32 seconds.\n\nannotation &lt;- future_lapply(corpus_split, annotate_splits, future.seed = T)\n\nAs you might guess, the output is a list of data frames, so we’ll combine them using rbindlist().\n\nannotation &lt;- data.table::rbindlist(annotation)",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part-of-speech tagging and dependency parsing</span>"
    ]
  },
  {
    "objectID": "labs/feature-engineering.html#process-with-quanteda",
    "href": "labs/feature-engineering.html#process-with-quanteda",
    "title": "7  Part-of-speech tagging and dependency parsing",
    "section": "7.5 Process with quanteda",
    "text": "7.5 Process with quanteda\n\n7.5.1 Format the data for quanteda\nIf we want to do any further processing in quanteda, we need to make a couple of adjustments to our data frame.\n\nanno_edit &lt;- annotation %&gt;%\n  dplyr::select(doc_id, sentence_id, token_id, token, lemma, upos, xpos, head_token_id, dep_rel) %&gt;%\n  rename(pos = upos, tag = xpos)\n\nanno_edit &lt;- structure(anno_edit, class = c(\"spacyr_parsed\", \"data.frame\"))\n\n\n\n7.5.2 Convert to tokens\n\nsub_tkns &lt;- as.tokens(anno_edit, include_pos = \"tag\", concatenator = \"_\")\n\n\n\n7.5.3 Create a dfm\nWe will also extract and assign the variable text_type to the tokens object.\n\ndoc_categories &lt;- names(sub_tkns) %&gt;%\n  data.frame(text_type = .) %&gt;%\n  mutate(text_type = str_extract(text_type, \"^[a-z]+\"))\n\ndocvars(sub_tkns) &lt;- doc_categories\n\nsub_dfm &lt;- dfm(sub_tkns)\n\nAnd check the frequencies:\n\n\nCode\ntextstat_frequency(sub_dfm, n = 10) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\n._.\n6452\n1\n40\nall\n\n\n,_,\n5900\n2\n40\nall\n\n\nthe_dt\n5217\n3\n40\nall\n\n\nand_cc\n2596\n4\n40\nall\n\n\nof_in\n2513\n5\n40\nall\n\n\na_dt\n2256\n6\n40\nall\n\n\nto_to\n1702\n7\n40\nall\n\n\nin_in\n1645\n8\n40\nall\n\n\ni_prp\n1497\n9\n36\nall\n\n\nyou_prp\n1202\n10\n36\nall\n\n\n\n\n\n\n\n\n\n7.5.4 Filter/select tokens\nThere are multiple ways to filter/select the tokens we want to count. We could, for example, just filter out all rows in the annotation data frame tagged as PUNCT, if we wanted to exclude punctuation from our counts.\nI would, however, advise against altering the original parsed file. We may want to try different options, and we want to avoid having to re-parse our corpus, as that is the most computationally intensive step in the processing pipeline. In fact, if this were part of an actual project, I would advise that you save the parsed data frame as a .csv file using write_csv() for later use.\nSo we will try an alternative. We use the tokens_select() function to either keep or remove tokens based on regular expressions.\n\nsub_dfm &lt;- sub_tkns %&gt;%\n  tokens_select(\"^.*[a-zA-Z0-9]+.*_[a-z]\", selection = \"keep\", valuetype = \"regex\", case_insensitive = T) %&gt;%\n  dfm()\n\nAnd check the frequencies:\n\n\nCode\ntextstat_frequency(sub_dfm, n = 10) |&gt;\n  gt() |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\nMost frequent tokens tagged for part-of-speech in sub-sample of the corpus.\n\n\nfeature\nfrequency\nrank\ndocfreq\ngroup\n\n\n\n\nthe_dt\n5217\n1\n40\nall\n\n\nand_cc\n2596\n2\n40\nall\n\n\nof_in\n2513\n3\n40\nall\n\n\na_dt\n2256\n4\n40\nall\n\n\nto_to\n1702\n5\n40\nall\n\n\nin_in\n1645\n6\n40\nall\n\n\ni_prp\n1497\n7\n36\nall\n\n\nyou_prp\n1202\n8\n36\nall\n\n\nit_prp\n1168\n9\n39\nall\n\n\nis_vbz\n1042\n10\n40\nall\n\n\n\n\n\n\n\nIf we want to compare one text-type (as our target corpus) to another (as our reference corpus), we can easily subset the data.\n\nacad_dfm &lt;- dfm_subset(sub_dfm, text_type == \"acad\") %&gt;% dfm_trim(min_termfreq = 1)\nfic_dfm &lt;- dfm_subset(sub_dfm, text_type == \"fic\") %&gt;% dfm_trim(min_termfreq = 1)\n\nAnd finally, we can generate a keyness table,\n\nacad_v_fic &lt;- keyness_table(acad_dfm, fic_dfm) %&gt;%\n  separate(col = Token, into = c(\"Token\", \"Tag\"), sep = \"_\")\n\nFrom that data, we can filter specific lexical classes, like modal verbs:\n\n\nCode\nacad_v_fic %&gt;% filter(Tag == \"md\") |&gt;\n  gt() |&gt;\n  fmt_number(columns = c('LL', 'LR', 'Per_10.4_Tar', 'Per_10.4_Ref'), decimals = 2) |&gt;\n  fmt_number(columns = c('DP_Tar', 'DP_Ref'), decimals = 3) |&gt;\n  fmt_number(columns = c('PV'), decimals = 5) |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\nA keyness comparision of modal verbs in a sub-sample of the academic vs. fiction text-types.\n\n\nToken\nTag\nLL\nLR\nPV\nAF_Tar\nAF_Ref\nPer_10.4_Tar\nPer_10.4_Ref\nDP_Tar\nDP_Ref\n\n\n\n\nmay\nmd\n3.99\n1.43\n0.04583\n13\n5\n10.28\n3.81\n0.165\n0.401\n\n\nwill\nmd\n3.66\n1.14\n0.05579\n17\n8\n13.44\n6.09\n0.539\n0.599\n\n\nill\nmd\n1.42\n1.05\n0.23275\n1\n0\n0.79\n0.00\n0.797\nNA\n\n\nought\nmd\n1.42\n1.05\n0.23275\n1\n0\n0.79\n0.00\n0.800\nNA\n\n\nmust\nmd\n0.13\n0.32\n0.71618\n6\n5\n4.74\n3.81\n0.431\n0.602\n\n\nwo\nmd\n0.00\n0.05\n0.97895\n1\n1\n0.79\n0.76\n0.797\n0.800\n\n\nca\nmd\n−0.17\n−0.53\n0.68389\n2\n3\n1.58\n2.28\n0.797\n0.599\n\n\nshould\nmd\n−0.22\n−0.36\n0.64138\n6\n8\n4.74\n6.09\n0.261\n0.346\n\n\ncan\nmd\n−3.34\n−0.80\n0.06761\n16\n29\n12.65\n22.08\n0.348\n0.224\n\n\nmight\nmd\n−3.63\n−1.95\n0.05659\n2\n8\n1.58\n6.09\n0.598\n0.677\n\n\ncould\nmd\n−6.15\n−0.91\n0.01316\n22\n43\n17.39\n32.74\n0.259\n0.241\n\n\nwould\nmd\n−9.22\n−1.31\n0.00240\n14\n36\n11.07\n27.41\n0.313\n0.215\n\n\n'll\nmd\n−12.97\n−3.75\n0.00032\n1\n14\n0.79\n10.66\n0.797\n0.239\n\n\n'd\nmd\n−32.38\n−5.53\n0.00000\n0\n24\n0.00\n18.28\nNA\n0.234\n\n\n\n\n\n\n\n\n\n\n\n7.5.5 Extract phrases\nWe can also extract phrases of specific types. To so so, we first use the function as_phrasemachine() to add a new column to our annotation called phrase_tag.\n\nannotation$phrase_tag &lt;- as_phrasemachine(annotation$upos, type = \"upos\")\n\nNext, we can use the function keywords_phrases() to extract phrase-types based on regular expressions. Refer to the documentation for suggested regex patterns: https://www.rdocumentation.org/packages/udpipe/versions/0.8.6/topics/keywords_phrases.\nYou can also read examples of use cases: https://bnosac.github.io/udpipe/docs/doc7.html.\nFirst, we’ll subset our data into annotations by text-type.\n\nacad_anno &lt;- annotation %&gt;% filter(str_detect(doc_id, \"acad\"))\nfic_anno &lt;- annotation %&gt;% filter(str_detect(doc_id, \"fic\"))\n\n\nacad_nps &lt;- keywords_phrases(x = acad_anno$phrase_tag, term = tolower(acad_anno$token), \n                          pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                          is_regex = TRUE, detailed = T)\n\n\nfic_nps &lt;- keywords_phrases(x = fic_anno$phrase_tag, term = tolower(fic_anno$token), \n                             pattern = \"(A|N)*N(P+D*(A|N)*N)*\", \n                             is_regex = TRUE, detailed = T)\n\n\n\nCode\nacad_nps |&gt;\n  head(25) |&gt;\n  gt() |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\nNoun phrases extracted from a sub-sample of the corpus.\n\n\nkeyword\nngram\npattern\nstart\nend\n\n\n\n\nlargest creatures\n2\nAN\n2\n3\n\n\ncreatures\n1\nN\n3\n3\n\n\nearth\n1\nN\n9\n9\n\n\nanimals\n1\nN\n11\n11\n\n\napatosaurus\n1\nN\n14\n14\n\n\naka\n1\nN\n16\n16\n\n\naka brontosaurus\n2\nNN\n16\n17\n\n\nbrontosaurus\n1\nN\n17\n17\n\n\npaleontologists\n1\nN\n19\n19\n\n\npicture\n1\nN\n23\n23\n\n\nthey\n1\nN\n26\n26\n\n\nenglish anatomist\n2\nAN\n35\n36\n\n\nanatomist\n1\nN\n36\n36\n\n\npaleontologist\n1\nN\n39\n39\n\n\npaleontologist richard\n2\nNN\n39\n40\n\n\npaleontologist richard owen\n3\nNNN\n39\n41\n\n\nrichard\n1\nN\n40\n40\n\n\nrichard owen\n2\nNN\n40\n41\n\n\nowen\n1\nN\n41\n41\n\n\nword\n1\nN\n44\n44\n\n\ndinosaur\n1\nN\n46\n46\n\n\nnew category\n2\nAN\n51\n52\n\n\nnew category of reptiles\n4\nANPN\n51\n54\n\n\ncategory\n1\nN\n52\n52\n\n\ncategory of reptiles\n3\nNPN\n52\n54\n\n\n\n\n\n\n\nNote that although the function uses the term keywords, it is NOT executing a hypothesis test of any kind.\n\n\n7.5.6 Extract only unique phrases\nNote that udpipe extracts overlapping constituents of phrase structures. Normally, we would want only unique phrases. To find those we’ll take advantage of the start and end indexes, using the between() function from the data.table package.\nThat will generate a logical vector, which we can use to filter out only those phrases that don’t overlap with another.\n\nidx &lt;- seq(1:nrow(acad_nps))\n\nis_unique &lt;- lapply(idx, function(i) sum(data.table::between(acad_nps$start[i], acad_nps$start, acad_nps$end) & data.table::between(acad_nps$end[i], acad_nps$start, acad_nps$end)) == 1) %&gt;% unlist()\n\nacad_nps &lt;- acad_nps[is_unique, ]\n\n\nidx &lt;- seq(1:nrow(fic_nps))\n\nis_unique &lt;- lapply(idx, function(i) sum(data.table::between(fic_nps$start[i], fic_nps$start, fic_nps$end) & data.table::between(fic_nps$end[i], fic_nps$start, fic_nps$end)) == 1) %&gt;% unlist()\n\nfic_nps &lt;- fic_nps[is_unique, ]\n\nWe can also add a rough accounting of the lengths of the noun phrases by summing the spaces and adding 1.\n\nacad_nps &lt;- acad_nps %&gt;%\n  mutate(phrase_length = str_count(keyword, \" \") + 1)\n\nfic_nps &lt;- fic_nps %&gt;%\n  mutate(phrase_length = str_count(keyword, \" \") + 1)\n\n\n\nCode\nfic_nps |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\nUnique noun phrases extracted from a sub-sample of the corpus.\n\n\nkeyword\nngram\npattern\nstart\nend\nphrase_length\n\n\n\n\nit\n1\nN\n1\n1\n1\n\n\npleasant summer night\n3\nANN\n4\n6\n3\n\n\nwind off the ocean\n4\nNPDN\n10\n13\n4\n\n\ntrees along copley square\n4\nNPNN\n16\n19\n4\n\n\ni\n1\nN\n21\n21\n1\n\n\nboston public library\n3\nNNN\n25\n27\n3\n\n\nsquare\n1\nN\n31\n31\n1\n\n\ncopley plaza bar\n3\nNNN\n37\n39\n3\n\n\nfirst time\n2\nAN\n44\n45\n2\n\n\nsammy\n1\nN\n47\n47\n1",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Part-of-speech tagging and dependency parsing</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html",
    "href": "labs/logistic-regression.html",
    "title": "8  Logistic Regression",
    "section": "",
    "text": "8.1 Load the needed packages\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(nnet)\nlibrary(gt)\nLoad metadata:\nload(\"../data/micusp_meta.rda\")\nload(\"../data/bawe_meta.rda\")\nCode\nmicusp_meta |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  tab_options(table.font.size = px(10),\n              quarto.disable_processing = TRUE) |&gt;\n  as_raw_html()\n\n\n\n  \n  Metadata associated with the Michigan Corpus of Upper-Level Student\nPapers (MICUSP)\n  \n    \n      doc_id\n      paper_title\n      paper_discipline\n      student_level\n      discipline_cat\n      level_cat\n      student_gender\n      speaker_status\n      speaker_l1\n      paper_type\n      paper_features\n    \n  \n  \n    BIO.G0.01.1\nThe Ecology and Epidemiology of Plague\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nReport\nLiterature review section, Reference to sources\n    BIO.G0.02.1\nHost-Parasite Interactions: On the Presumed Sympatric Speciation of Vidua\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nTables, graphs or figures, Reference to sources\n    BIO.G0.02.2\nSensory Drive and Speciation\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nReference to sources\n    BIO.G0.02.3\nPlant Pollination Systems: Evolutionary Trends in Generalization and Specialization\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nReference to sources\n    BIO.G0.02.4\nChromosomal Rearrangements, Recombination Suppression, and Speciation: A Review of Rieseberg 2001\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nReference to sources\n    BIO.G0.02.5\nOn the Origins of Man: Understanding the Last Two Million Years\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nDefinitions, Discussion of results section, Tables, graphs or figures, Reference to sources\n    BIO.G0.02.6\nGenetic Analysis of Drosophila Melanogaster Mutants: To Determine Inheritance and Linkage Patterns\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nResearchPaper\nAbstract, Methodology section, Discussion of results section, Tables, graphs or figures, Reference to sources\n    BIO.G0.03.1\nLab 3: Plant Competition\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nResearchPaper\nDiscussion of results section, Tables, graphs or figures, Reference to sources\n    BIO.G0.03.2\nThe Effects of Motor Oil on Aquatic Insect Predation\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nResearchPaper\nAbstract, Methodology section, Discussion of results section, Reference to sources\n    BIO.G0.03.3\nComparison of Hypothesis Testing in a High vs. Low-impact Journal\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nResearchPaper\nMethodology section, Discussion of results section, Tables, graphs or figures, Reference to sources\nCode\nmicusp_meta |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  tab_options(table.font.size = px(10),\n              quarto.disable_processing = TRUE) |&gt;\n  as_raw_html()\n\n\n\n  \n  Metadata associated with the British Academic Written English (BAWE)\ncorpus.\n  \n    \n      doc_id\n      paper_title\n      paper_discipline\n      student_level\n      discipline_cat\n      level_cat\n      student_gender\n      speaker_status\n      speaker_l1\n      paper_type\n      paper_features\n    \n  \n  \n    BIO.G0.01.1\nThe Ecology and Epidemiology of Plague\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nReport\nLiterature review section, Reference to sources\n    BIO.G0.02.1\nHost-Parasite Interactions: On the Presumed Sympatric Speciation of Vidua\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nTables, graphs or figures, Reference to sources\n    BIO.G0.02.2\nSensory Drive and Speciation\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nReference to sources\n    BIO.G0.02.3\nPlant Pollination Systems: Evolutionary Trends in Generalization and Specialization\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nReference to sources\n    BIO.G0.02.4\nChromosomal Rearrangements, Recombination Suppression, and Speciation: A Review of Rieseberg 2001\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nReference to sources\n    BIO.G0.02.5\nOn the Origins of Man: Understanding the Last Two Million Years\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nReport\nDefinitions, Discussion of results section, Tables, graphs or figures, Reference to sources\n    BIO.G0.02.6\nGenetic Analysis of Drosophila Melanogaster Mutants: To Determine Inheritance and Linkage Patterns\nBiology\nFinal Year Undergraduate\nBIO\nG0\nM\nNS\nNA\nResearchPaper\nAbstract, Methodology section, Discussion of results section, Tables, graphs or figures, Reference to sources\n    BIO.G0.03.1\nLab 3: Plant Competition\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nResearchPaper\nDiscussion of results section, Tables, graphs or figures, Reference to sources\n    BIO.G0.03.2\nThe Effects of Motor Oil on Aquatic Insect Predation\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nResearchPaper\nAbstract, Methodology section, Discussion of results section, Reference to sources\n    BIO.G0.03.3\nComparison of Hypothesis Testing in a High vs. Low-impact Journal\nBiology\nFinal Year Undergraduate\nBIO\nG0\nF\nNS\nNA\nResearchPaper\nMethodology section, Discussion of results section, Tables, graphs or figures, Reference to sources",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#case-1",
    "href": "labs/logistic-regression.html#case-1",
    "title": "8  Logistic Regression",
    "section": "8.2 Case 1",
    "text": "8.2 Case 1\nPreparing linguistic data for logistic regression is often a complex process, as it will be for this lab. Our problem comes from Brezina (pg. 130) and concerns that vs. which:\n\nWhile writing this chapter, I encountered the following situation: my word processor underlines with a wiggly line a phrse that included the relative pronoun which, signaling a potential grammatical error or inaccuracy. The correction off correction offered was to either add a comma in front of which or use the relative pronoun that instead., with the reasoning being as follows: ‘If these words are not essential to the meaning of your sentences, use “which” and separate the words with a comma.’\n\nWe are going to mimic Brezina’s experiment, but instead of using the BE06 and AmE06 corpora, we will use the Michigan Corpus of Upper-Level Student Papers (MICUSP) and the British Academic Written English corpus (BAWE).\nTo save time, the data has already been tagged with udpipe. To tag files this way is relatively straightforward. Don’t run the chunk below, but you can see how it would work if you uncommented the code lines.\n\n# udmodel &lt;- udpipe_load_model(file = \"english-ewt-ud-2.5-191206.udpipe\")\n# target_folder &lt;- \"/Users/user/Downloads/bawe_udpipe/\"\n# files_list &lt;- list.files(\"/Users/user/Downloads/bawe_body\", full.names = T)\n\ntag_text &lt;- function(x){\n  \n  file_name &lt;- basename(x) %&gt;% str_remove(\".txt\")\n  output_file &lt;- paste0(target_folder, file_name, \"_udp.txt\")\n  txt &lt;- readr::read_file(x) %&gt;% str_squish()\n  annotation &lt;- udpipe_annotate(udmodel, txt, parser = \"none\") %&gt;% \n    as.data.frame() %&gt;%\n    dplyr::select(token, xpos) %&gt;%\n    unite(\"token\", token:xpos)\n  \n  new_txt &lt;- paste(annotation$token, collapse=\" \")\n  write.table(new_txt, output_file, quote = F, row.names = F, col.names = F)\n  \n}\n\n# lapply(files_list, tag_text)",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#prepare-tokens",
    "href": "labs/logistic-regression.html#prepare-tokens",
    "title": "8  Logistic Regression",
    "section": "8.3 Prepare tokens",
    "text": "8.3 Prepare tokens\nHere, again, you could download the full data set and place it the data directory. Then, we could down-sample our data.\n\nset.seed(123)\n\nus_files_list &lt;- list.files(\"../data/micusp_udpipe\", full.names = T)\nuk_files_list &lt;- list.files(\"../data/bawe_udpipe\", full.names = T)\n\nus_sample &lt;- sample(us_files_list, 100)\nuk_sample &lt;- sample(uk_files_list, 100)\n\nTo save time, we’ll load the down-sampled data directly.\n\nload(\"../data/student_samples.rda\")\n\nAnd tokenize our sub-sample.\n\nus_tokens &lt;- us_sample %&gt;%\n  corpus() %&gt;%\n  tokens(what = \"fastestword\")\n  \nuk_tokens &lt;- uk_sample %&gt;%\n  corpus() %&gt;%\n  tokens(what = \"fastestword\")\n\n\n8.3.1 Select token sequences\nFor this experiment, we want to select sequences that have have noun followed by that or which. Importantly, we also want sequences that have a comma between the noun and the relative pronoun. Imagine a possible phrase like: however, the research [that/which] has been done has focused primarily on. We would want to capture all of these possible combinations:\n\nresearch that\nresearch which\nresearch, that\nresearch, which\n\nWe’ll begin by generating what are called skipgrams. A skipgram “skips” over a specified number of tokens. We’ll generate skipgrams 2 to 3 tokens long and skipping over 0 or 1 tokens. Thus, we’ll generate a series of phrases, 2 to 3 tokens in length.\n\nus_grams &lt;- tokens_skipgrams(us_tokens, n=2:3, skip=0:1, concatenator = \" \")\nuk_grams &lt;- tokens_skipgrams(uk_tokens, n=2:3, skip=0:1, concatenator = \" \")\n\nFor our purposes, we don’t need all of these. So we want to begin culling our tokens. First we know that we’re only interested in those ngrams ending with that or which. So we can first identify those.\n\nus_grams &lt;- tokens_select(us_grams, \"that_\\\\S+$|which_\\\\S+$\", selection = \"keep\", valuetype = \"regex\", case_insensitive = T)\n\nHowever, we need to sort further. Our tokens of interest can appear in a variety of contexts. For example, “that” frequently appears following a verb of thinking or speaking, as it does here: has_VHZ discussed_VVN that_WDT.\nWe only want those instances where “that” or “which” is modifying a noun, as in this example: belief_NN that_WDT.\nSo next, we’ll select only those ngrams that being with a word that’s been tagged as a noun (having the _NN tag).\n\nus_grams &lt;- tokens_select(us_grams, \"^[a-z]+_NN\\\\d?\", selection = \"keep\", valuetype = \"regex\", case_insensitive = T)\n\nFinally, we want only those 3 token sequences that have a medial comma like: earth_NN ,_, that_WDT\n\nus_grams &lt;- tokens_select(us_grams, \"\\\\s[^,]+_[^,]+\\\\s\", selection = \"remove\", valuetype = \"regex\", case_insensitive = T)\n\nSo let’s repeat the sorting process with the UK data.\n\nuk_grams &lt;- uk_grams %&gt;%\n  tokens_select(\"that_\\\\S+$|which_\\\\S+$\", selection = \"keep\", valuetype = \"regex\", case_insensitive = T) %&gt;%\n  tokens_select(\"^[a-z]+_NN\\\\d?\", selection = \"keep\", valuetype = \"regex\", case_insensitive = T) %&gt;%\n  tokens_select(\"\\\\s[^,]+_[^,]+\\\\s\", selection = \"remove\", valuetype = \"regex\", case_insensitive = T)\n\n\n\n8.3.2 Structuring the data\nNow let’s convert our data structure to a data frame.\n\nus_grams &lt;- data.frame(feature = unlist(us_grams), stringsAsFactors = F)\nuk_grams &lt;- data.frame(feature = unlist(uk_grams), stringsAsFactors = F)\n\n\n\n\n\n\n\n\n  \n  \n  US sequences with a relative pronoun.\n  \n    \n      feature\n    \n  \n  \n    organisms_NNS that_WDT\n    frequency_NN which_WDT\n    bacteria_NNS that_WDT\n    colonies_NNS that_WDT\n    bacteria_NN that_WDT\n    genes_NNS that_WDT\n    bacteria_NN that_WDT\n    bacteria_NN that_IN\n    replica_NN ,_, that_IN\n    plates_NNS ,_, that_IN\n  \n  \n  \n\n\n\n\n\n  \n  \n  UK sequences with a relative pronoun.\n  \n    \n      feature\n    \n  \n  \n    belief_NN that_DT\n    Waters_NNP that_IN\n    news_NN that_DT\n    process_NN which_WDT\n    anvil_NN which_WDT\n    factors_NNS that_WDT\n    Postmodernists_NNS that_IN\n    Lyotard_NNP that_DT\n    classes_NNS that_IN\n    assumption_NN that_IN\n  \n  \n  \n\n\n\n\n\nWe’re going to follow Brezina’s recommendation on pg. 122 for stucturing and idenfiying our variables using some prefixing.\n\nus_grams &lt;- us_grams %&gt;%\n  rownames_to_column(\"doc_id\") %&gt;%\n  mutate(doc_id = str_replace(doc_id, \"_\\\\S+$\", \"\")) %&gt;%\n  mutate(comma_sep = ifelse(str_detect(feature, \",\") == T, \"B_yes\", \"A_no\")) %&gt;%\n  mutate(rel_type = ifelse(str_detect(feature, \"that_\") == T, \"A_that\", \"B_which\"))\n\nuk_grams &lt;- uk_grams %&gt;%\n  rownames_to_column(\"doc_id\") %&gt;%\n  mutate(doc_id = str_replace(doc_id, \"_\\\\S+$\", \"\")) %&gt;%\n  mutate(comma_sep = ifelse(str_detect(feature, \",\") == T, \"B_yes\", \"A_no\")) %&gt;%\n  mutate(rel_type = ifelse(str_detect(feature, \"that_\") == T, \"A_that\", \"B_which\"))\n\nNow let’s join some metadata. We’ll select the speaker_status variable. Again, we’ll clean it using some prefixing. Finally, we’ll add a column that identifyies the location as being in US.\n\nus_grams &lt;- us_grams %&gt;% \n  left_join(select(micusp_meta, doc_id, speaker_status), by = \"doc_id\") %&gt;%\n  mutate(speaker_status = str_replace(speaker_status, \"NNS\", \"B_NNS\")) %&gt;%\n  mutate(speaker_status = str_replace(speaker_status, \"^NS$\", \"A_NS\")) %&gt;%\n  mutate(nat_id = \"A_US\")\n\nThe UK data doesn’t have a speaker_status column but it does have a first_language column. We can make a column that matches the US data using ifelse().\n\nuk_grams &lt;- uk_grams %&gt;% \n  left_join(dplyr::select(bawe_meta, doc_id, speaker_l1), by = \"doc_id\") %&gt;%\n  mutate(speaker_l1 = ifelse(str_detect(speaker_l1, \"English\") == T, \"A_NS\", \"B_NNS\")) %&gt;%\n  rename(speaker_status = speaker_l1) %&gt;%\n  mutate(nat_id = \"B_UK\")\n\n\n\n8.3.3 Logistic regression model\nBefore running the regression model, we can combine the two tables. We also need to convert some character columns into factors (or categorical variables).\n\nrel_data &lt;- bind_rows(us_grams, uk_grams) %&gt;%\n  mutate_at(3:6, factor)\n\nTo understand how to set up the model, it might help to refer to pg. 119. Our outcome variable is that/which or the rel_type column. For logistic regression, this must be binary. For our first model, we’ll set up 3 predictor variables:\n\ncomma_sep: whether or not a comma appears between the noun and the relative pronoun\nnat_id: whether the student is in the US or the UK\nspeaker_status: whether the student is a native speaker of English or not\n\n\nglm_fit &lt;- glm(rel_type ~ comma_sep + nat_id + speaker_status, data = rel_data, family = \"binomial\")\n\n\n\n8.3.4 Evaluating the model\nLogistic regression has some prerequisites and assumptions. One of which is there is no colinearity between predictors. One tool for colinearity diagnostics is VIF. VIF (or variance inflation factors) measure the inflation in the variances of the parameter estimates due to collinearities that exist among the predictors and range from 1 upwards. The numerical value for VIF tells you (in decimal form) what percentage the variance is inflated for each coefficient. For example, a VIF of 1.9 tells you that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity — if there was no correlation with other predictors.\n\ncar::vif(glm_fit)\n\n     comma_sep         nat_id speaker_status \n      1.001520       1.079571       1.078274 \n\n\nNow let’s look at odds ratios. We can calculate the odds ratio by exponentiating the coefficients (or log odds). For example on pg. 125 of Brezina, he shows an estimate of 6.802 for Context_type B_determined. If we were to exponentiate that value:\nexp(6.802)\nWe would get odd roughly equal to 900, as Brezina’s table shows. So here we could calculate our odds ratios and our confidence intervals:\nexp(cbind(OR = coef(glm_fit), confint(glm_fit)))\nFor a nicely formatted output, we can use the gtsummary package to put those values into a table.\n\ngtsummary::tbl_regression(glm_fit, exponentiate = TRUE) |&gt;\n  gtsummary::add_global_p() |&gt;\n  gtsummary::add_glance_table(\n    include = c(nobs, logLik, AIC, BIC))\n\n\n\n\n\nLogistic regression estimates of which or that.\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n1\n95% CI\n1\np-value\n\n\n\n\ncomma_sep\n\n\n\n\n&lt;0.001\n\n\n    A_no\n—\n—\n\n\n\n\n    B_yes\n4.90\n4.29, 5.60\n\n\n\n\nnat_id\n\n\n\n\n&lt;0.001\n\n\n    A_US\n—\n—\n\n\n\n\n    B_UK\n1.43\n1.29, 1.60\n\n\n\n\nspeaker_status\n\n\n\n\n0.7\n\n\n    A_NS\n—\n—\n\n\n\n\n    B_NNS\n1.02\n0.90, 1.16\n\n\n\n\nNo. Obs.\n6,761\n\n\n\n\n\n\nLog-likelihood\n-4,067\n\n\n\n\n\n\nAIC\n8,143\n\n\n\n\n\n\nBIC\n8,170\n\n\n\n\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhile no exact equivalent to the R2 of linear regression exists, an R2 index can be used to assess the model fit. Note that pseudo R2 have been critiqued for their lack of accuracy. See Brezina pg. 125.\nWe can also generate what Brezina calls a C-index. The C-index is the area under an ROC. The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5. These can be calculated and plotted using packages like ROCR. You can find examples of the plots and the resulting AUC values as in this one:\nhttps://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/\nFor our purposes we’ll just use the Cstat() function from *DescTools.\n\nDescTools::Cstat(glm_fit)\n\n[1] 0.6482311",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#case-2",
    "href": "labs/logistic-regression.html#case-2",
    "title": "8  Logistic Regression",
    "section": "8.4 Case 2",
    "text": "8.4 Case 2\nLet’s trying making another model. This time, we’ll just use the untaggged MICUSP data. Note that our choices are a little different here. We’re leaving in punctuation and numbers. This is because we’ll be doing something a little different with this tokens object.\n\n8.4.1 Prepare the data\n\nload(\"../data/micusp_mini.rda\")\n\nmicusp_tokens &lt;- micusp_mini %&gt;%\n  corpus() %&gt;%\n  tokens(include_docvars=T, remove_punct = F, remove_numbers = F, remove_symbols = T, what = \"word\")\n\nNow we load in a dictionary. This dictionary has almost 15,000 entries. It is organized into only 2 categories: phrases that communicate high confidence and those that communicate hedged confidence. Go and look at the Hyland article on stance and engagement for more on the importance of these kinds of features to academic writing.\n\nhb_dict &lt;- dictionary(file = \"../data/hedges_boosters.yml\")\n\nNext we create a new tokens object from our original one. By using tokens_lookup() with our dictionary, we will create groupings based on our dictionary. Note that our dictionary has only 1 level. But if we can a more complex taxonomy, we can specify which level of the taxonomy we’d like to group our tokens under.\n\nhb &lt;- micusp_tokens %&gt;%\n  tokens_lookup(dictionary = hb_dict, levels = 1) %&gt;%\n  dfm() %&gt;%\n  convert(to = \"data.frame\") %&gt;% \n  mutate(\n    tokens_total = ntoken(micusp_tokens),\n    hedges_norm = (confidencehedged/tokens_total)*100,\n    boosters_norm = (confidencehigh/tokens_total)*100,\n  )",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#check-assumptions",
    "href": "labs/logistic-regression.html#check-assumptions",
    "title": "8  Logistic Regression",
    "section": "8.5 Check assumptions",
    "text": "8.5 Check assumptions\nThis time we’ll do a quick check for colinearity before building the model by calculating the correlation between frequencies of hedges and boosters\n\ncor(hb$hedges_norm, hb$boosters_norm)\n\n[1] 0.02185522\n\n\nHow does this look? Check Brezina pg. 121.\nNow lets check some distributions. First we’ll create a data structure for ggplot.\n\nhb_df &lt;- hb %&gt;% \n  select(hedges_norm, boosters_norm) %&gt;% \n  pivot_longer(everything(), names_to = \"confidence\", values_to = \"freq_norm\")\n\nNow plot histograms.\n\nggplot(hb_df,aes(x = freq_norm, color = confidence, fill = confidence)) + \n  geom_histogram(bins = 10, alpha=.5, position = \"identity\") +\n  theme_classic() +\n  theme(axis.text = element_text(size=5)) +\n  facet_wrap(~ confidence)\n\n\n\n\nHistograms of hedging and boosing tokens in MICUSP.\n\n\n\n\nAnd boxplots.\n\nggplot(hb_df,aes(x = confidence, y = freq_norm)) + \n  geom_boxplot() +\n  xlab(\"\") +\n  ylab(\"Frequency (per 100 tokens)\") +\n  scale_x_discrete(labels= c(\"Boosters\", \"Hedges\")) +\n  theme_classic() +\n  coord_flip()\n\n\n\n\nFrequencies (per 100 tokens) of hedges and boosters in MICUSP.\n\n\n\n\nHow do these look to you?",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#format-the-data",
    "href": "labs/logistic-regression.html#format-the-data",
    "title": "8  Logistic Regression",
    "section": "8.6 Format the data",
    "text": "8.6 Format the data\nNow let’s create some data for our regression models. For this, we’ll combine our frequency counts with some metadata: discipline category, speaker status, gender, and paper type. We’ll also move the text_id to row names to exclude that column from further processing.\n\nlr_df &lt;- hb %&gt;% \n  mutate(doc_id = str_remove_all(doc_id, \".txt\")) %&gt;% \n  dplyr::select(doc_id, hedges_norm, boosters_norm) %&gt;% \n  left_join(select(micusp_meta, doc_id, discipline_cat, speaker_status, student_gender, paper_type), by =\"doc_id\") %&gt;% \n  remove_rownames %&gt;% column_to_rownames(var=\"doc_id\")\n\nFor the mulinomial regression, we’re going to want to collapse all of the discipline categories into 3: Science, Humanities, and Social Science.\n\nlr_df$discipline_cat &lt;- str_replace_all(lr_df$discipline_cat, \"BIO|CEE|ECO|IOE|MEC|NRE|PHY\", \"SCI\")\nlr_df$discipline_cat &lt;- str_replace_all(lr_df$discipline_cat, \"CLS|ENG|HIS|PHI\", \"HUM\")\nlr_df$discipline_cat &lt;- str_replace_all(lr_df$discipline_cat, \"ECO|EDU|LIN|NUR|POL|PSY|SOC\", \"SOCSCI\")\n\nTo carry out our regression, we need to convert our character columns to factors. In other words, they need to be treated like categories not strings. We can do them all with one simple line of code.\n\nlr_df &lt;- lr_df %&gt;%  mutate_if(is.character, as.factor)",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#logistic-regression-model-1",
    "href": "labs/logistic-regression.html#logistic-regression-model-1",
    "title": "8  Logistic Regression",
    "section": "8.7 Logistic regression model",
    "text": "8.7 Logistic regression model\nWe’ll start with student gender as our outcome variable and hedges and boosters as our predictors. The family argument specifies logistic regression.\n\nglm_fit1 &lt;- glm(student_gender ~ boosters_norm + hedges_norm, data = lr_df, family = \"binomial\")\n\nAnd we do something similar for speaker status.\n\nglm_fit2 &lt;- glm(speaker_status ~ boosters_norm + hedges_norm, data = lr_df, family = \"binomial\")\n\nAnd finally, let’s repeat this process with a subset of our data. We have 3 discipline categories, so let’s subset out only 2.\n\nlr_sub &lt;- lr_df %&gt;% filter(discipline_cat == \"HUM\" | discipline_cat == \"SCI\")\nlr_sub$discipline_cat &lt;- droplevels(lr_sub$discipline_cat)\n\nglm_fit3 &lt;- glm(discipline_cat ~ boosters_norm + hedges_norm, data = lr_sub, family = \"binomial\")\n\n\n\nCode\ntbl_1 &lt;- gtsummary::tbl_regression(glm_fit1, exponentiate = TRUE) |&gt;\n  gtsummary::add_global_p()\n\ntbl_2 &lt;- gtsummary::tbl_regression(glm_fit2, exponentiate = TRUE) |&gt;\n  gtsummary::add_global_p()\n\ntbl_3 &lt;- gtsummary::tbl_regression(glm_fit3, exponentiate = TRUE) |&gt;\n  gtsummary::add_global_p()\n\ngtsummary::tbl_merge(\n    list(tbl_1, tbl_2, tbl_3),\n    tab_spanner = c(\"**Model 1**\", \"**Model 2**\", \"**Model 3**\")\n  )\n\n\n\n\n\n\nLogistic regression estimates of hedges or boosters for student gender (Model 1), spaker status (Model 2), and discipline (Model 3).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\nOR\n1\n95% CI\n1\np-value\nOR\n1\n95% CI\n1\np-value\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nboosters_norm\n1.79\n0.83, 4.82\n0.15\n6.49\n1.27, 43.3\n0.021\n0.01\n0.00, 0.08\n&lt;0.001\n\n\nhedges_norm\n0.90\n0.43, 1.86\n0.8\n0.91\n0.35, 2.57\n0.9\n11.9\n2.94, 62.4\n&lt;0.001\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nWhat can we gather from the model summaries?",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/logistic-regression.html#multinomial-regression-model",
    "href": "labs/logistic-regression.html#multinomial-regression-model",
    "title": "8  Logistic Regression",
    "section": "8.8 Multinomial regression model",
    "text": "8.8 Multinomial regression model\nNow let’s try multinomial regression on all 3 of the discipline categories. This isn’t covered in the textbook, but it’s worth looking at even if briefly.\n\nmr_fit &lt;- multinom(discipline_cat ~ boosters_norm + hedges_norm, data = lr_df)\n\n# weights:  12 (6 variable)\ninitial  value 186.764089 \niter  10 value 160.727032\nfinal  value 160.726590 \nconverged\n\n\nWe first see that some output is generated by running the model, even though we are assigning the model to a new R object. This model-running output includes some iteration history and includes the final log-likelihood 814.275451. This value multiplied by two is then seen in the model summary as the Residual Deviance and it can be used in comparisons of nested models.\nLet’s look at a couple of boxplots to give us some context for these numbers.\n\nggplot(lr_df, aes(x = reorder(discipline_cat, boosters_norm, FUN = median), y = boosters_norm)) +\n  geom_boxplot() +\n  xlab(\"\") +\n  ylab(\"Boosters (per 100 tokens)\") +\n  theme_classic() +\n  coord_flip()\n\n\n\n\nBoxplots of boosting tokens by disciplinary category in MICUSP.\n\n\n\n\n\nggplot(lr_df, aes(x = reorder(discipline_cat, hedges_norm, FUN = median), y = hedges_norm)) +\n  geom_boxplot() +\n  xlab(\"\") +\n  ylab(\"Hedges (per 100 tokens)\") +\n  theme_classic() +\n  coord_flip()\n\n\n\n\nBoxplots of hedging tokens by disciplinary category in MICUSP.\n\n\n\n\nMuch like logistic regression, th ratio of the probability of choosing one outcome category over the probability of choosing the baseline category is the relative risk or odds. The relative risk is the right-hand side linear equation exponentiated, leading to the fact that the exponentiated regression coefficients are relative risk ratios for a unit change in the predictor variable. We can exponentiate the coefficients from our model to see these odds ratios.\n\n\nCode\nmultinom_pivot_wider &lt;- function(x) {\n  # create tibble of results\n  df &lt;- tibble::tibble(outcome_level = unique(x$table_body$groupname_col))\n  df$tbl &lt;- \n    purrr::map(\n      df$outcome_level,\n      function(lvl) {\n        gtsummary::modify_table_body(\n          x, \n          ~dplyr::filter(.x, .data$groupname_col %in% lvl) %&gt;%\n            dplyr::ungroup() %&gt;%\n            dplyr::select(-.data$groupname_col)\n        )\n      }\n    )\n  \n  gtsummary::tbl_merge(df$tbl, tab_spanner = paste0(\"**\", df$outcome_level, \"**\"))\n}\n\nnnet::multinom(discipline_cat ~ boosters_norm + hedges_norm, data = lr_df) %&gt;%\n  gtsummary::tbl_regression(exponentiate = TRUE) |&gt;\n  gtsummary::add_global_p() |&gt;\n  multinom_pivot_wider()\n\n\n# weights:  12 (6 variable)\ninitial  value 186.764089 \niter  10 value 160.727032\nfinal  value 160.726590 \nconverged\n\n\n\n\n\n\nMultinomial regression estimates of hedges or boosters.\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nSCI\n\n\nSOCSCI\n\n\n\nOR\n1\n95% CI\n1\np-value\nOR\n1\n95% CI\n1\np-value\n\n\n\n\nboosters_norm\n0.01\n0.00, 0.07\n&lt;0.001\n0.28\n0.07, 1.10\n&lt;0.001\n\n\nhedges_norm\n7.36\n2.14, 25.4\n0.002\n2.72\n0.82, 9.04\n0.002\n\n\n\n1\nOR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\nSometimes a plot can be helpful in interpreting the results. Let’s start by making some dummy data. For this we’ll sequence frequencies of hedges from 0 to 6 percent of a text, and frequencies of boosters on an inverse scale: from 6 to 0 percent of a text. In essence, we creating hypothetical texts that have at one end have low frequencies of hedges and high frequencies of boosters, have balanced frequencies in the middle, and have high frequencies of hedges and low frequencies of boosters.\n\nhb_new &lt;- data.frame(hedges_norm = seq(0, 6, by = .1), boosters_norm = seq(6, 0, by = -.1))\n\nNext, we create a data frame of discipline probabilities based on our fit.\n\nprob_disc &lt;- cbind(hb_new, predict(mr_fit, newdata = hb_new, type = \"probs\", se = TRUE))\n\nWe’ll format a data frame for plotting.\n\nplot_prob &lt;- prob_disc %&gt;% \n  pivot_longer(hedges_norm:boosters_norm, names_to = \"feature\", values_to = \"confidence\") %&gt;% \n  pivot_longer(HUM:SOCSCI, names_to = \"variable\", values_to = \"probability\")\n\nFinally, we’ll create a plot the probabilities and color by hedges & boosters.\n\nggplot(plot_prob, aes(x = confidence, y = probability, color = feature)) + geom_line() + \n  theme_classic() +\n  facet_grid(variable ~ ., scales = \"free\")\n\n\n\n\nPredicted probabilities across token frequencies for hedging and boosting facetted by disciplinary category.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "labs/multi-dimensional-analysis.html",
    "href": "labs/multi-dimensional-analysis.html",
    "title": "9  Multi-Dimensional Analysis",
    "section": "",
    "text": "9.1 Case 1: Biber Tagger\nIn order to carry out MDA, we would like to have 5 times as many observations than variables. This generally precludes carrying out MDA (or factor analysis) with simple word counts. We need data that has, in some way, been tagged.\nFor this lab, we will use data prepared using the R package pseudobibeR, which emulates the classification system that Biber has used and reported in much of his research. The package aggregates the lexicogrammatical and functional features widely used for text-type, register, and genre classification tasks.\nThe scripts are not really taggers. Rather, they use udpipe or spaCy part-of-speech tagging and dependency parsing to summarize patterns. They organize 67 categories that are described here:\nhttps://cmu-textstat-docs.readthedocs.io/en/latest/pseudobibeR/pseudobibeR.html\nFor this lab, you won’t need to use the package functions. But if you’d like to try it out for any of your projects, you can follow the instructions here:\nhttps://cmu-textstat-docs.readthedocs.io/en/latest/pseudobibeR/pseudobibeR.html",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "labs/multi-dimensional-analysis.html#case-1-biber-tagger",
    "href": "labs/multi-dimensional-analysis.html#case-1-biber-tagger",
    "title": "9  Multi-Dimensional Analysis",
    "section": "",
    "text": "9.1.1 The Brown Corpus\nLet’s start with counts from the Brown Corpus. The Brown family of corpora is discussed on pg. 16 of Brezina. You can also find more about it here:\nhttp://icame.uib.no/brown/bcm.html\n\nbc &lt;- read_csv(\"https://raw.githubusercontent.com/browndw/cmu-textstat-docs/main/docs/_static/labs_files/data-csv/bc_biber.csv\", show_col_types = FALSE)\n\nbc_meta &lt;- read_csv(\"https://raw.githubusercontent.com/browndw/cmu-textstat-docs/main/docs/_static/labs_files/data-csv/brown_meta.csv\", show_col_types = FALSE)\n\nWe will join the data with the metadata, in order to calculate dimension scores by register and evaluate them. Note that it must be formatted as a factor. For convenience sake, we’ll move the file names to the row names and put our factor as the first column.\n\nbc &lt;- bc %&gt;%\n  left_join(dplyr::select(bc_meta, doc_id, text_type)) %&gt;%\n  mutate(text_type = as.factor(text_type)) %&gt;%\n  column_to_rownames(\"doc_id\") %&gt;%\n  dplyr::select(text_type, everything())\n\n\n\n9.1.2 Correlation matrix\nBefore calculating our factors, let’s check a correlation matrix. Note that we’re dropping the first (factor) column.\n\nbc_cor &lt;- cor(bc[-1], method = \"pearson\")\n\n\ncorrplot::corrplot(bc_cor, type = \"upper\", order = \"hclust\", \n         tl.col = \"black\", tl.srt = 45, diag = F, tl.cex = 0.5)\n\n\n\n\nCorrelation matrix of lexico-grammatical categories.\n\n\n\n\n\n\n9.1.3 Determining number of factors\nTypically, the number of factors is chosen after inspecting a scree plot.\n\nscreeplot_mda(bc)\n\n\n\n\nScree plot of factors.\n\n\n\n\nA common method for interpreting a scree plot is to look for the “bend” in the elbow, which would be 3 or 4 factors in this case. We can also look at the results of other kinds of solutions like optimal coordinates, which measures the gradients associated with eigenvalues and their preceding coordinates, and acceleration factor, which determines the coordinate where the slope of the curve changes most abruptly. In this case OC suggests 6 factors and AF 1.\nFor the purposes of this exercise, we’ll start with 3 factors.\n\n\n9.1.4 Calculating factor loadings and MDA scores\nIn factor analysis factors so that they pass through the middle of the relevant variables. For linguistic variable it is conventional to use a promax rotation (see Brezina pgs. 164-167). There is also a nice explanation of rotations here:\nhttps://personal.utdallas.edu/~herve/Abdi-rotations-pretty.pdf\nTo place our categories along the dimensions, data is standardized by converting to z-scores. For each text, a dimension score is calculated by summing all of the high-positive variables subtracting all of the high-negative variables. Then, the mean is calculated for each category.\nFor these calculations, we will use the mda_loadings() function.\n\nbc_mda &lt;- mda_loadings(bc, n_factors = 3)\n\nWe can access factor loadings and group means through attributes.\n\nattr(bc_mda, 'loadings')|&gt;\n  rownames_to_column(\"Feature\") |&gt;\n  gt() |&gt; \n  fmt_number(\n    columns = everything(),\n    decimals = 2\n  ) |&gt;\n  as_raw_html()\n\n\n  \n  \n\nFactor loadings for the Brown Corpus.\n\n\nFeature\nFactor1\nFactor2\nFactor3\n\n\n\n\nf_01_past_tense\n−0.15\n1.10\n0.16\n\n\nf_02_perfect_aspect\n0.05\n0.52\n0.28\n\n\nf_03_present_tense\n0.60\n−1.06\n−0.04\n\n\nf_04_place_adverbials\n0.17\n0.42\n−0.11\n\n\nf_05_time_adverbials\n0.23\n0.32\n0.06\n\n\nf_06_first_person_pronouns\n0.44\n0.15\n0.22\n\n\nf_07_second_person_pronouns\n0.66\n−0.11\n−0.14\n\n\nf_08_third_person_pronouns\n0.19\n0.73\n0.16\n\n\nf_09_pronoun_it\n0.32\n0.11\n0.39\n\n\nf_10_demonstrative_pronoun\n0.36\n−0.15\n0.37\n\n\nf_11_indefinite_pronouns\n0.49\n0.30\n0.26\n\n\nf_12_proverb_do\n0.54\n0.04\n0.14\n\n\nf_13_wh_question\n0.39\n0.10\n0.05\n\n\nf_14_nominalizations\n−0.51\n−0.39\n0.21\n\n\nf_15_gerunds\n−0.02\n−0.16\n−0.03\n\n\nf_16_other_nouns\n−0.29\n−0.32\n−0.77\n\n\nf_17_agentless_passives\n−0.48\n−0.21\n0.00\n\n\nf_18_by_passives\n−0.45\n−0.16\n0.02\n\n\nf_19_be_main_verb\n0.46\n−0.09\n0.41\n\n\nf_20_existential_there\n0.13\n0.13\n0.30\n\n\nf_21_that_verb_comp\n−0.18\n0.08\n0.44\n\n\nf_22_that_adj_comp\n0.01\n−0.10\n0.38\n\n\nf_23_wh_clause\n0.36\n0.23\n0.22\n\n\nf_24_infinitives\n0.10\n0.01\n0.24\n\n\nf_25_present_participle\n0.12\n0.48\n−0.03\n\n\nf_27_past_participle_whiz\n−0.53\n0.08\n−0.11\n\n\nf_28_present_participle_whiz\n−0.22\n0.03\n−0.16\n\n\nf_30_that_obj\n0.00\n−0.11\n0.26\n\n\nf_31_wh_subj\n−0.14\n−0.10\n0.14\n\n\nf_32_wh_obj\n−0.03\n0.04\n0.27\n\n\nf_33_pied_piping\n−0.17\n−0.17\n0.34\n\n\nf_34_sentence_relatives\n−0.21\n−0.07\n0.08\n\n\nf_35_because\n0.33\n−0.13\n0.15\n\n\nf_36_though\n−0.20\n0.16\n0.32\n\n\nf_37_if\n0.53\n−0.27\n0.12\n\n\nf_38_other_adv_sub\n0.02\n−0.06\n0.33\n\n\nf_39_prepositions\n−0.68\n−0.23\n−0.06\n\n\nf_40_adj_attr\n−0.41\n−0.47\n0.18\n\n\nf_41_adj_pred\n0.14\n0.16\n0.26\n\n\nf_42_adverbs\n0.57\n0.21\n0.51\n\n\nf_43_type_token\n0.14\n0.12\n−0.06\n\n\nf_44_mean_word_length\n−0.65\n−0.36\n0.04\n\n\nf_45_conjuncts\n−0.20\n−0.41\n0.37\n\n\nf_46_downtoners\n0.13\n−0.03\n0.38\n\n\nf_47_hedges\n0.40\n0.10\n0.24\n\n\nf_48_amplifiers\n0.04\n−0.11\n0.36\n\n\nf_49_emphatics\n0.41\n−0.26\n0.26\n\n\nf_50_discourse_particles\n0.41\n0.07\n0.00\n\n\nf_51_demonstratives\n−0.10\n−0.31\n0.31\n\n\nf_52_modal_possibility\n0.45\n−0.39\n0.29\n\n\nf_53_modal_necessity\n0.10\n−0.36\n0.19\n\n\nf_54_modal_predictive\n0.43\n−0.12\n−0.09\n\n\nf_55_verb_public\n0.07\n0.38\n0.09\n\n\nf_56_verb_private\n0.25\n0.44\n0.44\n\n\nf_57_verb_suasive\n−0.12\n0.03\n0.03\n\n\nf_58_verb_seem\n−0.02\n0.13\n0.39\n\n\nf_59_contractions\n0.61\n0.25\n−0.03\n\n\nf_60_that_deletion\n0.25\n0.26\n0.03\n\n\nf_63_split_auxiliary\n−0.04\n−0.07\n0.37\n\n\nf_64_phrasal_coordination\n−0.15\n−0.39\n−0.02\n\n\nf_65_clausal_coordination\n0.47\n0.37\n0.28\n\n\nf_66_neg_synthetic\n0.07\n0.32\n0.35\n\n\nf_67_neg_analytic\n0.57\n0.20\n0.38\n\n\n\n\n\n\n\n\n\n9.1.5 Plotting the results\nThe means are conventionally positioned on a stick plot of the kind Brezina shows on pg. 169.\n\nstickplot_mda(bc_mda, n_factor = 1)\n\n\n\n\nDimension score means by discipline plotted along Factor 1.\n\n\n\n\nWe can also show the same plot with the factor loadings.\n\nheatmap_mda(bc_mda, n_factor = 1)\n\n\n\n\nDimension score means by discipline plotted along Factor 1.\n\n\n\n\n\n\n9.1.6 Evaluating MDA\nTypically, MDA is evaluated using ANOVA, reporting the F statistic, degrees of freedom, and R-squared. We can extract that information from a linear model.\n\nf_aov &lt;- aov(Factor1 ~ group, data = bc_mda)\nbroom::tidy(f_aov)\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic   p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 group        14 51553.  3682.      32.1  3.49e-60\n2 Residuals   485 55651.   115.      NA   NA       \n\n\n\ngtsummary::tbl_regression(lm(Factor1 ~ group, data = bc_mda)) |&gt;\n  gtsummary::add_glance_source_note() |&gt;\n  gtsummary::as_gt() |&gt;\n  as_raw_html()\n\n\n  \n  \n  \n    \n      Characteristic\n\n      Beta\n\n      95% CI\n1\n      p-value\n\n    \n  \n  \n    group\n\n\n\n        BELLES-LETTRES\n—\n—\n\n        FICTION: ADVENTURE\n14\n9.0, 18\n&lt;0.001\n        FICTION: GENERAL\n12\n7.1, 16\n&lt;0.001\n        FICTION: MYSTERY\n20\n15, 25\n&lt;0.001\n        FICTION: ROMANCE\n22\n17, 27\n&lt;0.001\n        FICTION: SCIENCE\n16\n7.1, 25\n&lt;0.001\n        HUMOR\n12\n4.8, 20\n0.001\n        LEARNED\n-9.7\n-13, -6.3\n&lt;0.001\n        MISCELLANEOUS: GOVERNMENT & HOUSE ORGANS\n-14\n-19, -9.6\n&lt;0.001\n        POPULAR LORE\n-1.2\n-5.1, 2.7\n0.5\n        PRESS: EDITORIAL\n3.2\n-1.6, 7.9\n0.2\n        PRESS: REPORTAGE\n-6.9\n-11, -2.9\n&lt;0.001\n        PRESS: REVIEWS\n-0.57\n-6.2, 5.1\n0.8\n        RELIGION\n3.3\n-2.3, 9.0\n0.2\n        SKILL AND HOBBIES\n-0.51\n-4.8, 3.8\n0.8\n  \n  \n    \n      R² = 0.481; Adjusted R² = 0.466; Sigma = 10.7; Statistic = 32.1; p-value = &lt;0.001; df = 14; Log-likelihood = -1,888; AIC = 3,807; BIC = 3,874; Deviance = 55,651; Residual df = 485; No. Obs. = 500\n\n    \n  \n  \n    \n      1 CI = Confidence Interval\n\n    \n  \n\n\n\n\n\ngtsummary::tbl_regression(lm(Factor2 ~ group, data = bc_mda)) |&gt;\n  gtsummary::add_glance_source_note() |&gt;\n  gtsummary::as_gt() |&gt;\n  as_raw_html()\n\n\n  \n  \n  \n    \n      Characteristic\n\n      Beta\n\n      95% CI\n1\n      p-value\n\n    \n  \n  \n    group\n\n\n\n        BELLES-LETTRES\n—\n—\n\n        FICTION: ADVENTURE\n15\n12, 17\n&lt;0.001\n        FICTION: GENERAL\n13\n11, 16\n&lt;0.001\n        FICTION: MYSTERY\n14\n12, 17\n&lt;0.001\n        FICTION: ROMANCE\n14\n12, 17\n&lt;0.001\n        FICTION: SCIENCE\n8.1\n3.3, 13\n0.001\n        HUMOR\n7.3\n3.2, 11\n&lt;0.001\n        LEARNED\n-7.8\n-9.6, -5.9\n&lt;0.001\n        MISCELLANEOUS: GOVERNMENT & HOUSE ORGANS\n-9.6\n-12, -7.1\n&lt;0.001\n        POPULAR LORE\n-0.58\n-2.7, 1.5\n0.6\n        PRESS: EDITORIAL\n-2.3\n-4.8, 0.28\n0.081\n        PRESS: REPORTAGE\n0.71\n-1.5, 2.9\n0.5\n        PRESS: REVIEWS\n-2.9\n-6.0, 0.18\n0.065\n        RELIGION\n-3.8\n-6.9, -0.75\n0.015\n        SKILL AND HOBBIES\n-5.6\n-7.9, -3.2\n&lt;0.001\n  \n  \n    \n      R² = 0.662; Adjusted R² = 0.652; Sigma = 5.82; Statistic = 67.7; p-value = &lt;0.001; df = 14; Log-likelihood = -1,582; AIC = 3,196; BIC = 3,264; Deviance = 16,404; Residual df = 485; No. Obs. = 500\n\n    \n  \n  \n    \n      1 CI = Confidence Interval\n\n    \n  \n\n\n\n\n\ngtsummary::tbl_regression(lm(Factor3 ~ group, data = bc_mda)) |&gt;\n  gtsummary::add_glance_source_note() |&gt;\n  gtsummary::as_gt() |&gt;\n  as_raw_html()\n\n\n  \n  \n  \n    \n      Characteristic\n\n      Beta\n\n      95% CI\n1\n      p-value\n\n    \n  \n  \n    group\n\n\n\n        BELLES-LETTRES\n—\n—\n\n        FICTION: ADVENTURE\n-0.81\n-3.7, 2.1\n0.6\n        FICTION: GENERAL\n-0.23\n-3.1, 2.7\n0.9\n        FICTION: MYSTERY\n3.1\n-0.02, 6.2\n0.051\n        FICTION: ROMANCE\n2.9\n0.02, 5.8\n0.048\n        FICTION: SCIENCE\n5.4\n-0.21, 11\n0.059\n        HUMOR\n2.9\n-1.7, 7.6\n0.2\n        LEARNED\n-0.11\n-2.2, 2.0\n&gt;0.9\n        MISCELLANEOUS: GOVERNMENT & HOUSE ORGANS\n-9.2\n-12, -6.3\n&lt;0.001\n        POPULAR LORE\n-2.4\n-4.8, 0.09\n0.059\n        PRESS: EDITORIAL\n-0.87\n-3.8, 2.1\n0.6\n        PRESS: REPORTAGE\n-10\n-13, -7.8\n&lt;0.001\n        PRESS: REVIEWS\n-3.9\n-7.4, -0.29\n0.034\n        RELIGION\n4.3\n0.73, 7.9\n0.018\n        SKILL AND HOBBIES\n-5.1\n-7.8, -2.4\n&lt;0.001\n  \n  \n    \n      R² = 0.273; Adjusted R² = 0.252; Sigma = 6.76; Statistic = 13.0; p-value = &lt;0.001; df = 14; Log-likelihood = -1,657; AIC = 3,346; BIC = 3,414; Deviance = 22,146; Residual df = 485; No. Obs. = 500\n\n    \n  \n  \n    \n      1 CI = Confidence Interval",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "labs/multi-dimensional-analysis.html#case-2-docuscope",
    "href": "labs/multi-dimensional-analysis.html#case-2-docuscope",
    "title": "9  Multi-Dimensional Analysis",
    "section": "9.2 Case 2: DocuScope",
    "text": "9.2 Case 2: DocuScope\nUnlike the Biber tagger, DocuScope is a dictionary based tagger. It has been developed at CMU by David Kaufer and Suguru Ishizaki since the early 2000s. A small version of it is on Canvas, which you can download. You can find the dictionary categories described here:\nhttps://docuscospacy.readthedocs.io/en/latest/docuscope.html#categories\n\n9.2.1 Load the dictionary\n\nload(\"../data/ds_dict.rda\")\nload(\"../data/micusp_mini.rda\")\n\nDocuScope is a very large dictionary (or lexicon) that organizes tens of millions of words and phrases into rhetorically oriented categories. It has some overlap with a few Biber’s functional categories (like hedges), but is fundamentally different, as it isn’t bases on parts-of-speech.\nThe ds_dict is a small quanteda dictionary that organizes a smaller set of words of phrases (tens of thousands rather than tens of millions). Here is a sample from 3 of the categories:\n\nds_dict[1:3]\n\nDictionary object with 3 key entries.\n- [AcademicTerms]:\n  - a chapter in, a couple, a declaration of, a detail, a distinction between, a domain, a force, a forced, a form of, a grade, a hint of, a home for, a hub, a kind of, a kind of a, a load, a loaded, a metaphor for, a mix of, a mixture of [ ... and 8,884 more ]\n- [AcademicWritingMoves]:\n  - . in this article ,, . in this paper, . this essay, . this paper, . this report, . this work, . to avoid, a better understanding, a common problem, a debate about, a debate over, a first step, a goal of, a great deal of attention, a huge problem, a key to, a major problem, a method of, a notion that, a number of studies [ ... and 1,141 more ]\n- [Character]:\n  - ; block, ; bring, ; call, ; center, ; check, ; chill, ; close, ; color, ; control, ; cook, ; cool, ; cover, ; cross, ; cut, ; design, ; discard, ; don, ; down, ; drain, ; e-mail [ ... and 18,754 more ]\n\n\n\n\n9.2.2 Tokenize the corpus\nAgain, we will use the **micusp_mini*, and we’ll begin by tokenizing the data. Note that we’re retaining as much of the original data as possible including punctuation. This is because our dictionary includes punctuation marks in it’s entries.\n\nmicusp_tokens &lt;- micusp_mini %&gt;%\n  corpus() %&gt;%\n  tokens(remove_punct = F, remove_numbers = F, remove_symbols = F, what = \"word\")\n\nNext, we will use the tokens_lookup() function to count and categorize our features.\n\nds_counts &lt;- micusp_tokens %&gt;%\n  tokens_lookup(dictionary = ds_dict, levels = 1, valuetype = \"fixed\") %&gt;%\n  dfm() %&gt;%\n  convert(to = \"data.frame\") %&gt;%\n  as_tibble()\n\nFinally, we need to normalize the counts. Because DocuScope is not categorizing ALL of our tokens, we need a total count from the original tokens object.\n\ntot_counts &lt;- quanteda::ntoken(micusp_tokens) %&gt;%\n  data.frame(tot_counts = .) %&gt;%\n  tibble::rownames_to_column(\"doc_id\") %&gt;%\n  dplyr::as_tibble()\n\nds_counts &lt;- dplyr::full_join(ds_counts, tot_counts, by = \"doc_id\")\n\nNow we can normalize by the total counts before preparing the data for factor analysis.\n\nds_counts &lt;- ds_counts %&gt;%\n  dplyr::mutate_if(is.numeric, list(~./tot_counts), na.rm = TRUE) %&gt;%\n  dplyr::mutate_if(is.numeric, list(~.*100), na.rm = TRUE) %&gt;%\n  dplyr::select(-tot_counts)\n\nds_counts &lt;- ds_counts %&gt;%\n  mutate(text_type = str_extract(doc_id, \"^[A-Z]+\")) %&gt;%\n  mutate(text_type = as.factor(text_type)) %&gt;%\n  column_to_rownames(\"doc_id\")\n\n\n\n9.2.3 Calculating factor loadings and MDA score\nAgain, we will use 3 factors.\n\nmicusp_mda &lt;- mda_loadings(ds_counts, n_factors = 3)\n\n\n\n9.2.4 Evaluating MDA\nWe can again check to see how explanatory our dimensions are.\n\ngtsummary::tbl_regression(lm(Factor1 ~ group, data = micusp_mda)) |&gt;\n  gtsummary::add_glance_source_note() |&gt;\n  gtsummary::as_gt() |&gt;\n  as_raw_html()\n\n\n  \n  \n  \n    \n      Characteristic\n\n      Beta\n\n      95% CI\n1\n      p-value\n\n    \n  \n  \n    group\n\n\n\n        BIO\n—\n—\n\n        CEE\n-4.3\n-7.3, -1.3\n0.005\n        CLS\n7.7\n4.8, 11\n&lt;0.001\n        ECO\n0.40\n-2.6, 3.4\n0.8\n        EDU\n5.8\n2.9, 8.8\n&lt;0.001\n        ENG\n10\n7.3, 13\n&lt;0.001\n        HIS\n4.8\n1.8, 7.8\n0.002\n        IOE\n-0.29\n-3.3, 2.7\n0.8\n        LIN\n2.8\n-0.19, 5.7\n0.066\n        MEC\n-4.8\n-7.8, -1.9\n0.002\n        NRE\n0.20\n-2.8, 3.2\n0.9\n        NUR\n3.4\n0.40, 6.3\n0.026\n        PHI\n9.5\n6.5, 12\n&lt;0.001\n        PHY\n-1.9\n-4.9, 1.1\n0.2\n        POL\n6.0\n3.0, 9.0\n&lt;0.001\n        PSY\n3.2\n0.20, 6.1\n0.037\n        SOC\n5.1\n2.2, 8.1\n&lt;0.001\n  \n  \n    \n      R² = 0.646; Adjusted R² = 0.609; Sigma = 3.36; Statistic = 17.5; p-value = &lt;0.001; df = 16; Log-likelihood = -438; AIC = 912; BIC = 969; Deviance = 1,722; Residual df = 153; No. Obs. = 170\n\n    \n  \n  \n    \n      1 CI = Confidence Interval\n\n    \n  \n\n\n\n\n\ngtsummary::tbl_regression(lm(Factor2 ~ group, data = micusp_mda)) |&gt;\n  gtsummary::add_glance_source_note() |&gt;\n  gtsummary::as_gt() |&gt;\n  as_raw_html()\n\n\n  \n  \n  \n    \n      Characteristic\n\n      Beta\n\n      95% CI\n1\n      p-value\n\n    \n  \n  \n    group\n\n\n\n        BIO\n—\n—\n\n        CEE\n-1.2\n-4.7, 2.3\n0.5\n        CLS\n-7.6\n-11, -4.1\n&lt;0.001\n        ECO\n-3.3\n-6.8, 0.24\n0.067\n        EDU\n-2.9\n-6.4, 0.65\n0.11\n        ENG\n-8.2\n-12, -4.7\n&lt;0.001\n        HIS\n-10\n-14, -6.5\n&lt;0.001\n        IOE\n-4.0\n-7.5, -0.53\n0.024\n        LIN\n-0.51\n-4.0, 3.0\n0.8\n        MEC\n-1.3\n-4.8, 2.2\n0.5\n        NRE\n-8.4\n-12, -4.9\n&lt;0.001\n        NUR\n-3.3\n-6.8, 0.16\n0.061\n        PHI\n-1.6\n-5.1, 1.9\n0.4\n        PHY\n-1.0\n-4.5, 2.5\n0.6\n        POL\n-13\n-17, -9.7\n&lt;0.001\n        PSY\n-1.3\n-4.8, 2.2\n0.5\n        SOC\n-7.2\n-11, -3.7\n&lt;0.001\n  \n  \n    \n      R² = 0.506; Adjusted R² = 0.454; Sigma = 3.96; Statistic = 9.78; p-value = &lt;0.001; df = 16; Log-likelihood = -466; AIC = 969; BIC = 1,025; Deviance = 2,405; Residual df = 153; No. Obs. = 170\n\n    \n  \n  \n    \n      1 CI = Confidence Interval\n\n    \n  \n\n\n\n\n\ngtsummary::tbl_regression(lm(Factor3 ~ group, data = micusp_mda)) |&gt;\n  gtsummary::add_glance_source_note() |&gt;\n  gtsummary::as_gt() |&gt;\n  as_raw_html()\n\n\n  \n  \n  \n    \n      Characteristic\n\n      Beta\n\n      95% CI\n1\n      p-value\n\n    \n  \n  \n    group\n\n\n\n        BIO\n—\n—\n\n        CEE\n0.18\n-2.5, 2.9\n0.9\n        CLS\n-1.7\n-4.4, 0.96\n0.2\n        ECO\n3.5\n0.84, 6.2\n0.010\n        EDU\n3.0\n0.29, 5.6\n0.030\n        ENG\n-2.2\n-4.9, 0.49\n0.11\n        HIS\n-3.2\n-5.9, -0.55\n0.019\n        IOE\n3.4\n0.75, 6.1\n0.012\n        LIN\n-0.97\n-3.6, 1.7\n0.5\n        MEC\n-0.50\n-3.2, 2.2\n0.7\n        NRE\n3.0\n0.37, 5.7\n0.026\n        NUR\n5.2\n2.6, 7.9\n&lt;0.001\n        PHI\n-0.73\n-3.4, 1.9\n0.6\n        PHY\n-1.3\n-4.0, 1.4\n0.3\n        POL\n1.5\n-1.2, 4.2\n0.3\n        PSY\n0.08\n-2.6, 2.8\n&gt;0.9\n        SOC\n-0.04\n-2.7, 2.6\n&gt;0.9\n  \n  \n    \n      R² = 0.387; Adjusted R² = 0.322; Sigma = 3.02; Statistic = 6.03; p-value = &lt;0.001; df = 16; Log-likelihood = -420; AIC = 877; BIC = 933; Deviance = 1,398; Residual df = 153; No. Obs. = 170\n\n    \n  \n  \n    \n      1 CI = Confidence Interval\n\n    \n  \n\n\n\n\n\n\n9.2.5 Plotting the results\nAnd we can plot the first factor.\n\nheatmap_mda(micusp_mda, n_factor = 1)\n\n\n\n\nDimension score means by discipline plotted along Factor 1.\n\n\n\n\n\n\n9.2.6 Interpreting the factors as dimensions\nThe functional interpretation of factors as dimensions (Brezina pgs. 167-168) is probably the most challenging part of MDA. As analysts, we need to make sense out of why features (whether parts-of-speech, rhetorical categories, or other measures) are grouping together and contributing to the patterns of variation evident in products of the analysis.\nThat interpretation usually involves giving names to the dimensions based on their constituent structures. In Biber’s original study, he called his first, most explanatory dimension Involved vs. Informational Production. At the positive (Involved) end of the dimension are telephone and face-to-face conversations. At the negative (Information) end are official documents and academic prose.\nFeatures with high positive loadings include private verbs (like think), contractions, and first and second person pronouns. Features with high negative loadings include nouns and propositional phrases. Biber concludes that these patterns reflect the communicative purposes of the registers. Ones that are more interactive and affective vs. others that are more instructive and informative.\nIn order to understand how certain features are functioning, it is important to see how they are being used, which we can do effienciently with Key Words in Context (KWIC). Here we take “Confidence High” from the positive end of the dimension and “Academic Writing Moves” from the negative.\n\nch &lt;- kwic(micusp_tokens, ds_dict[\"ConfidenceHigh\"])\n\nawm &lt;- kwic(micusp_tokens, ds_dict[\"AcademicWritingMoves\"])\n\n\n\nCode\nch |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nBIO.G0.02.1\n193\n193\nsympatry ; do these examples\nsimply\nrepresent another head on the\nConfidenceHigh\n\n\nBIO.G0.02.1\n405\n406\nspeciation in this genus ,\nmost likely\nunder sympatric conditions . The\nConfidenceHigh\n\n\nBIO.G0.02.1\n712\n712\nthat this mechanism is not\nvery\nefficient , and depends on\nConfidenceHigh\n\n\nBIO.G0.02.1\n1151\n1151\nnormal host species respond in\npredictable\nmanners : choosing to mate\nConfidenceHigh\n\n\nBIO.G0.02.1\n1436\n1437\nexplored later ; however ,\nit is\nimportant to note here that\nConfidenceHigh\n\n\nBIO.G0.02.1\n1731\n1731\nof finding a mate that\nknows\nthe same song as you\nConfidenceHigh\n\n\nBIO.G0.02.1\n1731\n1732\nof finding a mate that\nknows the\nsame song as you may\nConfidenceHigh\n\n\nBIO.G0.02.1\n1755\n1755\nassume that many colonization events\nprobably\noccurred - - only to\nConfidenceHigh\n\n\nBIO.G0.02.1\n1782\n1782\n. , 2004 ) .\nConclusively\nshowing that the observed diversification\nConfidenceHigh\n\n\nBIO.G0.02.1\n1897\n1897\nupon genetic data - -\nobvious\narguments against this methodology and\nConfidenceHigh\n\n\n\n\n\n\n\n\n\nCode\nawm |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  as_raw_html()\n\n\n\n  \n  \n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\n\nBIO.G0.02.1\n260\n262\n, yet another possible example\nhas been described\nby science , which may\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n598\n600\n, this type of behavior\nhas been observed\nin the indigobirds of Vidua\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n920\n921\n( Lonchura striata ) .\nThey conducted\na second experiment in 2000\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n946\n947\n1998 study . Their experiment\nwas designed\nprincipally to test three hypotheses\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n1027\n1028\n, the Bengalese , finch\nwere used\n. In the cross-foster experiments\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n1213\n1214\nbefore fledging - - this\nfinding is\nalso consistent with Payne et\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n1235\n1237\nbrood parasitizing bird species ,\nthese results are\nunique to the Vidua .\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n1236\n1237\nparasitizing bird species , these\nresults are\nunique to the Vidua .\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n1416\n1417\n- - implying that this\nexperimental data\nis supported by observational studies\nAcademicWritingMoves\n\n\nBIO.G0.02.1\n1417\n1418\n- implying that this experimental\ndata is\nsupported by observational studies .\nAcademicWritingMoves",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "labs/cluster-analysis.html",
    "href": "labs/cluster-analysis.html",
    "title": "10  Cluster Analysis",
    "section": "",
    "text": "10.1 Hierarchical Agglomerative Clustering\nWe’ll start with hierarchical agglomerative clustering (pgs. 154, 159 & 236 in Brezina). Hierarchical cluster analysis is visualized with a dendrogram. If you are unfamiliar with these plots, there is a nice explanation here:\nhttps://wheatoncollege.edu/wp-content/uploads/2012/08/How-to-Read-a-Dendrogram-Web-Ready.pdf",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "labs/cluster-analysis.html#hierarchical-agglomerative-clustering",
    "href": "labs/cluster-analysis.html#hierarchical-agglomerative-clustering",
    "title": "10  Cluster Analysis",
    "section": "",
    "text": "10.1.1 Prepare the data\nOur clustering will be based on part-of-speech counts, so we need to parse some data using udpipe. First, we’ll get data from the micusp_mini corpus and subset out the Biology and English papers.\n\nload(\"../data/micusp_mini.rda\")\n\n\nsub_df &lt;- micusp_mini %&gt;%\n  filter(str_detect(doc_id, \"BIO|ENG\"))\n\nAnd parse the data using udpipe. This will take a couple of minutes.\n\nud_model &lt;- udpipe_load_model(\"../models/english-ewt-ud-2.5-191206.udpipe\")\nannotation &lt;- udpipe_annotate(ud_model, x = sub_df$text, doc_id = sub_df$doc_id, parser = \"none\")\n\nNow, we’re going to do something new. We’re going to combine our upos and xpos columns.\n\nanno_edit &lt;- annotation %&gt;%\n  as_tibble() %&gt;%\n  unite(\"upos\", upos:xpos)\n\nNext we create a named list from the new, concatenated column.\n\nsub_tokens &lt;- split(anno_edit$upos, anno_edit$doc_id)\n\nThis is what the data looks like:\n\nsub_tokens$BIO.G0.02.1[1:10]\n\n [1] \"PROPN_NNP\" \"PROPN_NNP\" \"ADV_RB\"    \"VERB_VBD\"  \"PUNCT_,\"   \"PUNCT_``\" \n [7] \"ADJ_JJ\"    \"NOUN_NN\"   \"AUX_VBZ\"   \"ADP_IN\"   \n\n\nNow, we’ll use that as our tokens object and filter out a few of the tokens to simplify our feature matrix.\n\nsub_tokens &lt;- as.tokens(sub_tokens)\nsub_tokens &lt;- tokens_remove(sub_tokens, \"^punct_\\\\S+\", valuetype = \"regex\")\nsub_tokens &lt;- tokens_remove(sub_tokens, \"^sym_\\\\S+\", valuetype = \"regex\")\nsub_tokens &lt;- tokens_remove(sub_tokens, \"^x_\\\\S+\", valuetype = \"regex\")\n\nFrom that, we’ll generate a dfm. We’ll weight the raw counts, and convert the result to a data frame.\n\nsub_dfm &lt;- sub_tokens %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\") %&gt;%\n  convert(to = \"data.frame\")\n\nFinally, we’re going to convert the first row (doc_id) into row names. And, for convenience, we’ll order our columns alphabetically.\n\nsub_dfm &lt;- sub_dfm %&gt;% column_to_rownames(\"doc_id\") %&gt;% \n  dplyr::select(order(colnames(.)))\n\nAs we did with factor analysis, we’ll scale our variables. Scaling the variables transforms them such that they have a mean of roughly zero, and a standard deviation of 1. See Brezina pg. 152-153. We can check the noun column, for example.\n\nsub_dfm &lt;- sub_dfm %&gt;% scale() %&gt;% data.frame()\n\n\nround(mean(sub_dfm$noun_nn), 5)\n\n[1] 0\n\nsd(sub_dfm$noun_nn)\n\n[1] 1\n\n\n\n\n10.1.2 Create a distance matrix\nWe can use some base R functions to create our dendrogram from the following steps. First, we need to create a difference matrix based on distances. The two most common distance measures are euclidean and manhattan, which are described on pg. 153. Note, however, that there are other options, many of which are described here:\nhttps://numerics.mathdotnet.com/Distance.html\nA detailed defense of manhattan distance is located here:\nhttp://rstudio-pubs-static.s3.amazonaws.com/476168_58516a3d6685427badf52a263e690975.html\nAnd a comparative study of distance measure is published here:\nhttps://arxiv.org/ftp/arxiv/papers/1411/1411.7474.pdf\nWe’ll start with euclidean distance.\n\nd &lt;- dist(sub_dfm, method = \"euclidean\")\n\n\n\n10.1.3 Clustering and linkage methods\nThe next step is to determine the linkage method. Brezina details these on pg. 154-159. Here they are in summary:\n\nMaximum or complete linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the largest value (i.e., maximum value) of these dissimilarities as the distance between the two clusters. It tends to produce more compact clusters.\nMinimum or single linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long, “loose” clusters.\nMean or average linkage clustering: It computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters.\nCentroid linkage clustering: It computes the dissimilarity between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.\nWard’s minimum variance method: It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.\n\nWe’ll carry out the hierarchical clustering using Ward.\n\nhc &lt;- hclust(d, method = \"ward.D2\")\n\nAnd plot the result.\n\nplot(hc, cex = 0.6, hang = -1, main = \"\", sub = \"\", xlab = \"\")\n\n\n\n\nCluster dendrogram (using Euclidean distances and Ward’s method) of sample papers in English and Biology.\n\n\n\n\nNote that height is the value of the criterion associated with the clustering method for the particular agglomeration. In this case, Ward’s criterion is the total within-cluster error sum of squares, which increases as you go up the tree and make the clusters bigger.\nWe can follow these same steps using functions from the cluster package, too. These provide us with a few additional options, like get_dist(), which we’ll use to create a distance matrix.\n\nd &lt;- get_dist(sub_dfm)\n\nNow let’s visualize that matrix.\n\nfviz_dist(d, gradient = list(low = \"tomato\", mid = \"white\", high = \"steelblue\"))\n\n\n\n\nA distance matrix (using Euclidean distances) of sample papers in English and Biology.\n\n\n\n\n\n\n10.1.4 Clustering structure\nWe create our plot using the agnes() function, this time. Agglomerative Nesting is fully described in chapter 5 of Kaufman and Rousseeuw (1990), Finding Groups in Data: An Introduction to Cluster Analysis, which is available online through the CMU library:\nhttps://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019521734604436\nCompared to other agglomerative clustering methods such as hclust, agnes has the following feature: it yields the agglomerative coefficient (see agnes.object) which measures the amount of clustering structure found.\n\nhc &lt;- agnes(d, method = \"ward\" )\n\nThis can be plotted in a similar way.\n\nplot(as.hclust(hc), cex = 0.6, hang = -1, main = \"\", sub = \"\", xlab = \"\")\n\n\n\n\nCluster dendrogram (using Euclidean distances and Ward’s method) of sample papers in English and Biology.\n\n\n\n\nBut we can also retrieve an agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\n\nhc$ac\n\n[1] 0.5992656\n\n\nThus, we can see how the structure changes with different linkage methods, First, we can create a vector and a simple function.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(d, method = x)$ac\n}\n\n\n\nCode\nmap_dbl(m, ac) |&gt;\n  tibble::enframe() |&gt;\n  gt() |&gt;  \n  cols_label(\n    name = md(\"**Method**\"),\n    value = md(\"**Coeff**\")\n  )\n\n\n\n\n\n\nAgglomerative coefficients for various linkage methods.\n\n\n\n\n\n\nMethod\nCoeff\n\n\n\n\naverage\n0.2636975\n\n\nsingle\n0.2220678\n\n\ncomplete\n0.4304886\n\n\nward\n0.5992656\n\n\n\n\n\n\n\n\n\n10.1.5 Cutting a dendrogram\nWe can also “cut” our dendrogram in any number of clusters. The question is: How many clusters are optimal. Here, we can use some plotting functions that are part of the factoextra package. The first is the familiar “elbow” method.\n\nfviz_nbclust(sub_dfm, FUN = hcut, method = \"wss\")\n\n\n\n\nA scree plot for within-sum-of-squares.\n\n\n\n\nWith our data, the result isn’t particularly helpful. We can then try the “silhouette” methods. The average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\nA high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.\n\nfviz_nbclust(sub_dfm, FUN = hcut, method = \"silhouette\")\n\n\n\n\nA silhouette plot.\n\n\n\n\nWe can use the result to choose how we want to “cut” our dendrogram. Here we’ll cut it into two clusters.\n\nplot(as.hclust(hc), cex = 0.6, hang = -1, main = \"\", sub = \"\", xlab = \"\")\nrect.hclust(hc, k = 2)\n\n\n\n\nCluster dendrogram (using Euclidean distances and Ward’s method) of sample papers in English and Biology cut into 2 clusters.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "labs/cluster-analysis.html#k-means",
    "href": "labs/cluster-analysis.html#k-means",
    "title": "10  Cluster Analysis",
    "section": "10.2 K-means",
    "text": "10.2 K-means\nAlthough it’s not covered in Brezina, another very common clustering method is called k-means. The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized.\nThe k-means algorithm can be summarized as follows:\nBy the analyst:\n\nSpecify the number of clusters (k) to be created\n\nBy the algorithm:\n\nSelect randomly k objects from the data set as the initial cluster centers or means\nAssign each observation to their closest centroid, based on the Euclidean distance between the object and the centroid\nFor each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a kth cluster is a vector of length p containing the means of all variables for the observations in the kth cluster; p is the number of variables.\nIteratively minimize the total within sum-of-squares. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value for the maximum number of iterations.\n\nWe’ve already determined that our data is best divided into 2 clusters. So we specify “centers” to be 2.\n\nkm &lt;- kmeans(sub_dfm, centers = 2, nstart = 25)\n\nNote that we can access important information about our clusters. For example, we can return the within sum-of-squares:\n\nkm$withinss\n\n[1] 370.3961 323.0377\n\n\nOr the between sum-of-squares:\n\nkm$betweenss\n\n[1] 180.5661\n\n\nPlotting the result is easy with fviz_cluster().\n\nfviz_cluster(km, data = sub_dfm)\n\n\n\n\n\n\n\n\nBut there a variety of ways to make effective plots. Let’s make one that gives us more control over the details.\n\n10.2.1 Plotting and dimension reduction\nDimension reduction for plotting k-means is typically done using PCA. So lets start there.\n\nkm_pca &lt;- prcomp(sub_dfm)\n\nWe can check the percent of variance explained by looking at the eigen values.\n\n\nCode\nround(get_eigenvalue(km_pca), 1) |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\n\n\neigenvalue\nvariance.percent\ncumulative.variance.percent\n\n\n\n\n10.3\n22.4\n22.4\n\n\n6.3\n13.6\n36.0\n\n\n4.7\n10.2\n46.1\n\n\n4.2\n9.1\n55.2\n\n\n3.5\n7.6\n62.8\n\n\n2.8\n6.2\n68.9\n\n\n2.4\n5.2\n74.1\n\n\n2.0\n4.4\n78.5\n\n\n1.6\n3.4\n81.9\n\n\n1.6\n3.4\n85.3\n\n\n\n\n\n\n\nWe can also extract the coordinates for the 2 principal components and create a data frame. We’ll also add columns for discipline and cluster membership.\n\ncoord_df &lt;- data.frame(km_pca$x[,1:2]) %&gt;%\n  mutate(Discipline = str_extract(rownames(sub_dfm), \"^[A-Z]+\")) %&gt;%\n  mutate(Cluster = as.factor(paste0(\"Cluster \", km$cluster)))\n\n\nggplot(coord_df) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(aes(x = PC1, y = PC2, fill = Discipline), size = 1, shape = 21, alpha = .75) +\n  viridis::scale_fill_viridis(discrete = T, direction = -1) +\n  xlab(paste0(\"Dimension 1\")) +\n  ylab(\"Dimension 2\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank()) +\n  theme(panel.grid.minor.x = element_blank()) +\n  theme(panel.grid.major.y = element_blank()) +\n  theme(panel.grid.minor.y = element_blank()) +\n  theme(legend.position=\"top\") +\n  facet_grid(~Cluster)\n\n\n\n\nCluster dendrogram (using Euclidean distances and Ward’s method) of sample papers in English and Biology cut into 2 clusters.\n\n\n\n\n\n\n10.2.2 Variable contributions\nVariable contributions to each PC and their relationship to individual observations and clusters can be visualized using a biplot.\n\nfviz_pca_biplot(km_pca, repel = TRUE,\n                select.var = list(contrib=10),\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n)\n\n\n\n\nBiplot showing the variables with the 10 highest contributions to principal components 1 and 2.\n\n\n\n\nTo extract the precise percentage each variable contributes to a PC we can use fviz_contrib(), but let’s briefly look at how those contributions are calculated.\nFirst, we can use get_pca_var() from the factoextra package to extract the loadings.\n\nkm_pca_var &lt;- get_pca_var(km_pca)\n\nLoadings are the coordinates of the features/variables on the principal components. Loadings are unstandardized eigenvectors’ elements.\nContributions are the square of the loading matrix (the cos2 output from get_pca_var() function) divided by the column sums of the cos2 matrix, which are the variances of PCs.\nThere is a nice explanation here:\nhttps://littlebitofdata.com/en/2017/12/pca/\nTo verify this, we can check to see if the relevant vectors are equal:\n\nall.equal(km_pca_var$cos2[,1] * 100 / sum(km_pca_var$cos2[,1]), km_pca_var$contrib[,1])\n\n[1] TRUE\n\n\nHere they are in tablular form:\n\n\nCode\n# PC1 % Contribution\n\nkm_pca_var$contrib[,1] |&gt;\n  sort(decreasing = T) |&gt;\n  tibble::enframe() |&gt;\n  gt()\n\n\n\n\n\n\nContributions to PC1\n\n\nname\nvalue\n\n\n\n\npron_prp.\n7.8706438229\n\n\nverb_vbz\n7.8654608079\n\n\npron_wp\n7.1835320873\n\n\nnoun_nns\n6.8934596760\n\n\npart_pos\n6.1554504170\n\n\npron_prp\n6.0864832934\n\n\naux_vbn\n5.7925986271\n\n\naux_vb\n4.0960766283\n\n\nnum_cd\n3.8757888030\n\n\npron_nn\n3.4708980200\n\n\nsconj_in\n3.4555264918\n\n\npron_ex\n3.3612599603\n\n\nverb_vbn\n2.9344382766\n\n\naux_vbz\n2.7829282302\n\n\nadj_jjr\n2.5653410022\n\n\naux_vbd\n2.4692363155\n\n\ndet_pdt\n2.3551755683\n\n\npart_to\n2.2685753217\n\n\nadv_rbr\n2.1291027186\n\n\nadp_in\n1.7670925206\n\n\naux_md\n1.7255226533\n\n\nadj_jj\n1.6057856824\n\n\npron_wp.\n1.4832639796\n\n\nnoun_nn\n1.4555340031\n\n\npropn_nnp\n1.4342964611\n\n\nadv_rb\n1.3924723259\n\n\naux_vbp\n1.1821743393\n\n\nadv_wrb\n0.8379468930\n\n\nadj_nn\n0.6343316872\n\n\nnoun_vbg\n0.6189356316\n\n\nverb_vb\n0.4233746477\n\n\npron_wdt\n0.3029131527\n\n\ncconj_cc\n0.2387999315\n\n\npart_rb\n0.2265508617\n\n\npropn_nnps\n0.2179496691\n\n\nadp_rp\n0.1847443396\n\n\nverb_vbp\n0.1671091533\n\n\nintj_uh\n0.1281066668\n\n\nadj_jjs\n0.1152184061\n\n\naux_vbg\n0.0838218686\n\n\nverb_vbd\n0.0559400984\n\n\nverb_vbg\n0.0523675707\n\n\npron_dt\n0.0315891361\n\n\nadv_rbs\n0.0175221007\n\n\ndet_dt\n0.0043273031\n\n\ndet_wdt\n0.0003328488",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "labs/time-series.html",
    "href": "labs/time-series.html",
    "title": "11  Time Series",
    "section": "",
    "text": "11.1 Importing Google Ngrams Data\nWe won’t do much with the google_ngram() function because most of Google data table are HUGE. Though they are formatted as simple tab-delimited text files, they often run in the multiple gigabytes in size. You can access them here:\nhttp://storage.googleapis.com/books/ngrams/books/datasetsv2.html\nWith that in mind, we’ll do a simple demo of the function with one of the smaller 1-gram tables: Q. First, we’ll make a vector of the word forms we want to count. In this case, we’ll count 3 common forms of quiz.\nwfs &lt;- c(\"quiz\", \"quizzes\", \"quizzed\")\nq &lt;- google_ngram(wfs, variety = \"eng\", by = \"year\")\nNote that before we plot, we should check for gaps – in other words, years when counts are not in the data and, thus, zero. We can use the is.sequence() function.\nHere we’ll check the full Year column and only years after 1799.\nq$Year %&gt;% is.sequence()\n\n[1] FALSE\n\nq$Year[q$Year &gt; 1799] %&gt;% is.sequence()\n\n[1] TRUE",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "labs/time-series.html#peaks-and-troughs",
    "href": "labs/time-series.html#peaks-and-troughs",
    "title": "11  Time Series",
    "section": "11.2 Peaks and Troughs",
    "text": "11.2 Peaks and Troughs\nIf we wanted to plot from the first instance (1515) onward, we would need to fill in the all missing years (rows) with zero values. We would also want to check the underlying data. A quick search of Google Books seems to suggest that sixteenth century examples come from passages that are not in English and often in the form qu’iz.\nInstead, we will plot only data from 1800 and after. And we can plot the data with a confidence interval to identify “peaks and troughs” (Brezina pgs. 241-247).\n\nggplot(q %&gt;% filter(Year &gt; 1799), aes(x=Year, y=Per_10.6)) +\n    geom_point(size = .5) +\n    geom_smooth(method = \"gam\", formula = y ~ s(x, bs = \"cs\"), size=.25) +\n    labs(x=\"Year\", y = \"Frequency (per million words)\")+ \n    theme(panel.grid.minor.x=element_blank(),\n          panel.grid.major.x=element_blank()) +\n    theme(panel.grid.minor.y =   element_blank(),\n          panel.grid.major.y =   element_line(colour = \"gray\",size=0.25)) +\n    theme(rect = element_blank()) +\n    theme(legend.title=element_blank())\n\n\n\n\nFrequencies (per million words) of lemmatized quiz from the 18th through the 20th centuries.\n\n\n\n\nWe can also aggregate the counts by decade.\n\nq &lt;- google_ngram(wfs, variety = \"eng\", by = \"decade\")\n\n\nggplot(q %&gt;% filter(Decade &gt; 1799), aes(x=Decade, y=Per_10.6)) +\n    geom_bar(stat = \"identity\") +\n    labs(x=\"Decade\", y = \"Frequency (per million words)\")+ \n    theme(panel.grid.minor.x=element_blank(),\n          panel.grid.major.x=element_blank()) +\n    theme(panel.grid.minor.y =   element_blank(),\n          panel.grid.major.y =   element_line(colour = \"gray\",size=0.25)) +\n    theme(rect = element_blank()) +\n    theme(legend.title=element_blank())\n\n\n\n\nFrequencies by decade (per million words) of lemmatized quiz from the 18th through the 20th centuries.\n\n\n\n\n\n11.2.1 Confidence Intervals\nTo add confidence intervals to a bar plot, we need to calculate the upper and lower bounds. For this we’ll use the prop.cint() from the corpora package:\nhttps://www.rdocumentation.org/packages/corpora/versions/0.5/topics/prop.cint\nWe only need to pass the function a vector of frequencies and a vector of total counts (corpus sizes). We’ll bind those to a new data frame and normalize per million tokens.\n\nword_freq &lt;- q %&gt;% \n  bind_cols(corpora::prop.cint(k = q$AF, n = q$Total, conf.level = 0.95)) %&gt;%\n  mutate(lower = lower*1000000) %&gt;%\n  mutate(upper = upper*1000000)\n\nAnd plot adding geom_errorbar()\n\nggplot(word_freq %&gt;% filter(Decade &gt; 1799), aes(x=Decade, y=Per_10.6)) +\n    geom_bar(stat = \"identity\", fill=\"steelblue\") +\n    geom_errorbar(aes(ymin=lower, ymax=upper), width=.1) +\n    labs(x=\"Decade\", y = \"Frequency (per million words)\") + \n    theme(panel.grid.minor.x=element_blank(),\n          panel.grid.major.x=element_blank()) +\n    theme(panel.grid.minor.y =   element_blank(),\n          panel.grid.major.y =   element_line(colour = \"gray\",size=0.25)) +\n    theme(rect = element_blank()) +\n    theme(legend.title=element_blank())\n\n\n\n\nFrequencies by decade (per million words and with 95% confidence intervals) of lemmatized quiz from the 18th through the 20th centuries.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "labs/time-series.html#periodization",
    "href": "labs/time-series.html#periodization",
    "title": "11  Time Series",
    "section": "11.3 Periodization",
    "text": "11.3 Periodization\nJust eyeballing the data, it looks like there might be some interesting changes in frequency in the middle of the 20th century, and late in the 20th century.\nTo better understand how these changes group together (what is called “periodization”) we can turn turn Variability-Based Neighbor clustering.\nWe’re going take data for the 20th century onward and begin with a scree plot.\n\nq &lt;- q %&gt;% filter(Decade &gt; 1899)\n\n\nvnc_scree(q$Decade, q$Per_10.6)\n\n\n\n\nScree plot of standard deviations for lemmatized quiz.\n\n\n\n\nFrom the scree plot, it looks like we have 2-3 well-formed clusters. Now, we’ll generate the data for our dendrogram. Keep in mind this is a very specific implementation of hierarchical clustering as we need to maintain the order of our time series.\nThe distance is based on standard deviations of sequential pairs of time intervals. Alternatively, you can set the distance.measure to “cv” for to use the coefficient of variation.\n\nhc &lt;- vnc_clust(q$Decade, q$Per_10.6, distance.measure = \"sd\")\nplot(hc, hang = -1, main = \"\", sub = \"\", xlab = \"\")\n\n\n\n\nVNC dendrogram showing frequencies of lemmatized quiz in 20th century English.\n\n\n\n\nThe purpose of Variability-Based Neighbor Clustering is to divide the use of a word or phrase into historical periods based on changes in frequency. Rather than assuming that a year, decade, or other division is statistically meaningful, the algorithm clusters segments of time into periods.\nNow let’s look at some other data: frequencies of the bigram witch hunt and the plural witch hunts. These also comes from Google Books. You can gather the data yourself at a later time using google_ngram(), if you want, but for the purposes of this exercise we’ll skip that step to save time.\n\nload(\"../data/witch_hunt.rda\")\n\nwh &lt;- witch_hunt %&gt;%\n  filter(decade &gt; 1899) %&gt;% \n  dplyr::select(decade, counts_permil)\n\nvnc_scree(wh$decade, wh$counts_permil, distance.measure = \"sd\")\n\n\n\n\nScree plot of standard deviations for lemmatized witch hunt.\n\n\n\n\n\nhc &lt;- vnc_clust(wh$decade, wh$counts_permil, distance.measure = \"sd\")\n\nFor the next step, we’ll cut the dendrogram into 3 clusters based on the output of the scree plot we that generated. Note that we’re storing the output into a list cut_hc.\n\nplot(hc, hang = -1)\ncut_hc &lt;- rect.hclust(hc, k=3)\n\n\n\n\nVNC dendrogram showing frequencies of lemmatized witch hunt in 20th century English.y cut into 3 clusters.\n\n\n\n\n\n\n\n11.3.1 Advanced plotting\nWe’ve already plotted our data with base R. However, if we want more control, we probably want to use ggplot2. To do that, we need to go through a couple of intermediate steps. First, convert the cut_hc object that we just generated into a data frame and join that with our original witch hunt data.\n\nclust_df &lt;- data.frame(decade=as.numeric(names(unlist(cut_hc))),\n  clust=rep(c(paste0(\"clust_\", seq(1:length(cut_hc)))),\n  times=sapply(cut_hc,length)))\n\nclust_df &lt;- clust_df %&gt;% right_join(wh, by = \"decade\")\n\nNext, we’ll convert our cluster data into dendrogram data using as.dendrogram() from ggdendro. We also MUST maintain the order of our time series. There are a variety of ways of doing this, but dendextend has an easy function called sort(). We’ll take the easy way!\nTo get ggplot-friendly data, we have to transform it yet again… This time using the ggdendro package’s function dendro_data().\n\ndend &lt;- as.dendrogram(hc) %&gt;% sort\ndend_data &lt;- dendro_data(dend, type = \"rectangle\")\n\nNow let’s do some fancy plotting! We’re going to combine the dendrogram and a time series line plot like Gries and Hilpert (Gries and Hilpert 2012) do on pg. 140 of their chapter on VNC.\nThe first three lines pull data from clust_df for the line plot using the clusters to color each point according to group. The geom_segment pulls data from dend_data to build the dendrogram. For the tick marks we again pull from dend_data using the x column for the breaks and and the label column to label the breaks.\n\nggplot(clust_df, aes(x = as.numeric(rownames(clust_df)), y = counts_permil)) +\n  geom_line(linetype = \"dotted\") +\n  geom_point(aes(color = clust), size = 2) +\n  geom_segment(data = dend_data$segments, aes(x = x, y = y, xend = xend, yend = yend))+\n  scale_x_continuous(breaks = dend_data$labels$x,\n    labels=as.character(dend_data$labels$label)) +\n  xlab(\"\") + ylab(\"Frequency (per million words)\") +\n  theme_minimal()\n\n\n\n\nVNC dendrogram showing frequencies of lemmatized witch hunt in 20th century English cut into 3 clusters.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "labs/time-series.html#works-cited",
    "href": "labs/time-series.html#works-cited",
    "title": "11  Time Series",
    "section": "11.4 Works cited",
    "text": "11.4 Works cited\n\n\n\n\nGries, Stefan, and Martin Hilpert. 2012. “Variability-Based Neighbor Clustering.” The Oxford Handbook of the History of English, 134–44. https://www.stgries.info/research/2012_STG-MH_VarNeighbClustering_OxfHBHistEngl.pdf.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "labs/vector-models.html",
    "href": "labs/vector-models.html",
    "title": "12  Introduction to Vector Models and Word Embeddings",
    "section": "",
    "text": "12.1 Basics\nA variety of models are available for you on CMU Box:\nhttps://cmu.box.com/s/o2y5lbaonmguf51h5kipa6i2v4y3nm3n\nYou can simply download a model and place it in the models directory. Then load the model.\nFor demo purposes, we’ll use a small one that contains the 50,000 most frequent tokens in COCA and embeddings in only 100 dimensions.\n# model &lt;- starspace_load_model(\"../models/en_coca_vec_sm.ruimtehol\")\n# model &lt;- starspace_load_model(\"../models/en_stanford_cc_lg.ruimtehol\")\n\nmodel &lt;- starspace_load_model(\"../models/en_coca_vec_50k.ruimtehol\")\nNow we can check how similarities work.\nembedding_similarity(\n  starspace_embedding(model, \"dog\"),\n  starspace_embedding(model, \"cat\"), \n  type = \"cosine\") |&gt;\n  tibble::enframe() |&gt;\n  gt()\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\ndog\n0.8887681\nLook for nearest embeddings.\nstarspace_knn(model, \"shakespeare\", 10) |&gt;\n  data.frame() |&gt;\n  gt()\n\n\n\n\n\n\n\ninput\nprediction.label\nprediction.similarity\nprediction.rank\n\n\n\n\nshakespeare\nshakespeare\n1.0000000\n1\n\n\nshakespeare\nmacbeth\n0.8593846\n2\n\n\nshakespeare\nshakespearean\n0.8482000\n3\n\n\nshakespeare\nelizabethan\n0.7955307\n4\n\n\nshakespeare\ndramatist\n0.7793428\n5\n\n\nshakespeare\nchekhov\n0.7715034\n6\n\n\nshakespeare\nchaucer\n0.7700459\n7\n\n\nshakespeare\nlyric\n0.7639523\n8\n\n\nshakespeare\nsondheim\n0.7615547\n9\n\n\nshakespeare\nlear\n0.7589097\n10\nWe can also calculate document similarity.\nembedding_similarity(\n  starspace_embedding(model, \"what does this bunch of text look like\", type = \"document\"),\n  starspace_embedding(model, \"word abracadabra is even in the dictionary\", type = \"document\"), \n  type = \"cosine\") |&gt;\n  tibble::enframe() |&gt;\n  gt()\n\n\n\n\n\n\n\nname\nvalue\n\n\n\n\nwhat does this bunch of text look like\n0.9002735\nNote that we can extract embeddings, as well.\nembedding &lt;- as.matrix(model)\nembedding[c(\"dog\", \"cat\"), ][,1:10] |&gt;\n  data.frame() |&gt;\n  gt()\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\n\n\n\n\n0.3358528\n-0.06600608\n-0.082166046\n0.3292679\n-0.2417040\n0.07791379\n-0.07094227\n-0.076595172\n-0.3657310\n-0.1289230\n\n\n0.3675254\n-0.10713406\n0.002169422\n0.2818099\n-0.2916131\n0.01043738\n-0.05357843\n0.002390111\n-0.2753031\n-0.1173636\nThis ability is useful if we want to plot the locations of words or documents.",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Vector Models and Word Embeddings</span>"
    ]
  },
  {
    "objectID": "labs/vector-models.html#plotting",
    "href": "labs/vector-models.html#plotting",
    "title": "12  Introduction to Vector Models and Word Embeddings",
    "section": "13.1 Plotting",
    "text": "13.1 Plotting\nFor dimension reduction use a different technique: t-SNE. For more about t-Distributed Stochastic Neighbor Embedding see here:\nhttps://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1\n\nm &lt;- dist(doc_embeddings[,3:102]) %&gt;% as.matrix()\ndimnames(m) &lt;- dimnames(m) &lt;- list(doc_embeddings$doc_id, doc_embeddings$doc_id) \ndf_pairs &lt;- t(combn(doc_embeddings$doc_id, 2))\ndist_df &lt;- data.frame(df_pairs, dist=m[df_pairs])\ndist_df &lt;- dist_df %&gt;% arrange(dist)\n\n\ndoc_tsne &lt;- Rtsne(as.matrix(doc_embeddings[,3:102]), check_duplicates = FALSE, pca = FALSE, perplexity=5, theta=0.5, dims=2)\n\ndoc_tsne &lt;- as.data.frame(doc_tsne$Y) %&gt;% bind_cols(select(doc_embeddings, doc_id, group))\n\n\nggplot(doc_tsne, aes(x = V1, y = V2, fill = group)) +\n  geom_point(shape = 21) +\n  ggrepel::geom_text_repel(aes(label = doc_id), size = 3) +\n  viridis::scale_fill_viridis(discrete = T) +\n  theme_linedraw() +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\nHistogram of the token the",
    "crumbs": [
      "Labs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Vector Models and Word Embeddings</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_01.html",
    "href": "lab_sets/LabSet_01.html",
    "title": "13  Lab Set 1",
    "section": "",
    "text": "13.1 Questions related to the first week discussions",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lab Set 1</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_01.html#questions-related-to-the-first-week-discussions",
    "href": "lab_sets/LabSet_01.html#questions-related-to-the-first-week-discussions",
    "title": "13  Lab Set 1",
    "section": "",
    "text": "13.1.1 Task 1\nConsider the results of the preliminary study described in slides 65-70:\nIf you were working on this project, what would you suggest the team do next? In other words, what limitations do you see in the results of this initial study? What might be done to increase its reliability? Or its generalizability? And what potential challenges do you foresee in applying your suggestions?\nDiscuss with a couple of your neighbors and write your response in a short paragraph.\n\nYour response:\n\n\n\n13.1.2 Task 2\nAnswer the the following question:\nWhat interests you about the quantitative analysis of text?\n\nYour response:",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lab Set 1</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_01.html#mosteller-wallace",
    "href": "lab_sets/LabSet_01.html#mosteller-wallace",
    "title": "13  Lab Set 1",
    "section": "13.2 Mosteller & Wallace",
    "text": "13.2 Mosteller & Wallace\n\n13.2.1 Task 1\nMosteller and Wallace talk about a “little book of decisions” – a record of the choices that they made in carrying out their project. Most data-driven projects require similarly complex choices. In defending them, sometimes in defending them, we check them. In making choice x, have we unduly influenced the result?\nPick one of Mosteller and Wallace’s decisions and describe how you might check whether it affecting their findings.\n\nYour response\n\n\n\n13.2.2 Task 2\nOur model predicts all but 55 were written by Madison. Our model is not particularly confident about that result. This hews pretty closely to Mosteller & Wallace’s findings, through they come down (sort of) on the side of Madison for 55. However, they also acknowledge that the evidence is weak and not very convincing.\nGive at least 3 possible factors that might explain that low probability?\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lab Set 1</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_01.html#nlp-basics",
    "href": "lab_sets/LabSet_01.html#nlp-basics",
    "title": "13  Lab Set 1",
    "section": "13.3 NLP Basics",
    "text": "13.3 NLP Basics\n\n13.3.1 Task 1\n\n13.3.1.1 What counts as a token?\nThese choices are important. To carry our any statistical analysis on texts, we radically reorganize texts into counts. Precisely how we choose to do that–the decisions we make in exactly what to count–affects everything else downstream.\nSo let’s look at a chunk of text that is a little more complicated than the example from A Tale of Two Cities. Consider the following text:\n\nIn spite of some problems, we saw a 35% uptick in our user-base in the U.S. But that’s still a lot fewer than we had last year. We’d like to get that number closer to what we’ve experienced in the U.K.–something close to 3 million users.\n\nYou are a member of team tasked with tokenizing 20,000 texts that are similar to this one. A member of the suggests using a regular expression that splits on word boundaries: \\\\b using the str_split() function, try splitting the example string above.\n\n# your code goes here\n\nBased on the result, do you think the suggested strategy is a good one? Why or why not?\n\nYour response\n\n\n\n\n13.3.2 Task 2\nBriefly describe the tokens that you want to output\n\nYour response:\n\nWhat are some kinds of tokens that you think are particularly challenging to deal with (e.g., hyphenated words, contractions, abbreviations, etc.)?\n\nYour response:",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lab Set 1</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_01.html#tokenizing-with-quanteda",
    "href": "lab_sets/LabSet_01.html#tokenizing-with-quanteda",
    "title": "13  Lab Set 1",
    "section": "13.4 Tokenizing with quanteda",
    "text": "13.4 Tokenizing with quanteda\n\n13.4.1 Task 1\nUse the following text:\n\n“The more I dove in, though, the less I cared. I watched BTS perform their 2018 anthem”Idol” on The Tonight Show and wondered how their lungs didn’t explode from exertion. I watched the sumptuous short film for their 2016 hit “Blood, Sweat, and Tears” and couldn’t tell whether I was more impressed by the choreography or the high-concept storytelling. And I was entranced by the video for “Spring Day,” with its dreamlike cinematography and references to Ursula K. Le Guin and Bong Joon-ho’s film Snowpiercer. When I learned that the video is often interpreted as a tribute to the school-age victims of 2014’s Sewol ferry disaster, I replayed it and cried.”\n\nIn the code chunk below, construct a pipeline that:\n\ntokenizes the text\ncreates a corpus object\ncreates a dfm\ngenerates a frequency count of tokens\nuses the mutate() function to add a RF column (for “relative frequency”) to the data frame.\n\nHint: Relative frequency (or normalized frequency) just takes the frequency, divides it by total number of tokens/words, and multiplies by a normalizing factor (e.g., by 100 for percent of tokens).\n\n# your code goes here\n\nAnd report the results in a gt table:\n\n\n13.4.2 Task 2\nData from Lab 02:\n\nlibrary(tidyverse)\nlibrary(quanteda)\n\n\nsource(\"../R/helper_functions.R\")\n\n\ntotc_txt &lt;- \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n\ntext_2 &lt;- \"Jane Austen was not credited as the author of 'Pride and Prejudice.' In 1813, the title page simply read \\\"by the author of Sense and Sensibility.\\\" It wasn't until after Austen's death that her identity was revealed. #MentalFlossBookClub with @HowLifeUnfolds #15Pages https://pbs.twimg.com/media/EBOUqbfWwAABEoj.jpg\"\n\ncomb_corpus &lt;-   data.frame(doc_id = c(\"text_1\", \"text_2\"), text = c(totc_txt, text_2)) %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus()\n\ndocvars(comb_corpus) &lt;- data.frame(text_type = c(\"Fiction\", \"Twitter\"))\n\ncomb_tkns &lt;- comb_corpus %&gt;%\n  tokens(what = \"fastestword\")\n\n\n# structure 1:\ncomb_dfm &lt;- dfm(comb_tkns) %&gt;% dfm_group(groups = text_type)\ncomb_freq &lt;- dfm(comb_tkns) %&gt;% quanteda.textstats::textstat_frequency(groups = text_type)\n\n# structure 2:\ncomb_ntoken &lt;- data.frame(\"Tokens\" = ntoken(comb_tkns), docvars(comb_tkns))\n\nUse one of these 2 data structures (comb_freq or comb_ntoken) to make a corpus composition table. It should have 2 columns (one for “Text Type” and the other for “Tokens”) and 3 rows (“Fiction”, “Twitter” and “Total”). And report the results in a gt table.\n\n\n\n\n\n\nAggregating across columns\n\n\n\nUse the gt function grand_summary_rows() to create a count of totals or other measures.\n\n\n\n# your code for a gt table goes here",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Lab Set 1</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_02.html",
    "href": "lab_sets/LabSet_02.html",
    "title": "14  Lab Set 2",
    "section": "",
    "text": "14.1 Distributions",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Lab Set 2</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_02.html#distributions",
    "href": "lab_sets/LabSet_02.html#distributions",
    "title": "14  Lab Set 2",
    "section": "",
    "text": "14.1.1 Task 1\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n\n\nload(\"../data/sample_corpus.rda\")\nsource(\"../R/dispersion_functions.R\")\n\n\nsc_tokens &lt;- sample_corpus %&gt;%\n  corpus() %&gt;%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, what = \"word\") %&gt;%\n  tokens_tolower()\n\nsc_dfm &lt;- sc_tokens %&gt;%\n  dfm()\n\nsc_freq &lt;- sc_dfm %&gt;%\n  textstat_frequency() %&gt;%\n  mutate(RF = (frequency/sum(frequency))*1000000)\n\nPlot a histogram (or histograms) for the the 1st, 10th, and 100th most frequent tokens in the sample corpus.\n\n# your code goes here\n\nWhat do you notice (or what conclusions can you draw) from the plots you’ve generated about the distributions of tokens as their frequency decreases?\n\nYour response\n\n\n\n14.1.2 Task 2\n\nthe &lt;- dispersions_token(sc_dfm, \"the\") %&gt;% unlist()\ndata &lt;- dispersions_token(sc_dfm, \"data\") %&gt;% unlist()\n\n\nthe['Deviation of proportions DP']\n\nDeviation of proportions DP \n                  0.1388907 \n\ndata['Deviation of proportions DP']\n\nDeviation of proportions DP \n                   0.845857 \n\n\nWhat do you note about the difference in the Deviation of Proportions for the vs. data?\n\nYour response\n\n\n\n14.1.3 Task 3\n\nsc_ft &lt;- frequency_table(sc_tokens)\n\nWhich token is the most frequent? The most dispersed?\n\nYour response\n\nWrite a sentence or two reporting the frequencies and dispersions of the and data fowling the examples on page 53 of Brezina:\n\nYour response\n\n\nggplot(sc_freq %&gt;% filter(rank &lt; 101), aes(x = rank, y = frequency)) +\n  geom_point(shape = 1, alpha = .5) +\n  theme_classic() +\n  ylab(\"Absolute frequency\") +\n  xlab(\"Rank\")\n\n\n\n\nfig.cap=“Token rank vs. frequency.”\n\n\n\n\nThe relationship you’re seeing between the rank of a token and it’s frequency holds true for almost any corpus and is referred to as Zipf’s Law (see Brezina pg. 44).\n\n\n14.1.4 Task 4\nDescribe at least one statistical and one methodological implication of what the plot is illustrating.\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Lab Set 2</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_02.html#collocations-and-association-measures",
    "href": "lab_sets/LabSet_02.html#collocations-and-association-measures",
    "title": "14  Lab Set 2",
    "section": "14.2 Collocations and association measures",
    "text": "14.2 Collocations and association measures\n\n14.2.1 Task 1\n\nlibrary(tidyverse)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\n\n\nsource(\"../R/helper_functions.R\")\nsource(\"../R/utility_functions.R\")\nsource(\"../R/collocation_functions.R\")\n\n\nsc_tokens &lt;- sample_corpus %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus() %&gt;%\n  tokens(what=\"fastestword\", remove_numbers=TRUE)\n\n\nmoney_collocations &lt;- collocates_by_MI(sc_tokens, \"money\")\ntime_collocations &lt;- collocates_by_MI(sc_tokens, \"time\")\n\nReport the collocations of time and money in 2 or 3 sentences following the conventions described in Brezina (pg. 75).\n\nYour response\n\n\n\n14.2.2 Task 2\n\ntc &lt;- time_collocations %&gt;% filter(col_freq &gt;= 5 & MI_1 &gt;= 5)\nmc &lt;- money_collocations %&gt;% filter(col_freq &gt;= 5 & MI_1 &gt;= 5)\nnet &lt;- col_network(tc, mc)\n\n\nlibrary(ggraph)\n\n\nggraph(net, weight = link_weight, layout = \"stress\") + \n  geom_edge_link(color = \"gray80\", alpha = .75) + \n  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +\n  geom_node_text(aes(label = label), repel = T, size = 3) +\n  scale_alpha(range = c(0.2, 0.9)) +\n  theme_graph() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nWrite a 2-4 sentence interpretation of the time vs. money collocational network.\n\nYour response\n\n\n\n14.2.3 Task 3\nLoad the down-sampled screenplays, extract the dialogue, and tokenize the data.\n\nload(\"../data/screenplays.rda\")\n\nsp &lt;- from_play(sp, extract = \"dialogue\")\n\nsp &lt;-   sp %&gt;%\n  mutate(text = preprocess_text(text)) %&gt;%\n  corpus() %&gt;%\n  tokens(what=\"fastestword\", remove_numbers=TRUE)\n\n\nb &lt;- collocates_by_MI(sp, \"boy\", left = 3, right = 0)\nb &lt;- b %&gt;% filter(col_freq &gt;= 3 & MI_1 &gt;= 3)\n\ng  &lt;- collocates_by_MI(sp, \"girl\", left = 3, right = 0)\ng &lt;- g %&gt;% filter(col_freq &gt;= 3 & MI_1 &gt;= 3)\n\n\n14.2.3.1 Plot the network\n\nnet &lt;- col_network(b, g)\n\nggraph(net, weight = link_weight, layout = \"stress\") + \n  geom_edge_link(color = \"gray80\", alpha = .75) + \n  geom_node_point(aes(alpha = node_weight, size = 3, color = n_intersects)) +\n  geom_node_text(aes(label = label), repel = T, size = 3) +\n  scale_alpha(range = c(0.2, 0.9)) +\n  theme_graph() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nWrite a 3-5 sentence interpretation of the boy vs. girl collocational network, which includes reporting relevant association measures following the example in Brezina (pg. 75).\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Lab Set 2</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_02.html#keyness",
    "href": "lab_sets/LabSet_02.html#keyness",
    "title": "14  Lab Set 2",
    "section": "14.3 Keyness",
    "text": "14.3 Keyness\n\n14.3.1 Task 1\n\n14.3.1.1 Create a keyness table\n\nsource(\"../R/keyness_functions.R\")\nsource(\"../R/helper_functions.R\")\nload(\"../data/sample_corpus.rda\")\n\n\nIn the code block below, create a document-feature matrix of the blog text-type.\nIn the same code-block create a keyness table with the blog text-type as the target corpus and the news text-type as the reference.\n\n\n# your code goes here\n\n\nUse the code block below to output the head of the keyness table with an accompanying caption.\n\n\n# your table goes here\n\n\n\n14.3.1.2 Answer the following questions\n\nWhat are the 2 tokens with the highest keyness values?\n\n\nYour response\n\n\nPosit an explanation for their greater frequency in blog corpus, being as descriptive as possible. Think about the communicative purposes of these text-types, as opposed to value judgments about the writers or the genres.\n\n\nYour response\n\n\nWhat are the 2 tokens with the greatest effect sizes?\n\n\nYour response\n\n\nPosit a reason for that result.\n\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Lab Set 2</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_03.html",
    "href": "lab_sets/LabSet_03.html",
    "title": "15  Lab Set 3",
    "section": "",
    "text": "15.1 Lab 07",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lab Set 3</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_03.html#lab-07",
    "href": "lab_sets/LabSet_03.html#lab-07",
    "title": "15  Lab Set 3",
    "section": "",
    "text": "15.1.1 Task 1\nFrom the keyness table (acad_v_fic, line 271), identify the preposition with the highest keyness value. Report the relevant statistics (including frequencies and dispersions) following the conventions described in Brezina.\n\nYour response\n\nCreate a KWIC table (with the preposition as the node word and a context window of 3) of 10 rows.\n\n# Your table...\n\nPosit an explanation for the higher frequency of the preposition in the target corpus (academic writing) vs. the reference corpus (fiction).\n\nYour response\n\n\n\n15.1.2 Task 2\nCalculate the mean length of the noun phrases in the 2 two text-types (acad_nps and fic_nps). Report the results and posit an explanation for the findings that connects to the previous findings related to prepositions.\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lab Set 3</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_03.html#lab-08",
    "href": "lab_sets/LabSet_03.html#lab-08",
    "title": "15  Lab Set 3",
    "section": "15.2 Lab 08",
    "text": "15.2 Lab 08\n\n15.2.1 Task 1\nFollowing the example in Brezina (pg. 129), report and briefly interpret the output of the regression model (wt_regs, line 245).\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lab Set 3</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_03.html#task-2-1",
    "href": "lab_sets/LabSet_03.html#task-2-1",
    "title": "15  Lab Set 3",
    "section": "15.3 Task 2",
    "text": "15.3 Task 2\nWrite a brief interpretation of the probability curves illustrated in Figure 5.\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lab Set 3</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_03.html#lab-9",
    "href": "lab_sets/LabSet_03.html#lab-9",
    "title": "15  Lab Set 3",
    "section": "15.4 Lab 9",
    "text": "15.4 Lab 9\n\n15.4.1 Task 1\nBrezina similarly plots the Brown corpus registers on pg. 169. His process is a little different. Rather than extracting factor loadings from the Brown corpus, he uses the loadings from the original Biber data (some of which are listed on pg. 168).\nOur loadings for dimension 1 are similar to Biber’s, though with some differences. Likewise, the resulting plot is similar to the one on pg. 169. Why is this the case, do you think? (If you want to check Biber’s description of his corpus, it’s on pg. 66 of his book.)\n\nYour response\n\n\n\n15.4.2 Task 2\nUsing information from the factor loadings, the positions of the disciplines along the dimension, and KWIC tables, name dimension 1 following the X vs. Y convention. In a couple of sentences, explain your reasoning.\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lab Set 3</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_04.html",
    "href": "lab_sets/LabSet_04.html",
    "title": "16  Lab Set 4",
    "section": "",
    "text": "16.0.1 Task 1\nWhich linkage method gives the strongest clustering structure?",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab Set 4</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_04.html#lab-11",
    "href": "lab_sets/LabSet_04.html#lab-11",
    "title": "16  Lab Set 4",
    "section": "16.1 Lab 11",
    "text": "16.1 Lab 11\n\n16.1.1 Task 1\nWhy might it be important to periodize data from “the ground up” (using a technique like VNC), rather than just splitting data into intervals of, say, 10, 25, or 50 years?\n\nYour response\n\nFollowing the conventions described in Brezina (pgs. 240-241) report the results produced by the “witch hunt” VNC.\n\nYour response",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab Set 4</span>"
    ]
  },
  {
    "objectID": "lab_sets/LabSet_04.html#lab-12",
    "href": "lab_sets/LabSet_04.html#lab-12",
    "title": "16  Lab Set 4",
    "section": "16.2 Lab 12",
    "text": "16.2 Lab 12\n\n16.2.1 Task 1\nOur model predicts all but 55 were written by Madison. Our model is not particularly confident about that result. This hews pretty closely to Mosteller & Wallace’s findings, through they come down (sort of) on the side of Madison for 55. However, they also acknowledge that the evidence is weak and not very convincing.\nGive at least 3 possible factors that might explain that low probability?",
    "crumbs": [
      "Labs Sets",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Lab Set 4</span>"
    ]
  },
  {
    "objectID": "lectures/embed_linguistic-facts.html",
    "href": "lectures/embed_linguistic-facts.html",
    "title": "17  Linguistic Facts of Life",
    "section": "",
    "text": "View slides in full screen",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Linguistic Facts of Life</span>"
    ]
  },
  {
    "objectID": "lectures/embed_llms-history.html",
    "href": "lectures/embed_llms-history.html",
    "title": "18  LLMs History",
    "section": "",
    "text": "View slides in full screen\n       \n      \n    \n  \n\n18.0.0.1 There is a lot of information here. Some of this we’ll just touch, but make sure you read the key takeways at the end of the presentation.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>LLMs History</span>"
    ]
  }
]